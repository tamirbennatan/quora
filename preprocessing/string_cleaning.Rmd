### Cleaning text

#### 0. Load data and packages
```{R}
library(dplyr)
library(tidytext)
library(hunspell)
library(stringr)
```
```{R}
data.train <- read.csv("../data/train.csv")
# data.test <- read.csv("../data.test.csv")
```
```{R}
str(data.train)
```
```{R}
head(data.train)
```

### 1. Ivestigating the words in the questions

#### 1.1 A look at the words in the training data
```{R}
# Get a single dataframe with all the questions.
train.questions <- data.train %>%
      mutate(qid = qid1, question = question1) %>%
      select(qid, question) %>%
      bind_rows(
            data.train %>% 
                  mutate(qid = qid2, question = question2) %>%
                  select(qid, question)
      ) %>%
      group_by(qid, question) %>%
      filter(row_number() == 1) %>%
      ungroup()

```
```{R}
head(train.questions)

```

Now to open up each question into the individual words
```{R}
train.words <- train.questions %>%
      unnest_tokens(word, question) %>%
      mutate(valid.spelling = hunspell_check(word))
```
```{R}
train.words
```
```{R}
train.words %>% group_by(word) %>% 
      summarise(n = n()) %>% 
      arrange(n)

```

#### 1.2 Convert units to longform

```{R}
unitted <- "[0-9\\.]+[a-z]+"

train.words %>%
      filter(!valid.spelling) %>%
      filter(word == str_match(word, unitted)) %>%
      mutate(value = str_split_fixed(word, "[0-9\\.]+",2)[,1],
             unit = str_split_fixed(word,"[0-9\\.]+",2)[,2]) %>%
      group_by(unit) %>%
      summarize(n = n())

```
Some of these are actual units, while others are just words attatched to numbers.
For the proper words, split them into numbers and words. 

```{R}
train.words %>%
      filter(!valid.spelling) %>%
      filter(word == str_match(word, unitted)) %>%
      mutate(value = str_split_fixed(word, "[0-9\\.]+",2)[,1],
             unit = str_split_fixed(word,"[0-9\\.]+",2)[,2]) %>%
      group_by(unit) %>%
      summarize(n = n()) %>% 
      ungroup() %>%
      mutate(valid.spelling = hunspell_check(unit)) %>%
      filter(valid.spelling)

```

There seem to be some actual units attatched to numbers, and typos where regular words are attatched to numers. I'll first cast the shorthand units to longhand units, and then split all numbers characters from words by a space. 

```{R}
shorthand.units = c("m", "cm", "g","gm", "kg", "mg",
                     "min" , "j", "deg", "hr", "hrs", 
                    "in", "kcal", "kl", "kph", "kw", 
                     "mm", "mpg", "nm",  "sec", "secs",
                      "wk", "wks", "wpm", "yr", "yrs")

longhand.units = c("meter", "centimeter", "gram", "gram", "kilogram", "miligram", 
                   "minute", "joules", "degrees", "hour", "hours",
                   "inch", "kilocalorie", "kiloliter", "kilomters per hour", "kilowatt",
                   "milimeter", "miles per gallon", "nanometer", "second", "seconds",
                   "week", "weeks", "words per minute", "year", "years"
                   )
```
```{R}
print("FROM TO")
for (i in 1:length(shorthand.units)){
      print(paste(shorthand.units[i], longhand.units[i],sep = " "))
}

```

Replace all the shorthand units with longhand units, and appropriately space
```{R}
# regexes to match in each iteration
rgxs = rep("", length(shorthand.units))
for (i in 1:length(shorthand.units)){
      rgxs[i] <- regex(paste('([0-9]+)(', shorthand.units[i], ' | ',shorthand.units[i], ' )', sep = ""), ignore_case = TRUE)
}
# convert shorthanded units to longhanded united values in data
for (i in 1:length(shorthand.units)){
      .GlobalEnv$data.train <- .GlobalEnv$data.train %>%
            mutate(question1 = str_replace_all(question1, regex(rgxs[i]), paste("\\1 ", longhand.units[i]," ",  sep = "")),
                   question2 = str_replace_all(question2, regex(rgxs[i]), paste("\\1 ", longhand.units[i]," ",  sep = ""))
            )
}

```
Note: in the above cleaning, I'm changing numbers followed by a space, then suspected units to numbers and longform units. For example, _10 cm_ goes to _10 centimeter_. *Come back to this* - perhaps this is too strong, and I should only change things of the form _10cm_ to _10 centimete_

#### 1.3 A look at most commonly misspelled words

```{R}
train.words %>%
      filter(!valid.spelling) %>%
      group_by(word) %>%
      summarize(n = n()) %>% 
      ungroup() %>%
      mutate(valid.spelling = hunspell_check(word)) %>%
      arrange(desc(n))
```

Some of the most common errors are appreviations like _i'm_, _u.s_ and _nyc_


```{R}
train.words %>%
      filter(!valid.spelling) %>%
      group_by(word) %>%
      summarize(n = n()) %>% 
      ungroup() %>%
      mutate(valid.spelling = hunspell_check(word)) %>%
      mutate(len = str_length(word)) %>%
      filter(len == 2) %>%
       arrange(desc(n))

```
```{R}
# two letter words I'm going to replace
spelling.errs.2 <- c(" uk ", " os ", " eu ", " gb ", " fb ", " js ", " gf")
# replacements
spelling.fixes.2 = c("england", "operating system ", "european union", "gigabyte", "facebook", "javascript", "girlfriend")

# convert shorthanded units to longhanded united values in data
for (i in 1:length(spelling.errs.2)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(spelling.errs.2[i], ignore_case = TRUE), spelling.fixes.2[i]),
                   quetison2 = str_replace_all(question2, regex(spelling.errs.2[i], ignore_case = TRUE), spelling.fixes.2[i])
            )
}
```

```{R}
train.words %>%
      filter(!valid.spelling) %>%
      group_by(word) %>%
      summarize(n = n()) %>% 
      ungroup() %>%
      mutate(valid.spelling = hunspell_check(word)) %>%
      mutate(len = str_length(word)) %>%
      filter(len == 3) %>%
       arrange(desc(n))

```
```{R}
smpl <- data.train %>% filter(str_detect(question1, " u\\.s\\.a "))
View(smpl)
```

```{R}
spelling.errs.3 <- c("( |)i\'m ", " usa ", " u\\.s\\.a ", " u\\.s\\. ", "nyc " )
spelling.fixes.3 <- c(" i am ", " america ", " america ", " america  " new york city " )

# Fix spelling errors
for (i in 1:length(spelling.errs.3)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(spelling.errs.3[i], ignore_case = TRUE), spelling.fixes.3[i]),
                   question2 = str_replace_all(question2, regex(spelling.errs.3[i], ignore_case = TRUE), spelling.fixes.3[i])
            )
}
```

#### Look at most common words with alphanumeric characters 

```{R}
train.words %>%
      filter(str_detect(word, regex("[^a-z0-9\\.]", ignore_case = TRUE))) %>%
      group_by(word) %>%
      summarise(n = n()) %>%
      arrange(desc(n))

```

THe most common words with non alphanumeric characters are words abbreviated words, like _what's_. 

I'll split these abbreviations using general rules when possible, and then consider the most important cases individually. 

First, the suffix _-n't_ translates to _ - not_. 
```{R}
data.train <- data.train %>%
      mutate(question1 = str_replace_all( question1, regex("(can\'t )"), "can not "),
             question2 = str_replace_all( question2, regex("(can\'t )"), "can not "),
             question1 = str_replace_all( question1, regex("(won\'t )"), "will not "),
             question2 = str_replace_all( question2, regex("(won\'t )"), "will not "),
            question1 = str_replace_all( question1, regex("([a-z]+)(n\'t )"), "\\1 not "),
            question2 = str_replace_all( question2, regex("([a-z]+)(n\'t )"), "\\1 not "))

```

Now, convert the suffix _-'ve_ translates to _ - have_
```{R}
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'ve )"), "\\1 have "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'ve )"), "\\1 have "))
```

Now, convert the suffix _'re_ translates to _ - are_
```{R}
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'re )"), "\\1 are "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'re )"), "\\1 are "))
```

Now, convert the suffix _'ll_ translates to _ - will_
```{R}
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'ll )"), "\\1 will "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'ll )"), "\\1 will "))


```

Now, convert the suffix _'d_ translates to _ - would_


```{R}
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'d )"), "\\1 would "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'d )"), "\\1 would "))

```


### 2. Looking at words with non-alphanumeric characters

#### 2.1 The most frequently occuring words with special characters

```{R}
train.words %>%
      filter(!is.na(str_match(word, regex("[^a-z0-9\'\\,\\.\\- ]", ignore_case = TRUE)))) %>%
      group_by(word) %>%
      summarize(n = n()) %>% 
      arrange(desc(n))

```

One of the rifrst thing I notice is that some words use *’* as an appostrophe, while others use *'*. This means that much of the processing that I have done previously will not affect the text! I'll be sure to first cast all *’* characters to *'*, then re-apply the opertations above. 

If I assume that I have fixed these cases, the most frequently occuring words with special characters are:

```{R}
train.words %>%
      filter(!is.na(str_match(word, regex("[^a-z0-9\'\\’\\,\\.\\- ]", ignore_case = TRUE)))) %>%
      group_by(word) %>%
      summarize(n = n()) %>% 
      arrange(desc(n))

```

I see that some of the most common words are those that contain vowels with accents - _pokémon_, _schrödinger's_, etc. 

I'm sure that not all users take put in the time to type out these accented vowels. Since we plan to use word embeddings, pairs of words like _pokémon_ and _pokemon_ are not mapped to the same vector, which is a loss of information. 

Thus, I will standardize the words to remove the (most commonly occuring) accents on vowels. 

```{R}
accented <- c( "ã", "á", "ä", "å", "à", "ç", "é", "è", "í", "ï",  "ì", "ö", "ô", "ó", "ò", "ð", "ü", "ú", "ù", "û", "ñ")
unaccented <- c("a", "a", "a","a", "a", "c", "e", "e", "i", "i", "i", "o", "o", "o", "o", "o", "u", "u", "u","u", "n")
```
```{R}
for (i in 1:length(accented)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, accented[i], unaccented[i]), 
                   question2 = str_replace_all(question2, accented[i], unaccented[i]))
}
```


```{R}
data.train %>%
      unnest_tokens(word, question1) %>%
      mutate(word = paste(toupper(substr(word, 1, 1)), substr(word, 2, nchar(word)), sep="")) %>%
      mutate(valid.spelling = hunspell_check(word)) %>%
      filter(!valid.spelling)%>%
      group_by(word) %>%
      summarize(n = n()) %>% 
      arrange(desc(n))

```

#### 2.2 Misclelaneous fixes

One off fixes. Some of these are inspired by this notebook: https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text

```{R}
misc.errors <- c("π ", "([1-9][0-9]*)(k) ", " e\\-mail ", " cs ", " upvotes ", " calender ",
                 " actived ", " demonitization ", " intially ", " quikly ", " programing ", " bestfriend ", 
                 " the US ")
misc.fixes <-  c(" pi ", "\\1 k ", " email ", " computer science ", " up votes ", " calendar ",
                 " active ", " demonetization ", " initialy ", " quickly ", "programming ", " best friend ", " america ")

```
```{R}
for (i in 1:length(misc.errors)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(misc.errors[i], ignore_case = TRUE),
                                               regex(misc.fixes[i], ignore_case = TRUE)),
                   question2 = str_replace_all(question2, regex(misc.errors[i], ignore_case = TRUE),
                                               regex(misc.fixes[i], ignore_case = TRUE))
                   )
}
```
```{R}
data.train %>%
      filter(is.na(question1) | is.na(question2))

```


### 3. Removing non-alphanumieric characters

For now I will not remove punctuation, as I may want to create some linguistic features out of them in the future. I will however remove all other non-ascii characters. 

Removing punctuation, though it removes symantic meaning for a human, may be helpful in removing the noise in the data. For now, I will space out all punctuation marks with whitespace characters. This way, the string _"really?"_ will turn to _"really ?"_, and so the word _really_ will be identified in the word embeddings. Otherwise we would be trying to embedd the word _really?_, which likely isn't in the lexicon. 

When building models, should experiment with this. 

```{R}
data.train <- data.train %>% 
      mutate(question1 = str_replace_all(question1, regex("[^a-z0-9\\.\\,\\-\\+\\=\'\\?]", ignore_case = TRUE), " "),
             question2 = str_replace_all(question2, regex("[^a-z0-9\\.\\,\\-\\+\\=\'\\?]", ignore_case = TRUE), " "), 
             question1 = str_replace_all(question1, regex("[ ]{2,}"), " "),
             question2 = str_replace_all(question2, regex("[ ]{2,}"), " ")) 

```


### 4. Putting it all together (in order)

I applied all the transformations in the order that I discovered their neccesity - not in the order which they should be applied. Thus the data is currupted. 

I will re-load the data, and apply every operation over again. This block should be run (or modified) to clean the data in the future. 


#### 4.1 Re-load data. 
```{R}
data.train <- read.csv("../data/train.csv")
#data.test <- read.csv("../data/test.csv")
```

#### 4.2 Replace special characters with characters you want to ultimately keep 

```{R}
# replace accented vowels with their unaccented equivalents
accented <- c( "ã", "á", "ä", "å", "à", "ç", "é", "è", "í", "ï", "ì", "ö", "ô", "ó", "ò", "ð", "ü", "ú", "ù", "û", "ñ")
unaccented <- c("a", "a", "a","a", "a", "c", "e", "e", "i", "i", "i", "o", "o", "o", "o", "o", "u", "u", "u","u", "n")

for (i in 1:length(accented)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, accented[i], unaccented[i]), 
                   question2 = str_replace_all(question2, accented[i], unaccented[i]))
}

# replace ’ with '
data.train <- data.train %>% mutate(question1 = str_replace_all(question1, "’", "'"),
                     question2 = str_replace_all(question2, "’", "'"))

# replace '&' with 'and', and '=' with 'equals
data.train <- data.train %>% mutate(question1 = str_replace_all(question1, " & ", " and "),
                     question2 = str_replace_all(question2, " & ", " and "),
                     question1 = str_replace_all(question1, "=", " equals "),
                     question2 = str_replace_all(question2, "=", " equals "))

```
#### 4.3 Fix common spelling errors and abbreviations
```{R}
# two letter words I'm going to replace
spelling.errs.2 <- c(" uk ", " os ", " eu ", " gb ", " fb ", " js ", " gf ")
# replacements
spelling.fixes.2 = c(" england ", " operating system ", " european union", " gigabyte ", " facebook ", " javascript ", " girlfriend ")

# fix common 2 letter erors 
for (i in 1:length(spelling.errs.2)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(spelling.errs.2[i], ignore_case = TRUE), spelling.fixes.2[i]),
                   question2 = str_replace_all(question2, regex(spelling.errs.2[i], ignore_case = TRUE), spelling.fixes.2[i])
            )
}
```
```{R}
# three letter words I'm going to replace
spelling.errs.3 <- c("( |)i\'m ", " usa ", " u\\.s\\.a ", " u\\.s\\. ", " nyc " )
spelling.fixes.3 <- c(" i am ", " america ", " america ", " america ", " new york city " )

# Fix spelling errors
for (i in 1:length(spelling.errs.3)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(spelling.errs.3[i], ignore_case = TRUE), spelling.fixes.3[i]),
                   question2 = str_replace_all(question2, regex(spelling.errs.3[i], ignore_case = TRUE), spelling.fixes.3[i])
            )
}
```
#### 4.3 Misc. spellling errors
```{R}
misc.errors <- c("π ", "([1-9][0-9]*)(k) ", " e\\-mail ", " cs ", " upvotes ", " calender ",
                 " actived ", " demonitization ", " intially ", " quikly ", " programing ", " bestfriend ", 
                 " the US ")
misc.fixes <-  c(" pi ", "\\1 k ", " email ", " computer science ", " up votes ", " calendar ",
                 " active ", " demonetization ", " initialy ", " quickly ", "programming ", " best friend ", " america ")

```
```{R}

for (i in 1:length(misc.errors)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(misc.errors[i], ignore_case = TRUE),
                                               regex(misc.fixes[i], ignore_case = TRUE)),
                   question2 = str_replace_all(question2, regex(misc.errors[i], ignore_case = TRUE),
                                               regex(misc.fixes[i], ignore_case = TRUE))
                   )
}
```

#### 4.4 Convert shorthand units to longhand units
```{R}
shorthand.units = c("m", "cm", "g","gm", "kg", "mg",
                     "min" , "j", "deg", "hr", "hrs", 
                    "in", "kcal", "kl", "kph", "kw", 
                     "mm", "mpg", "nm",  "sec", "secs",
                      "wk", "wks", "wpm", "yr", "yrs")

longhand.units = c("meter", "centimeter", "gram", "gram", "kilogram", "miligram", 
                   "minute", "joules", "degrees", "hour", "hours",
                   "inch", "kilocalorie", "kiloliter", "kilomters per hour", "kilowatt",
                   "milimeter", "miles per gallon", "nanometer", "second", "seconds",
                   "week", "weeks", "words per minute", "year", "years"
                   )
```
```{R}
# regexes to match in each iteration
rgxs = rep("", length(shorthand.units))
for (i in 1:length(shorthand.units)){
      rgxs[i] <- regex(paste('([0-9]+)(', shorthand.units[i], ' | ',shorthand.units[i], ' )', sep = ""), ignore_case = TRUE)
}
# convert shorthanded units to longhanded united values in data
for (i in 1:length(shorthand.units)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(rgxs[i]), paste("\\1 ", longhand.units[i]," ",  sep = "")),
                   question2 = str_replace_all(question2, regex(rgxs[i]), paste("\\1 ", longhand.units[i]," ",  sep = ""))
            )
}
```

#### 4.5 Replace common abbreviations with longhand (would've -> would have)
```{R}
# pattern: can't -> can not 
data.train <- data.train %>%
      mutate(question1 = str_replace_all( question1, regex("(can\'t )"), "can not "),
             question2 = str_replace_all( question2, regex("(can\'t )"), "can not "),
             question1 = str_replace_all( question1, regex("(won\'t )"), "will not "),
             question2 = str_replace_all( question2, regex("(won\'t )"), "will not "),
            question1 = str_replace_all( question1, regex("([a-z]+)(n\'t )"), "\\1 not "),
            question2 = str_replace_all( question2, regex("([a-z]+)(n\'t )"), "\\1 not "))

```

Now, convert the suffix _-'ve_ translates to _ - have_
```{R}
# pattern: would've -> would have 
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'ve )"), "\\1 have "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'ve )"), "\\1 have "))
```
```{R}
# pattern: you're -> you are
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'re )"), "\\1 are "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'re )"), "\\1 are "))
```
```{R}
# pattern: you'll -> you will
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'ll )"), "\\1 will "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'ll )"), "\\1 will "))
```
```{R}
# pattern: they'd -> they would
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'d )"), "\\1 would "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'d )"), "\\1 would "))

```

#### 4.6 Space out punctuation
```{R}
data.train <- data.train %>% mutate(question1 = str_replace_all(question1, "([\\.\\,\\-\\?\"])", " \\1 "), 
                      question2 = str_replace_all(question2, "([\\.\\,\\-\\?\"])", " \\1 "))


```

#### 4.7 Remove all non-alphanumeric characters

```{R}
# also remove any repeates spaces that may have come about
data.train <- data.train %>% 
      mutate(question1 = str_replace_all(question1, regex("[^a-z0-9\\.\\,\\-\\+\'\\?]", ignore_case = TRUE), " "),
             question2 = str_replace_all(question2, regex("[^a-z0-9\\.\\,\\-\\+\'\\?]", ignore_case = TRUE), " "), 
             question1 = str_replace_all(question1, regex("[ ]{2,}"), " "),
             question2 = str_replace_all(question2, regex("[ ]{2,}"), " ")) 
```

#### 5. Write  processed data to disk

```{R}
write.csv(data.train, "../data/processed/train.csv")

```





