---
title: "Exploratory data cleaning"
output:
  html_document:
    fig_height: 6
    fig_width: 8
    messages: no
    toc: true
    toc_depth: 3
    df_print: paged
---

I recently dove deep into this Kaggle competition, and when reading users' solutions, I saw that many people drew inspiration from [https://www.kaggle.com/currie32/the-importance-of-cleaning-text](Currie32's great notebook) when preprocessing the question pair data. 

Instead of re-using Currie32's code, I used the task of cleaning this dataset as an opportunity to not only consolidate the data, but also explore it and get a feeling for its contents. 

In this notebook, I explore the Quora question pair training data and uncover ways in which I could clean the data. At the end of the notebook, I run all the cleaning operations I've deemed necessary, and write the cleaned dataset to disk. 


## 0. Load data and packages
```{R}
library(dplyr) # data manipulation
library(tidytext) # Tidy tokenizing
library(hunspell) # spell checking
library(stringr) # regex's and other string stuff
```
```{R}
data.train <- read.csv("../data/train.csv")
```
```{R}
str(data.train)
```
```{R}
head(data.train)
```

## 1. Investigating the words in the questions

### 1.1 A look at the words in the training data

First, I'd like to take a look at the individual words that make up the training data questions. Getting all the individual questions in a single dataframe:

```{R}
# Get a single dataframe with all the questions.
train.questions <- data.train %>%
      mutate(qid = qid1, question = question1) %>%
      select(qid, question) %>%
      bind_rows(
            data.train %>% 
                  mutate(qid = qid2, question = question2) %>%
                  select(qid, question)
      ) %>%
      group_by(qid, question) %>%
      filter(row_number() == 1) %>%
      ungroup()
```
```{R}
head(train.questions)
```

And now looking at the individual words which compose the questions:

```{R}
# see individual words in questions
train.words <- train.questions %>%
      unnest_tokens(word, question) %>%
      mutate(valid.spelling = hunspell_check(word))
```
```{R}
train.words
```

Looking at the least commonly occuring words:

```{R}
train.words %>% group_by(word) %>% 
      summarise(n = n()) %>% 
      arrange(n)
```

We can see that there is certainly some noise in this data, including words with non-ascii symbols and numeric values.-

### 1.2 Convert units to longform

One of the first thing I noticed is that many numeric words represented united values - for example _100kg_ is equivalent to _100 kilograms_. 

I would prefer to have unitted numeric values written in this latter form, for two reasons. First, I would like consistency between each question pair - if one question uses one representation and the other uses the second, simple similarity metrics (e.gl string edit distance) will not see that _kg_ and _kilograms_ carry the same semantic meaning.

Secondly, I plan on using pre-trained neural word embeddings to get a semantic representation of the words in a question. It might be the case that certain abbreviations do not exist in some pre-trained embedding libraries. 

```{R}
# simple regex pattern for unitted numerci value
unitted <- "[0-9\\.]+[a-z]+"

# finding the words that match this regex pattern
train.words %>%
      filter(!valid.spelling) %>%
      filter(word == str_match(word, unitted)) %>%
      mutate(value = str_split_fixed(word, "[0-9\\.]+",2)[,1],
             unit = str_split_fixed(word,"[0-9\\.]+",2)[,2]) %>%
      group_by(unit) %>%
      summarize(word = first(word),
                frequency = n())

```

Some of these are actual units, while others are just words attatched to numbers. For the proper words, split them into numbers and words. 

There seem to be some actual units attatched to numbers, and typos where regular words are attatched to numbers I'll first cast the shorthand units to longhand units, and then split the numbers and words within each word into seperate words

```{R}
shorthand.units = c("m", "cm", "g","gm", "kg", "mg",
                     "min" , "j", "deg", "hr", "hrs", 
                    "in", "kcal", "kl", "kph", "kw", 
                     "mm", "mpg", "nm",  "sec", "secs",
                      "wk", "wks", "wpm", "yr", "yrs")

longhand.units = c("meter", "centimeter", "gram", "gram", "kilogram", "miligram", 
                   "minute", "joules", "degrees", "hour", "hours",
                   "inch", "kilocalorie", "kiloliter", "kilomters per hour", "kilowatt",
                   "milimeter", "miles per gallon", "nanometer", "second", "seconds",
                   "week", "weeks", "words per minute", "year", "years"
                   )
```
```{R}
data_frame(from = shorthand.units, 
           to = longhand.units)
```

Now, I replace all the shorthand units with longhand units.

```{R}
# regexes to match in each iteration
rgxs = rep("", length(shorthand.units))
for (i in 1:length(shorthand.units)){
      rgxs[i] <- regex(paste('([0-9]+)(', shorthand.units[i], ' | ',shorthand.units[i], ' )', sep = ""), ignore_case = TRUE)
}
# convert shorthanded units to longhanded united values in data
for (i in 1:length(shorthand.units)){
      .GlobalEnv$data.train <- .GlobalEnv$data.train %>%
            mutate(question1 = str_replace_all(question1, regex(rgxs[i]), paste("\\1 ", longhand.units[i]," ",  sep = "")),
                   question2 = str_replace_all(question2, regex(rgxs[i]), paste("\\1 ", longhand.units[i]," ",  sep = ""))
            )
}

```

Note: in the above cleaning, I'm changing numbers followed by a space, then suspected units to numbers and longform units. For example, _10 cm_ goes to _10 centimeter_. 

Perhaps this operation is too presumptious, since it seems more likely that a number followed by what _might_ be a unit abbreviation is not in fact a unitted value if there is a space between the number and the potential unit. Perhaps I should only change things of the form _10cm_ to _10 centimeters_.

### 1.3 A look at most commonly misspelled words

Looking at the most commonly "misspelled" words (according to the `hunspell` library):

```{R}
train.words %>%
      filter(!valid.spelling) %>%
      group_by(word) %>%
      summarize(n = n()) %>% 
      ungroup() %>%
      mutate(valid.spelling = hunspell_check(word)) %>%
      arrange(desc(n))
```

Some of the most common errors are appreviations like _i'm_, _u.s_ and _nyc_. 

Also notic that named entities - such as _google_ and _facebook_, occur rather frequently in the dataset. Perhaps in the downstream classification task, one could benefit from using some Named Entities Recognition techniques.

Now looking at the most commonly misspelled words of character length two:

```{R}
train.words %>%
      filter(!valid.spelling) %>%
      group_by(word) %>%
      summarize(n = n()) %>% 
      ungroup() %>%
      mutate(valid.spelling = hunspell_check(word)) %>%
      mutate(len = str_length(word)) %>%
      filter(len == 2) %>%
       arrange(desc(n))

```


This list includes some common abbreviations that I think are worth converting. 

```{R}
# two letter words I'm going to replace
spelling.errs.2 <- c(" uk ", " os ", " eu ", " gb ", " fb ", " js ", " gf")
# replacements
spelling.fixes.2 = c("england", "operating system ", "european union", "gigabyte", "facebook", "javascript", "girlfriend")
```
```{R}
data_frame(from = spelling.errs.2,
           to = spelling.fixes.2)
```
```{R}
# convert shorthanded units to longhanded united values in data
for (i in 1:length(spelling.errs.2)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(spelling.errs.2[i], ignore_case = TRUE), spelling.fixes.2[i]),
                   quetison2 = str_replace_all(question2, regex(spelling.errs.2[i], ignore_case = TRUE), spelling.fixes.2[i])
            )
}
```


Now looking at the most commonly misspelled words of character length three:

```{R}
train.words %>%
      filter(!valid.spelling) %>%
      group_by(word) %>%
      summarize(n = n()) %>% 
      ungroup() %>%
      mutate(valid.spelling = hunspell_check(word)) %>%
      mutate(len = str_length(word)) %>%
      filter(len == 3) %>%
       arrange(desc(n))

```

```{R}
spelling.errs.3 <- c("( |)i\'m ", " usa ", " u\\.s\\.a ", " u\\.s\\. ", "nyc " )
spelling.fixes.3 <- c(" i am ", " america ", " america ", " america" , " new york city " )
```
```{R}
data_frame(from = spelling.errs.3,
           to = spelling.fixes.3)
```
```{R}
# Fix spelling errors
for (i in 1:length(spelling.errs.3)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(spelling.errs.3[i], ignore_case = TRUE), spelling.fixes.3[i]),
                   question2 = str_replace_all(question2, regex(spelling.errs.3[i], ignore_case = TRUE), spelling.fixes.3[i])
            )
}
```


Finally, I'm interested to see what are the most commonly misspelled words that contain non-alphanumeric characters:


```{R}
train.words %>%
      filter(str_detect(word, regex("[^a-z0-9\\.]", ignore_case = TRUE))) %>%
      group_by(word) %>%
      summarise(n = n()) %>%
      arrange(desc(n))

```

The most common words with non alphanumeric characters are words abbreviated words, like _what's_. 

I'll split these abbreviations using general rules when possible, and then consider the most important cases individually. 

First, the suffix _-n't_ translates to _ - not_.


```{R}
data.train <- data.train %>%
      mutate(question1 = str_replace_all( question1, regex("(can\'t )"), "can not "),
             question2 = str_replace_all( question2, regex("(can\'t )"), "can not "),
             question1 = str_replace_all( question1, regex("(won\'t )"), "will not "),
             question2 = str_replace_all( question2, regex("(won\'t )"), "will not "),
            question1 = str_replace_all( question1, regex("([a-z]+)(n\'t )"), "\\1 not "),
            question2 = str_replace_all( question2, regex("([a-z]+)(n\'t )"), "\\1 not "))

```

Now, convert the suffix _-'ve_ translates to _ - have_


```{R}
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'ve )"), "\\1 have "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'ve )"), "\\1 have "))
```

Now, convert the suffix _'re_ translates to _ - are_


```{R}
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'re )"), "\\1 are "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'re )"), "\\1 are "))
```

Now, convert the suffix _'ll_ translates to _ - will_


```{R}
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'ll )"), "\\1 will "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'ll )"), "\\1 will "))

```


Now, convert the suffix _'d_ translates to _ - would_


```{R}
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'d )"), "\\1 would "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'d )"), "\\1 would "))

```


## 2. Looking at words with non-alphanumeric characters


Many of the problematic tokens contain non-ascii characters (perhaps symbols from other languages, emojis, etc). This section focuses on the handling of words with these characters. 


### 2.1 The most frequently occuring words with special characters

One of the first thing I notice is that some words use *’* as an appostrophe, while others use *'*. This means that much of the processing that I have done previously will not affect the text! I'll be sure to first cast all *’* characters to *'*, then re-apply the opertations above. 

If I assume that I have fixed these cases, the most frequently occuring words with special characters are:

```{R}
train.words %>%
      filter(!is.na(str_match(word, regex("[^a-z0-9\'\\’\\,\\.\\- ]", ignore_case = TRUE)))) %>%
      group_by(word) %>%
      summarize(n = n()) %>% 
      arrange(desc(n))

```

I see that some of the most common words are those that contain vowels with accents - _pokémon_, _schrödinger's_, etc. 

I'm sure that not all Quora users  take the time to type out these accented vowels. Since we plan to use word embeddings, pairs of words like _pokémon_ and _pokemon_ are not mapped to the same vector, which is a loss of information. 

Thus, I will standardize the words to remove the (most commonly occuring) accents on vowels. 

```{R}
accented <- c( "ã", "á", "ä", "å", "à", "ç", "é", "è", "í", "ï",  "ì", "ö", "ô", "ó", "ò", "ð", "ü", "ú", "ù", "û", "ñ")
unaccented <- c("a", "a", "a","a", "a", "c", "e", "e", "i", "i", "i", "o", "o", "o", "o", "o", "u", "u", "u","u", "n")
```
```{R}
data_frame(from = accented, 
           to = unaccented)
```
```{R}
for (i in 1:length(accented)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, accented[i], unaccented[i]), 
                   question2 = str_replace_all(question2, accented[i], unaccented[i]))
}
```
```{R}
data.train %>%
      unnest_tokens(word, question1) %>%
      mutate(word = paste(toupper(substr(word, 1, 1)), substr(word, 2, nchar(word)), sep="")) %>%
      mutate(valid.spelling = hunspell_check(word)) %>%
      filter(!valid.spelling)%>%
      group_by(word) %>%
      summarize(n = n()) %>% 
      arrange(desc(n))

```

### 2.2 Misclelaneous fixes

One off fixes. Some of these fixes are inspired by [https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text](Currie32's notebook). 

```{R}
misc.errors <- c("π ", "([1-9][0-9]*)(k) ", " e\\-mail ", " cs ", " upvotes ", " calender ",
                 " actived ", " demonitization ", " intially ", " quikly ", " programing ", " bestfriend ", 
                 " the US ")
misc.fixes <-  c(" pi ", "\\1 k ", " email ", " computer science ", " up votes ", " calendar ",
                 " active ", " demonetization ", " initialy ", " quickly ", "programming ", " best friend ", " america ")

```
```{R}
data_frame(from = misc.errors, 
           to = misc.fixes)
```
```{R}
for (i in 1:length(misc.errors)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(misc.errors[i], ignore_case = TRUE),
                                               regex(misc.fixes[i], ignore_case = TRUE)),
                   question2 = str_replace_all(question2, regex(misc.errors[i], ignore_case = TRUE),
                                               regex(misc.fixes[i], ignore_case = TRUE))
                   )
}
```
```{R}
data.train %>%
      filter(is.na(question1) | is.na(question2))
```


## 3. Removing non-alphanumieric characters

Now, I'll put a space before/after every punctuation mark. This way, the string _"really?"_ will turn to _"really ?"_. This will make it easier to remove/isolate punctuation in the future, if deemed necessary. 

```{R}
data.train <- data.train %>% 
      mutate(question1 = str_replace_all(question1, regex("[^a-z0-9\\.\\,\\-\\+\\=\'\\?]", ignore_case = TRUE), " "),
             question2 = str_replace_all(question2, regex("[^a-z0-9\\.\\,\\-\\+\\=\'\\?]", ignore_case = TRUE), " "), 
             question1 = str_replace_all(question1, regex("[ ]{2,}"), " "),
             question2 = str_replace_all(question2, regex("[ ]{2,}"), " ")) 

```


## 4. Putting it all together (in order)

I applied all the transformations in the order that I discovered their neccesity - not in the order which they should be applied. Thus the data is corrupted. 

I will re-load the data, and apply every operation over again. This block should be run (or modified) to clean the data in the future. 


### 4.1 Re-load data. 
```{R}
data.train <- read.csv("../data/train.csv")
```

### 4.2 Replace special characters with characters you want to ultimately keep 

```{R}
# replace accented vowels with their unaccented equivalents
accented <- c( "ã", "á", "ä", "å", "à", "ç", "é", "è", "í", "ï", "ì", "ö", "ô", "ó", "ò", "ð", "ü", "ú", "ù", "û", "ñ")
unaccented <- c("a", "a", "a","a", "a", "c", "e", "e", "i", "i", "i", "o", "o", "o", "o", "o", "u", "u", "u","u", "n")

for (i in 1:length(accented)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, accented[i], unaccented[i]), 
                   question2 = str_replace_all(question2, accented[i], unaccented[i]))
}

# replace ’ with '
data.train <- data.train %>% mutate(question1 = str_replace_all(question1, "’", "'"),
                     question2 = str_replace_all(question2, "’", "'"))

# replace '&' with 'and', and '=' with 'equals
data.train <- data.train %>% mutate(question1 = str_replace_all(question1, " & ", " and "),
                     question2 = str_replace_all(question2, " & ", " and "),
                     question1 = str_replace_all(question1, "=", " equals "),
                     question2 = str_replace_all(question2, "=", " equals "))

```

### 4.3 Fix common spelling errors and abbreviations


```{R}
# two letter words I'm going to replace
spelling.errs.2 <- c(" uk ", " os ", " eu ", " gb ", " fb ", " js ", " gf ")
# replacements
spelling.fixes.2 = c(" england ", " operating system ", " european union", " gigabyte ", " facebook ", " javascript ", " girlfriend ")

# fix common 2 letter erors 
for (i in 1:length(spelling.errs.2)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(spelling.errs.2[i], ignore_case = TRUE), spelling.fixes.2[i]),
                   question2 = str_replace_all(question2, regex(spelling.errs.2[i], ignore_case = TRUE), spelling.fixes.2[i])
            )
}
```
```{R}
# three letter words I'm going to replace
spelling.errs.3 <- c("( |)i\'m ", " usa ", " u\\.s\\.a ", " u\\.s\\. ", " nyc " )
spelling.fixes.3 <- c(" i am ", " america ", " america ", " america ", " new york city " )

# Fix spelling errors
for (i in 1:length(spelling.errs.3)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(spelling.errs.3[i], ignore_case = TRUE), spelling.fixes.3[i]),
                   question2 = str_replace_all(question2, regex(spelling.errs.3[i], ignore_case = TRUE), spelling.fixes.3[i])
            )
}
```


### 4.3 Misc. spellling errors


```{R}
misc.errors <- c("π ", "([1-9][0-9]*)(k) ", " e\\-mail ", " cs ", " upvotes ", " calender ",
                 " actived ", " demonitization ", " intially ", " quikly ", " programing ", " bestfriend ", 
                 " the US ")
misc.fixes <-  c(" pi ", "\\1 k ", " email ", " computer science ", " up votes ", " calendar ",
                 " active ", " demonetization ", " initialy ", " quickly ", "programming ", " best friend ", " america ")

```
```{R}

for (i in 1:length(misc.errors)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(misc.errors[i], ignore_case = TRUE),
                                               regex(misc.fixes[i], ignore_case = TRUE)),
                   question2 = str_replace_all(question2, regex(misc.errors[i], ignore_case = TRUE),
                                               regex(misc.fixes[i], ignore_case = TRUE))
                   )
}
```

### 4.4 Convert shorthand units to longhand units


```{R}
shorthand.units = c("m", "cm", "g","gm", "kg", "mg",
                     "min" , "j", "deg", "hr", "hrs", 
                    "in", "kcal", "kl", "kph", "kw", 
                     "mm", "mpg", "nm",  "sec", "secs",
                      "wk", "wks", "wpm", "yr", "yrs")

longhand.units = c("meter", "centimeter", "gram", "gram", "kilogram", "miligram", 
                   "minute", "joules", "degrees", "hour", "hours",
                   "inch", "kilocalorie", "kiloliter", "kilomters per hour", "kilowatt",
                   "milimeter", "miles per gallon", "nanometer", "second", "seconds",
                   "week", "weeks", "words per minute", "year", "years"
                   )
```
```{R}
# regexes to match in each iteration
rgxs = rep("", length(shorthand.units))
for (i in 1:length(shorthand.units)){
      rgxs[i] <- regex(paste('([0-9]+)(', shorthand.units[i], ' | ',shorthand.units[i], ' )', sep = ""), ignore_case = TRUE)
}
# convert shorthanded units to longhanded united values in data
for (i in 1:length(shorthand.units)){
      data.train <- data.train %>%
            mutate(question1 = str_replace_all(question1, regex(rgxs[i]), paste("\\1 ", longhand.units[i]," ",  sep = "")),
                   question2 = str_replace_all(question2, regex(rgxs[i]), paste("\\1 ", longhand.units[i]," ",  sep = ""))
            )
}
```

### 4.5 Replace common abbreviations with longhand (would've -> would have)


```{R}
# pattern: can't -> can not 
data.train <- data.train %>%
      mutate(question1 = str_replace_all( question1, regex("(can\'t )"), "can not "),
             question2 = str_replace_all( question2, regex("(can\'t )"), "can not "),
             question1 = str_replace_all( question1, regex("(won\'t )"), "will not "),
             question2 = str_replace_all( question2, regex("(won\'t )"), "will not "),
            question1 = str_replace_all( question1, regex("([a-z]+)(n\'t )"), "\\1 not "),
            question2 = str_replace_all( question2, regex("([a-z]+)(n\'t )"), "\\1 not "))

```


Now, convert the suffix _-'ve_ translates to _ - have_


```{R}
# pattern: would've -> would have 
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'ve )"), "\\1 have "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'ve )"), "\\1 have "))
```
```{R}
# pattern: you're -> you are
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'re )"), "\\1 are "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'re )"), "\\1 are "))
```
```{R}
# pattern: you'll -> you will
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'ll )"), "\\1 will "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'ll )"), "\\1 will "))
```
```{R}
# pattern: they'd -> they would
data.train <- data.train %>%
      mutate(
            question1 = str_replace_all( question1, regex("([a-z]+)(\'d )"), "\\1 would "),
            question2 = str_replace_all( question2, regex("([a-z]+)(\'d )"), "\\1 would "))

```

### 4.6 Space out punctuation


```{R}
data.train <- data.train %>% mutate(question1 = str_replace_all(question1, "([\\.\\,\\-\\?\"])", " \\1 "), 
                      question2 = str_replace_all(question2, "([\\.\\,\\-\\?\"])", " \\1 "))


```

### 4.7 Remove all non-alphanumeric characters

```{R}
# also remove any repeates spaces that may have come about
data.train <- data.train %>% 
      mutate(question1 = str_replace_all(question1, regex("[^a-z0-9\\.\\,\\-\\+\'\\?]", ignore_case = TRUE), " "),
             question2 = str_replace_all(question2, regex("[^a-z0-9\\.\\,\\-\\+\'\\?]", ignore_case = TRUE), " "), 
             question1 = str_replace_all(question1, regex("[ ]{2,}"), " "),
             question2 = str_replace_all(question2, regex("[ ]{2,}"), " "))
```

### 5. Write  processed data to disk

```{R}
# there are two empty questions. remove these, and write to disk. 
#write.csv(data.train %>% filter(question1 != '' & question2 != ''), "../data/processed/train.csv")

```





