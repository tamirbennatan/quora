{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# from nltk import word_tokenize, pos_tag\n",
    "# from nltk.corpus import stopwords\n",
    "# stops = set(stopwords.words('english'))\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# import re\n",
    "\n",
    "# import spacy\n",
    "# # Load spaCy pipeline. Disable Named Entity Recognition, since I don't need it. \n",
    "\n",
    "# nlp = spacy.load('en_core_web_lg', disable=['ner'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Average WordNet Distance\n",
    "\n",
    "[This paper]((https://www.aaai.org/Papers/AAAI/2006/AAAI06-123.pdf) describes a method for scoring the sematic difference of sentences using WordNet. \n",
    "\n",
    "The idea is a scoring scheme which looks at the average \"distance\" of the words in the two sentences.\n",
    "\n",
    "To calculate the distance between sentences $T_1$ and $T_2$, go through the words in $T_1$ and find the word which is \"closest\" to it in $T_2$. Average these distances across all the words in $T_1$. \n",
    "\n",
    "To get a symmetric scoring scheme (i.e $Similiarity(T_1,T_2) = Similarity(T_2,T_1)$, perform the same operation, but this time starting with $T_2$ and findint the most similar words in $T_1$ to each word. The paper also suggests the use of _IDF_ scores.\n",
    "\n",
    "The score can be summarized: (Mihalcea, Corley, Strapparava)\n",
    "\n",
    "$$\n",
    "Similarity(T_1,T_2) = \\frac{1}{2}( \\frac{\\sum_{w \\in T_1}MaxSimilarity(w, T_2)idf(w)}{\\sum_{w \\in T_1}idf(w)}\n",
    "+ \\frac{\\sum_{w \\in T_2}MaxSimilarity(w, T_1)idf(w)}{\\sum_{w \\in T_2}idf(w)})\n",
    "$$\n",
    "\n",
    "Where $Similarity(w, T_i)$ is the greatest similarity between a word $w$ and word in $T_i$, according to some similarity metric (with respect to WordNet). This is a heuristic for getting the semantic similarity of sentences. \n",
    "\n",
    "Note: word comparisons are limited to words of the same part of speech. Furthermore, each word is always assumed to be the most common sense of that word. \n",
    "\n",
    "As a start, I'll implement this idea without _IDF_ scores, using a variety of WordNet distance metrics. If time permits, I'll include Idf scores as well. \n",
    "\n",
    "**Note**: Much of this code is taken from [this great blog post!](http://nlpforhackers.io/wordnet-sentence-similarity/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load data to experiment with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sample = data.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_intersection</th>\n",
       "      <th>len_intersection_sw</th>\n",
       "      <th>num_words_q1</th>\n",
       "      <th>...</th>\n",
       "      <th>cosine_distance_sentence_embeddings</th>\n",
       "      <th>cityblock_distance_sentence_embeddings</th>\n",
       "      <th>jaccard_distance_sentence_embeddings</th>\n",
       "      <th>braycurtis_distance_sentence_embeddings</th>\n",
       "      <th>q1_highest_tfidf_weight</th>\n",
       "      <th>q2_highest_tfidf_weight</th>\n",
       "      <th>q1_max_tf_idf_embedding</th>\n",
       "      <th>q2_max_tf_idf_embedding</th>\n",
       "      <th>euclidean_distance_max_tfidf_word</th>\n",
       "      <th>cosine_distance_max_tfidf_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84924</th>\n",
       "      <td>84925</td>\n",
       "      <td>84924</td>\n",
       "      <td>142416</td>\n",
       "      <td>143499</td>\n",
       "      <td>What are some of the best WhatsApp statuses ?</td>\n",
       "      <td>Survey Question What is your current WhatsApp ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076075</td>\n",
       "      <td>15.394181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.194825</td>\n",
       "      <td>statuses</td>\n",
       "      <td>survey</td>\n",
       "      <td>[  3.07579994e-01   6.17260002e-02  -3.1863999...</td>\n",
       "      <td>[-0.1737      0.14329    -0.042241    0.419270...</td>\n",
       "      <td>5.472867</td>\n",
       "      <td>0.816847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284521</th>\n",
       "      <td>284522</td>\n",
       "      <td>284523</td>\n",
       "      <td>404787</td>\n",
       "      <td>379437</td>\n",
       "      <td>What would be the best African country to reti...</td>\n",
       "      <td>What are some producers in the coral reef ?</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013847</td>\n",
       "      <td>6.951705</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.086751</td>\n",
       "      <td>retire</td>\n",
       "      <td>reef</td>\n",
       "      <td>[-0.49013999 -0.10305     0.24549    -0.12166 ...</td>\n",
       "      <td>[ -4.06040013e-01  -8.87480006e-02  -3.3338001...</td>\n",
       "      <td>6.275740</td>\n",
       "      <td>0.932312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321015</th>\n",
       "      <td>321016</td>\n",
       "      <td>321017</td>\n",
       "      <td>131701</td>\n",
       "      <td>4945</td>\n",
       "      <td>Are we going to war with Russia ?</td>\n",
       "      <td>What is the likelihood of war with Russia ?</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052232</td>\n",
       "      <td>13.755184</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.165243</td>\n",
       "      <td>russia</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>[-0.33985999  0.18606    -0.23565     0.070993...</td>\n",
       "      <td>[-0.017332   -0.08987    -0.18223999  0.55179 ...</td>\n",
       "      <td>5.320340</td>\n",
       "      <td>0.861500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228957</th>\n",
       "      <td>228958</td>\n",
       "      <td>228959</td>\n",
       "      <td>338162</td>\n",
       "      <td>338163</td>\n",
       "      <td>What's the best co - working space in Munich ?</td>\n",
       "      <td>What's the best co - working space in London c...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007342</td>\n",
       "      <td>4.892683</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>munich</td>\n",
       "      <td>co</td>\n",
       "      <td>[  2.35450000e-01  -3.10259998e-01  -3.6285999...</td>\n",
       "      <td>[ 0.09969    -0.20532     0.20522     0.19621 ...</td>\n",
       "      <td>5.898361</td>\n",
       "      <td>0.869842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402249</th>\n",
       "      <td>402250</td>\n",
       "      <td>402251</td>\n",
       "      <td>535718</td>\n",
       "      <td>535719</td>\n",
       "      <td>What is the monthly income of jokewap . com ?</td>\n",
       "      <td>What is your monthly income ?</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032440</td>\n",
       "      <td>10.577873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.130851</td>\n",
       "      <td>jokewap</td>\n",
       "      <td>monthly</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...</td>\n",
       "      <td>[ 0.12704    -0.092493    0.21707     0.038711...</td>\n",
       "      <td>4.427599</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      id    qid1    qid2  \\\n",
       "84924        84925   84924  142416  143499   \n",
       "284521      284522  284523  404787  379437   \n",
       "321015      321016  321017  131701    4945   \n",
       "228957      228958  228959  338162  338163   \n",
       "402249      402250  402251  535718  535719   \n",
       "\n",
       "                                                question1  \\\n",
       "84924      What are some of the best WhatsApp statuses ?    \n",
       "284521  What would be the best African country to reti...   \n",
       "321015                 Are we going to war with Russia ?    \n",
       "228957    What's the best co - working space in Munich ?    \n",
       "402249     What is the monthly income of jokewap . com ?    \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "84924   Survey Question What is your current WhatsApp ...             0   \n",
       "284521       What are some producers in the coral reef ?              0   \n",
       "321015       What is the likelihood of war with Russia ?              1   \n",
       "228957  What's the best co - working space in London c...             0   \n",
       "402249                     What is your monthly income ?              0   \n",
       "\n",
       "        len_intersection  len_intersection_sw  num_words_q1  \\\n",
       "84924                  0                    0             8   \n",
       "284521                 2                    0            10   \n",
       "321015                 2                    1             7   \n",
       "228957                 6                    4             8   \n",
       "402249                 3                    2             8   \n",
       "\n",
       "                     ...                cosine_distance_sentence_embeddings  \\\n",
       "84924                ...                                           0.076075   \n",
       "284521               ...                                           0.013847   \n",
       "321015               ...                                           0.052232   \n",
       "228957               ...                                           0.007342   \n",
       "402249               ...                                           0.032440   \n",
       "\n",
       "        cityblock_distance_sentence_embeddings  \\\n",
       "84924                                15.394181   \n",
       "284521                                6.951705   \n",
       "321015                               13.755184   \n",
       "228957                                4.892683   \n",
       "402249                               10.577873   \n",
       "\n",
       "        jaccard_distance_sentence_embeddings  \\\n",
       "84924                                    1.0   \n",
       "284521                                   1.0   \n",
       "321015                                   1.0   \n",
       "228957                                   1.0   \n",
       "402249                                   1.0   \n",
       "\n",
       "        braycurtis_distance_sentence_embeddings  q1_highest_tfidf_weight  \\\n",
       "84924                                  0.194825                 statuses   \n",
       "284521                                 0.086751                   retire   \n",
       "321015                                 0.165243                   russia   \n",
       "228957                                 0.061728                   munich   \n",
       "402249                                 0.130851                  jokewap   \n",
       "\n",
       "        q2_highest_tfidf_weight  \\\n",
       "84924                    survey   \n",
       "284521                     reef   \n",
       "321015               likelihood   \n",
       "228957                       co   \n",
       "402249                  monthly   \n",
       "\n",
       "                                  q1_max_tf_idf_embedding  \\\n",
       "84924   [  3.07579994e-01   6.17260002e-02  -3.1863999...   \n",
       "284521  [-0.49013999 -0.10305     0.24549    -0.12166 ...   \n",
       "321015  [-0.33985999  0.18606    -0.23565     0.070993...   \n",
       "228957  [  2.35450000e-01  -3.10259998e-01  -3.6285999...   \n",
       "402249  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
       "\n",
       "                                  q2_max_tf_idf_embedding  \\\n",
       "84924   [-0.1737      0.14329    -0.042241    0.419270...   \n",
       "284521  [ -4.06040013e-01  -8.87480006e-02  -3.3338001...   \n",
       "321015  [-0.017332   -0.08987    -0.18223999  0.55179 ...   \n",
       "228957  [ 0.09969    -0.20532     0.20522     0.19621 ...   \n",
       "402249  [ 0.12704    -0.092493    0.21707     0.038711...   \n",
       "\n",
       "        euclidean_distance_max_tfidf_word  cosine_distance_max_tfidf_word  \n",
       "84924                            5.472867                        0.816847  \n",
       "284521                           6.275740                        0.932312  \n",
       "321015                           5.320340                        0.861500  \n",
       "228957                           5.898361                        0.869842  \n",
       "402249                           4.427599                             NaN  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Fixing the error in the `cosine_distance_max_tfidf_word` feature\n",
    "\n",
    "When creating this feature, I forgot to account for a division by zero error. I've been pushing off fixing this issue, as writing this file takes a while. I'll do it now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quick fix - fixed on the features notebook but did not re-save data yet\n",
    "\n",
    "fill = data[pd.isnull(data['cosine_distance_max_tfidf_word'])].apply(lambda x: 0 if\n",
    "                        np.array_equal(x['q1_max_tf_idf_embedding'], x['q2_max_tf_idf_embedding'])  \n",
    "                        else 1, axis = 1)\n",
    "\n",
    "data['cosine_distance_max_tfidf_word'] = data['cosine_distance_max_tfidf_word'].fillna(fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fill = data[pd.isnull(data['cosine_distance_sentence_embeddings'])].apply(lambda x: 0 if\n",
    "                        np.array_equal(x['q1_embedding'], x['q2_embedding'])  \n",
    "                        else 1, axis = 1)\n",
    "\n",
    "if len(fill) > 0:\n",
    "    data['cosine_distance_sentence_embeddings'] = data['cosine_distance_sentence_embeddings'].fillna(fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple POS tagging\n",
    "\n",
    "In the proposed algorithm, we should only compare words of the same POS category (nouns, verbs, adjectives or adverbs). \n",
    "\n",
    "Below is a function which produces these broad tags for tokenized words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0 Conert Penn POS tags to broad tags\n",
    "\n",
    "`pos_tag` by NLTK automatcially gives fine POS tags. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('pretty', 'JJ'),\n",
       " ('sentence', 'NN')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(word_tokenize(\"this is a pretty sentence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function brings them to more simple POS tags (courtesy of [bogdani](http://nlpforhackers.io/author/bogdani/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', None),\n",
       " ('is', 'v'),\n",
       " ('a', None),\n",
       " ('pretty', 'a'),\n",
       " ('sentence', 'n'),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test conversion\n",
    "[(word, penn_to_wn(tag)) for (word,tag) in pos_tag(word_tokenize(\"this is a pretty sentence.\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: only nounds, verbs, adjectives and adverbs are tagged. Here, the determiners and punctuations are tagged as `None`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get synsets\n",
    "\n",
    "Now we can get the synset of the most common sense of word, filtered by part of speech (again courtesy of [bogdani](http://nlpforhackers.io/author/bogdani/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the most common synset for a tagged word from WordNet. \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    "    try:\n",
    "        # get the first synset of the word in WordNet\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('pretty', 'JJ'),\n",
       " ('sentence', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "ex = pos_tag(word_tokenize(\"this is a pretty sentence.\"))\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " Synset('be.v.01'),\n",
       " None,\n",
       " Synset('pretty.s.01'),\n",
       " Synset('sentence.n.01'),\n",
       " None]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tagged_to_synset(word,tag) for (word,tag) in ex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sentence similarities\n",
    "\n",
    "Now, I'll define series of functions for finding the sentence similarity scores for pairs of sentences, using a variety of wordnet distance metrics. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0 Path similarity\n",
    "\n",
    "This score is the distance betweeen synsets in WordNet - through the hyponym/hypernym path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('word', 'NN')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pos_tag(word_tokenize(\"this is a word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def path_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "    \n",
    "    # Zip up the synsets and the words/POS tags\n",
    "    zipped1 = zip(sentence1,synsets1)\n",
    "    zipped2 = zip(sentence2,synsets2)\n",
    "    \n",
    "    # filter out the Nones\n",
    "    zipped1 = [z for z in zipped1 if z[1] is not None]\n",
    "    zipped2 = [z for z in zipped2 if z[1] is not None]\n",
    " \n",
    " \n",
    "    score1, count1, score2, count2 = 0.0, 0, 0.0, 0 \n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for tup1 in zipped1:\n",
    "        try:\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([tup1[1].path_similarity(ss[1]) for ss in zipped2 if \\\n",
    "                              penn_to_wn(ss[0][1]) == penn_to_wn(tup1[0][1])])\n",
    "        except:\n",
    "            best_score = None\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score1 += best_score\n",
    "            count1 += 1\n",
    "            \n",
    "    for tup2 in zipped2:\n",
    "        try:\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([tup2[1].path_similarity(ss[1]) for ss in zipped1 if \\\n",
    "                              penn_to_wn(ss[0][1]) == penn_to_wn(tup2[0][1])])\n",
    "        except:\n",
    "            best_score = None\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score2 += best_score\n",
    "            count2 += 1\n",
    " \n",
    "    try:\n",
    "        # Average the values and add score from both sides to get symmetic distance\n",
    "        score = .5*(score1/count1 + score2/count2)\n",
    "        return(score)\n",
    "    except:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20575396825396824"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_similarity(\"The Mona-Lisa is pretty overrated if you ask me\", \\\n",
    "                      \"I hope that it will not rain tomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7555555555555555"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_similarity(\"Cats are beautiful animals.\", \"Dogs are awesome creatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12751702662416947"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_similarity(\"Trump reverses ban on importing remains of African elephants killed as trophies\", \\\n",
    "                      \"Native American tribe bracing for Keystone pipeline leak impact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Leacock-Chodorow Similarity\n",
    "\n",
    "This similarity score is similar to the path similarity, but also incorporates the depth of the word in the WordNet taxonomy. The deeper the taxonomy depth, the higher the similarity score.\n",
    "\n",
    "The intuition is that the deeper the word are in the tree, the longer the paths between them in genereal. The second factor counters this fact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lch_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "    \n",
    "    # Zip up the synsets and the words/POS tags\n",
    "    zipped1 = zip(sentence1,synsets1)\n",
    "    zipped2 = zip(sentence2,synsets2)\n",
    "    \n",
    "    # filter out the Nones\n",
    "    zipped1 = [z for z in zipped1 if z[1] is not None]\n",
    "    zipped2 = [z for z in zipped2 if z[1] is not None]\n",
    "    \n",
    "    score1, count1, score2, count2 = 0.0, 0, 0.0, 0 \n",
    "    \n",
    "    # For each word in the first sentence\n",
    "    for tup1 in zipped1:\n",
    "        try:\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([tup1[1].lch_similarity(ss[1]) for ss in zipped2 if \\\n",
    "                              penn_to_wn(ss[0][1]) == penn_to_wn(tup1[0][1])])\n",
    "        except:\n",
    "            best_score = None\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score1 += best_score\n",
    "            count1 += 1\n",
    "            \n",
    "    for tup2 in zipped2:\n",
    "        try:\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([tup2[1].lch_similarity(ss[1]) for ss in zipped1 if \\\n",
    "                              penn_to_wn(ss[0][1]) == penn_to_wn(tup2[0][1])])\n",
    "        except:\n",
    "            best_score = None\n",
    "            \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score2 += best_score\n",
    "            count2 += 1\n",
    " \n",
    "    try:\n",
    "        # Average the values and add score from both sides to get symmetic distance\n",
    "        score = .5*(score1/count1 + score2/count2)\n",
    "        return(score)\n",
    "    except:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.42868502671942"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lch_similarity(\"Trump reverses ban on importing remains of African elephants killed as trophies\", \\\n",
    "                      \"Native American tribe bracing for Keystone pipeline leak impact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lch_similarity(data.ix[221]['question1'], data.ix[221]['question2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Wu-Palmer Similarity\n",
    "\n",
    "This similarity score is \"based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer (most specific ancestor node).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wup_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "    \n",
    "    # Zip up the synsets and the words/POS tags\n",
    "    zipped1 = zip(sentence1,synsets1)\n",
    "    zipped2 = zip(sentence2,synsets2)\n",
    "    \n",
    "    # filter out the Nones\n",
    "    zipped1 = [z for z in zipped1 if z[1] is not None]\n",
    "    zipped2 = [z for z in zipped2 if z[1] is not None]\n",
    " \n",
    "    score1, count1, score2, count2 = 0.0, 0, 0.0, 0 \n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for tup1 in zipped1:\n",
    "        try:\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([tup1[1].wup_similarity(ss[1]) for ss in zipped2 if \\\n",
    "                              penn_to_wn(ss[0][1]) == penn_to_wn(tup1[0][1])])\n",
    "        except:\n",
    "            best_score = None\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score1 += best_score\n",
    "            count1 += 1\n",
    "            \n",
    "    for tup2 in zipped2:\n",
    "        try:\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([tup2[1].wup_similarity(ss[1]) for ss in zipped1 if \\\n",
    "                              penn_to_wn(ss[0][1]) == penn_to_wn(tup2[0][1])])\n",
    "        except:\n",
    "            best_score = None\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score2 += best_score\n",
    "            count2 += 1\n",
    " \n",
    "    # Average the values and add score from both sides to get symmetic distance\n",
    "    try:\n",
    "        score = .5*(score1/count1 + score2/count2)\n",
    "        return(score)\n",
    "    except:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9553571428571429"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wup_similarity(\"Cats are beautiful animals.\", \"Dogs are awesome creatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3392857142857143"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wup_similarity(\"The Mona-Lisa is pretty overrated if you ask me\", \\\n",
    "                      \"I hope that it will not rain tomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3534215959110917"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wup_similarity(\"Trump reverses ban on importing remains of African elephants killed as trophies\", \\\n",
    "                      \"Native American tribe bracing for Keystone pipeline leak impact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7467086834733894"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wup_similarity(\"I like to study in the montreal area with my friends\", \\\n",
    "                      \"I like to study in New York with my buddies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Including Idf scores\n",
    "\n",
    "In the original paper, the similarity score was designed to incorporate Idf scores:\n",
    "\n",
    "$$\n",
    "Similarity(T_1,T_2) = \\frac{1}{2}( \\frac{\\sum_{w \\in T_1}MaxSimilarity(w, T_2)idf(w)}{\\sum_{w \\in T_1}idf(w)}\n",
    "+ \\frac{\\sum_{w \\in T_2}MaxSimilarity(w, T_1)idf(w)}{\\sum_{w \\in T_2}idf(w)})\n",
    "$$\n",
    "\n",
    "So far I've only implemented a simplifaction of this model, without Idf scores:\n",
    "\n",
    "$$\n",
    "Similarity(T_1,T_2) = \\frac{1}{2}(\\frac{( \\sum_{w \\in T_1}MaxSimilarity(w, T_2)}{|T_1|} + \n",
    "\\frac{\\sum_{w \\in T_2}MaxSimilarity(w, T_1))}{|T_2|})\n",
    "$$\n",
    "\n",
    "Now, I'll calculate Idf scores, and re-implement these similarity scores to adhere to the original paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.0 Calculating Idf Scores\n",
    "\n",
    "To calculate Idf scores, I need the document frequency of each word.\n",
    "\n",
    "To do this, I'll use Sklearns `TfidfVectorizer`, and extract the Idf dictionary fit by this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the corpus in one numpy array\n",
    "values = data['question1'].append(data['question2']).apply(lambda x: re.sub('[ ]{2,}', ' ', re.sub('[^a-z]', ' ', x.strip()))\n",
    "                                                      ).values\n",
    "# train tf-idf weights to the training quesitons\n",
    "vectorizer = TfidfVectorizer(lowercase = True)\n",
    "vectorizer.fit(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store a dictionary of Idf weights. \n",
    "vectorizer.idf_\n",
    "idf_dict =  dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Redefining the distance score functions to include Idf weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def path_similarity_idf(sentence1, sentence2, idf_dict = idf_dict):\n",
    "    \"\"\" compute the sentence path similarity using Wordnet.\n",
    "    Incude IDF scores\n",
    "    \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "    \n",
    "    # Zip up the synsets and the words/POS tags\n",
    "    zipped1 = zip(sentence1,synsets1)\n",
    "    zipped2 = zip(sentence2,synsets2)\n",
    "    \n",
    "    # filter out the Nones\n",
    "    zipped1 = [z for z in zipped1 if z[1] is not None]\n",
    "    zipped2 = [z for z in zipped2 if z[1] is not None]\n",
    "    \n",
    "\n",
    "    score1, score2, idf_sum1, idf_sum2 = 0.0, 0.0, 0.0, 0.0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for tup1 in zipped1:\n",
    "        try:\n",
    "\n",
    "            # isolate the synset object\n",
    "            synset1 = tup1[1]\n",
    "            # isolate the part of speech \n",
    "            synset1_pos = tup1[0][1]\n",
    "            # isolate the word. lowercase so that can be found in IDF dictionary.\n",
    "            word1 = tup1[0][0].lower()\n",
    "            # isolate the idf score\n",
    "            idf1 = idf_dict[word1]\n",
    "            \n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([synset1.path_similarity(ss[1]) for \\\n",
    "                              ss in zipped2 if penn_to_wn(ss[0][1]) == penn_to_wn(synset1_pos)])\n",
    "        except Exception as e:\n",
    "            best_score = None\n",
    "            idf1 = None\n",
    " \n",
    "        # Check that the similarity could have been computed. \n",
    "        # This means that the word was found in WordNet and in the IDF score dictionary.\n",
    "        if best_score is not None and idf1 is not None:\n",
    "            score1 += best_score*idf1\n",
    "            idf_sum1 += idf1\n",
    "            \n",
    "    for tup2 in zipped2:\n",
    "        try:\n",
    "            # isolate the synset object\n",
    "            synset2 = tup2[1]\n",
    "            # isolate the part of speech \n",
    "            synset2_pos = tup2[0][1]\n",
    "            # isolate the word. lowercase so that can be found in IDF dictionary.\n",
    "            word2 = tup2[0][0].lower()            # isolate the idf score\n",
    "            idf2 = idf_dict[word2]\n",
    "\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([synset2.path_similarity(ss[1]) for ss in zipped1 if \\\n",
    "                              penn_to_wn(ss[0][1]) == penn_to_wn(synset2_pos)])\n",
    "            \n",
    "        except:\n",
    "            best_score = None\n",
    "            idf2 = None\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None and idf2 is not None:\n",
    "            score2 += best_score*idf2\n",
    "            idf_sum2 += idf2\n",
    "\n",
    " \n",
    "    try:\n",
    "        # Average the values and add score from both sides to get symmetic distance\n",
    "        score = .5*(score1/(idf_sum1) + score2/(idf_sum2))\n",
    "        return(score)\n",
    "    except:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20575396825396824"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_similarity(\"The Mona-Lisa is pretty overrated if you ask me\", \\\n",
    "                      \"I hope that it will not rain tomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.197041524099238"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_similarity_idf(\"The Mona-Lisa is pretty overrated if you ask me\", \\\n",
    "                      \"I hope that it will not rain tomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7555555555555555"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_similarity(\"Cats are beautiful animals.\", \"Dogs are awesome creatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lch_similarity_idf(sentence1, sentence2, idf_dict = idf_dict):\n",
    "    \"\"\" \n",
    "    compute the sentence Leacock-Chodorow similarity using Wordnet.\n",
    "    Incude IDF scores.\n",
    "    \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "    \n",
    "    # Zip up the synsets and the words/POS tags\n",
    "    zipped1 = zip(sentence1,synsets1)\n",
    "    zipped2 = zip(sentence2,synsets2)\n",
    "    \n",
    "    # filter out the Nones\n",
    "    zipped1 = [z for z in zipped1 if z[1] is not None]\n",
    "    zipped2 = [z for z in zipped2 if z[1] is not None]\n",
    "    \n",
    "\n",
    "    score1, score2, idf_sum1, idf_sum2 = 0.0, 0.0, 0.0, 0.0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for tup1 in zipped1:\n",
    "        try:\n",
    "\n",
    "            # isolate the synset object\n",
    "            synset1 = tup1[1]\n",
    "            # isolate the part of speech \n",
    "            synset1_pos = tup1[0][1]\n",
    "            # isolate the word. lowercase so that can be found in IDF dictionary.\n",
    "            word1 = tup1[0][0].lower()\n",
    "            # isolate the idf score\n",
    "            idf1 = idf_dict[word1]\n",
    "            \n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([synset1.lch_similarity(ss[1]) for \\\n",
    "                              ss in zipped2 if penn_to_wn(ss[0][1]) == penn_to_wn(synset1_pos)])\n",
    "        except Exception as e:\n",
    "            best_score = None\n",
    "            idf1 = None\n",
    " \n",
    "        # Check that the similarity could have been computed. \n",
    "        # This means that the word was found in WordNet and in the IDF score dictionary.\n",
    "        if best_score is not None and idf1 is not None:\n",
    "            score1 += best_score*idf1\n",
    "            idf_sum1 += idf1\n",
    "            \n",
    "    for tup2 in zipped2:\n",
    "        try:\n",
    "            # isolate the synset object\n",
    "            synset2 = tup2[1]\n",
    "            # isolate the part of speech \n",
    "            synset2_pos = tup2[0][1]\n",
    "            # isolate the word. lowercase so that can be found in IDF dictionary.\n",
    "            word2 = tup2[0][0].lower()            # isolate the idf score\n",
    "            idf2 = idf_dict[word2]\n",
    "\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([synset2.lch_similarity(ss[1]) for ss in zipped1 if \\\n",
    "                              penn_to_wn(ss[0][1]) == penn_to_wn(synset2_pos)])\n",
    "            \n",
    "        except:\n",
    "            best_score = None\n",
    "            idf2 = None\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None and idf2 is not None:\n",
    "            score2 += best_score*idf2\n",
    "            idf_sum2 += idf2\n",
    "\n",
    " \n",
    "    try:\n",
    "        # Average the values and add score from both sides to get symmetic distance\n",
    "        score = .5*(score1/(idf_sum1) + score2/(idf_sum2))\n",
    "        return(score)\n",
    "    except:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.42868502671942"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lch_similarity(\"Trump reverses ban on importing remains of African elephants killed as trophies\", \\\n",
    "                      \"Native American tribe bracing for Keystone pipeline leak impact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4068721839742344"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lch_similarity_idf(\"Trump reverses ban on importing remains of African elephants killed as trophies\", \\\n",
    "                      \"Native American tribe bracing for Keystone pipeline leak impact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0597479189743826"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lch_similarity(\"Cats are beautiful animals.\", \"Dogs are awesome creatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0283650929368324"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lch_similarity_idf(\"Cats are beautiful animals.\", \"Dogs are awesome creatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wup_similarity_idf(sentence1, sentence2, idf_dict = idf_dict):\n",
    "    \"\"\" \n",
    "    compute the sentence Wu-Palmer similarity using Wordnet.\n",
    "    Incude IDF scores.\n",
    "    \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "    \n",
    "    # Zip up the synsets and the words/POS tags\n",
    "    zipped1 = zip(sentence1,synsets1)\n",
    "    zipped2 = zip(sentence2,synsets2)\n",
    "    \n",
    "    # filter out the Nones\n",
    "    zipped1 = [z for z in zipped1 if z[1] is not None]\n",
    "    zipped2 = [z for z in zipped2 if z[1] is not None]\n",
    "    \n",
    "\n",
    "    score1, score2, idf_sum1, idf_sum2 = 0.0, 0.0, 0.0, 0.0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for tup1 in zipped1:\n",
    "        try:\n",
    "\n",
    "            # isolate the synset object\n",
    "            synset1 = tup1[1]\n",
    "            # isolate the part of speech \n",
    "            synset1_pos = tup1[0][1]\n",
    "            # isolate the word. lowercase so that can be found in IDF dictionary.\n",
    "            word1 = tup1[0][0].lower()\n",
    "            # isolate the idf score\n",
    "            idf1 = idf_dict[word1]\n",
    "            \n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([synset1.wup_similarity(ss[1]) for \\\n",
    "                              ss in zipped2 if penn_to_wn(ss[0][1]) == penn_to_wn(synset1_pos)])\n",
    "        except Exception as e:\n",
    "            best_score = None\n",
    "            idf1 = None\n",
    " \n",
    "        # Check that the similarity could have been computed. \n",
    "        # This means that the word was found in WordNet and in the IDF score dictionary.\n",
    "        if best_score is not None and idf1 is not None:\n",
    "            score1 += best_score*idf1\n",
    "            idf_sum1 += idf1\n",
    "            \n",
    "    for tup2 in zipped2:\n",
    "        try:\n",
    "            # isolate the synset object\n",
    "            synset2 = tup2[1]\n",
    "            # isolate the part of speech \n",
    "            synset2_pos = tup2[0][1]\n",
    "            # isolate the word. lowercase so that can be found in IDF dictionary.\n",
    "            word2 = tup2[0][0].lower()            # isolate the idf score\n",
    "            idf2 = idf_dict[word2]\n",
    "\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([synset2.wup_similarity(ss[1]) for ss in zipped1 if \\\n",
    "                              penn_to_wn(ss[0][1]) == penn_to_wn(synset2_pos)])\n",
    "            \n",
    "        except:\n",
    "            best_score = None\n",
    "            idf2 = None\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None and idf2 is not None:\n",
    "            score2 += best_score*idf2\n",
    "            idf_sum2 += idf2\n",
    "\n",
    " \n",
    "    try:\n",
    "        # Average the values and add score from both sides to get symmetic distance\n",
    "        score = .5*(score1/(idf_sum1) + score2/(idf_sum2))\n",
    "        return(score)\n",
    "    except:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9553571428571429"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wup_similarity(\"Cats are beautiful animals.\", \"Dogs are awesome creatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94574068329858463"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wup_similarity_idf(\"Cats are beautiful animals.\", \"Dogs are awesome creatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3534215959110917"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wup_similarity(\"Trump reverses ban on importing remains of African elephants killed as trophies\", \\\n",
    "                      \"Native American tribe bracing for Keystone pipeline leak impact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36194500203740954"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wup_similarity_idf(\"Trump reverses ban on importing remains of African elephants killed as trophies\", \\\n",
    "                      \"Native American tribe bracing for Keystone pipeline leak impact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wup_similarity(\"Cats are beautiful animals.\", \"Cats are beautiful animals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wup_similarity_idf(\"Cats are beautiful animals.\", \"Cats are beautiful animals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Variant: Similarity Scores using Word Embeddings\n",
    "\n",
    "As an extension, I will define two functions which apply the same scoring scheme, but this time the similarity between sentences is defined using cosine-similarity of the Word2Vec embeddings. \n",
    "\n",
    "This time, I will relax the requirement that words must be in the same part of speech. This is because I am no longer searching WordNet - which requires a part of speech tag to specify a synset. \n",
    "\n",
    "I will remove stopwords, however, after the sentence has been tagged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embedding_similarity_score(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Pre-trained Word2Vec embeddings from spaCy. \"\"\"\n",
    "    \n",
    "    # Process text - extract POS and embeddings\n",
    "    doc1 = nlp(unicode(sentence1))\n",
    "    doc2 = nlp(unicode(sentence2))\n",
    "    \n",
    "    # Get a list of tokens, only for those tokens which are not stopwords or punctuation\n",
    "    tokens1 = [token for token in doc1 if token.text not in stops and token.pos_ != u'PUNCT']\n",
    "    tokens2 = [token for token in doc2 if token.text not in stops and token.pos_ != u'PUNCT']\n",
    "    \n",
    "    # accumulate the Cosine similiarities between vectors, and number of matched vectors. \n",
    "    score1, count1, score2, count2 = 0.0, 0, 0.0, 0 \n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for tok1 in tokens1:\n",
    "        try:\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([tok1.similarity(tok2) for tok2 in tokens2])\n",
    "        except Exception as e:\n",
    "            best_score = None\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score1 += best_score\n",
    "            count1 += 1\n",
    "            \n",
    "    for tok2 in tokens2:\n",
    "        try:\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([tok2.similarity(tok1) for tok1 in tokens1])\n",
    "        except Exception as e:\n",
    "            best_score = None\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score2 += best_score\n",
    "            count2 += 1\n",
    " \n",
    "    try:\n",
    "        # Average the values and add score from both sides to get symmetic distance\n",
    "        score = .5*(score1/count1 + score2/count2)\n",
    "        return(score)\n",
    "    except:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70759452382723487"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_score(\"Cats are beautiful animals.\", \"Dogs are awesome creatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3409438909341892"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_score(\"Trump reverses ban on importing remains of African elephants killed as trophies\", \\\n",
    "                      \"Native American tribe bracing for Keystone pipeline leak impact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999996026357019"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_score(\"Cats are beautiful animals.\", \"Cats are beautiful animals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43963620565282974"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wup_similarity(\"The goverment official is a cheat\", \"The president has committed fraud.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51942833513021469"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_score(\"The goverment official is a cheat\", \"The president has committed fraud.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24959568058451018"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_score(\"The goverment official is a cheat\", \"Cats are beautiful animals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_similarity_score_idf(sentence1, sentence2, idf_dict = idf_dict):\n",
    "    \"\"\" compute the sentence similarity using Pre-trained Word2Vec embeddings from spaCy. \"\"\"\n",
    "    \n",
    "    # Process text - extract POS and embeddings\n",
    "    doc1 = nlp(unicode(sentence1))\n",
    "    doc2 = nlp(unicode(sentence2))\n",
    "    \n",
    "    # Get a list of tokens, only for those tokens which are not stopwords or punctuation\n",
    "    tokens1 = [token for token in doc1 if token.text not in stops and token.pos_ != u'PUNCT']\n",
    "    tokens2 = [token for token in doc2 if token.text not in stops and token.pos_ != u'PUNCT']\n",
    "    \n",
    "    # accumulate the Cosine similiarities between vectors, and number of matched vectors. \n",
    "    # Weighted by the Idf score. \n",
    "    score1, idf_sum1, score2, idf_sum2 = 0.0, 0, 0.0, 0 \n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for tok1 in tokens1:\n",
    "        try:\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([tok1.similarity(tok2) for tok2 in tokens2])\n",
    "            idf1 = idf_dict[tok1.text]\n",
    "        except Exception as e:\n",
    "            best_score = None\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score1 += best_score*idf1\n",
    "            idf_sum1 += idf1\n",
    "            \n",
    "    for tok2 in tokens2:\n",
    "        try:\n",
    "            # Get the similarity value of the most similar word in the other sentence\n",
    "            best_score = max([tok2.similarity(tok1) for tok1 in tokens1])\n",
    "            idf2 = idf_dict[tok2.text]\n",
    "        except Exception as e:\n",
    "            best_score = None\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score2 += best_score*idf2\n",
    "            idf_sum2 += idf2\n",
    " \n",
    "    try:\n",
    "        # Average the values and add score from both sides to get symmetic distance\n",
    "        score = .5*(score1/idf_sum1 + score2/idf_sum2)\n",
    "        return(score)\n",
    "    except:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70759452382723487"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_score(\"Cats are beautiful animals.\", \"Dogs are awesome creatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64727334108070211"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_score_idf(\"Cats are beautiful animals.\", \"Dogs are awesome creatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999997059223567"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_score_idf(\"Cats are beautiful animals.\", \"Cats are beautiful animals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51942833513021469"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_score(\"The goverment official is a cheat\", \"The president has committed fraud.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3561063402923107"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_score_idf(\"The goverment official is a cheat\", \"The president has committed fraud.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24959568058451018"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_score(\"The goverment official is a cheat\", \"Cats are beautiful animals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24327664680203132"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_similarity_score_idf(\"The goverment official is a cheat\", \"Cats are beautiful animals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Calculate and Save \n",
    "\n",
    "Now I Apply these functions to `data`, and save the augemented dataframe. \n",
    "\n",
    "NLTK's WordNet interface is slow. This may take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30min 19s, sys: 2min 5s, total: 32min 25s\n",
      "Wall time: 1h 11min 23s\n"
     ]
    }
   ],
   "source": [
    "%time data['path_similarity'] = data.apply(lambda x : path_similarity(x['question1'],x['question2']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0023968062371378818"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data['path_similarity'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31min 51s, sys: 2min 3s, total: 33min 54s\n",
      "Wall time: 41min 19s\n"
     ]
    }
   ],
   "source": [
    "%time data['path_similarity_idf'] = data.apply(lambda x : path_similarity_idf(x['question1'],x['question2']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026293137565299982"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data['path_similarity_idf'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 51s, sys: 1min 33s, total: 31min 25s\n",
      "Wall time: 35min 22s\n"
     ]
    }
   ],
   "source": [
    "%time data['lch_similarity'] = data.apply(lambda x : lch_similarity(x['question1'],x['question2']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 1s, sys: 2min 13s, total: 41min 15s\n",
      "Wall time: 1h 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%time data['lch_similarity_idf'] = data.apply(lambda x : lch_similarity_idf(x['question1'],x['question2']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0032971545037201204"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data['lch_similarity_idf'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44min 1s, sys: 1min 33s, total: 45min 34s\n",
      "Wall time: 50min 4s\n"
     ]
    }
   ],
   "source": [
    "%time data['wup_similarity'] = data.apply(lambda x : wup_similarity(x['question1'],x['question2']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0023968062371378818"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data['wup_similarity'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45min 13s, sys: 1min 37s, total: 46min 51s\n",
      "Wall time: 54min 22s\n"
     ]
    }
   ],
   "source": [
    "%time data['wup_similarity_idf'] = data.apply(lambda x : wup_similarity_idf(x['question1'],x['question2']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026293137565299982"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data['wup_similarity_idf'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time data['embedding_similarity_score'] = data.apply(lambda x : embedding_similarity_score(x['question1'],x['question2']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time data['embedding_similarity_score_idf'] = data.apply(lambda x : embedding_similarity_score_idf(x['question1'],x['question2']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017143719328795316"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data['embedding_similarity_score_idf'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_intersection</th>\n",
       "      <th>len_intersection_sw</th>\n",
       "      <th>num_words_q1</th>\n",
       "      <th>...</th>\n",
       "      <th>q2_max_tf_idf_embedding</th>\n",
       "      <th>euclidean_distance_max_tfidf_word</th>\n",
       "      <th>cosine_distance_max_tfidf_word</th>\n",
       "      <th>path_similarity</th>\n",
       "      <th>path_similarity_idf</th>\n",
       "      <th>lch_similarity_idf</th>\n",
       "      <th>wup_similarity_idf</th>\n",
       "      <th>embedding_similarity_score_idf</th>\n",
       "      <th>lch_similarity</th>\n",
       "      <th>wup_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.028461   -0.34314999  0.23021001 -0.063865...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.250036e-08</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.929129</td>\n",
       "      <td>3.348972</td>\n",
       "      <td>0.933559</td>\n",
       "      <td>0.951548</td>\n",
       "      <td>3.326693</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor Koh - i - Noor D...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>[  3.72870006e-02  -3.17849994e-01  -2.3973000...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.103800e-08</td>\n",
       "      <td>0.582143</td>\n",
       "      <td>0.534293</td>\n",
       "      <td>2.575241</td>\n",
       "      <td>0.666331</td>\n",
       "      <td>0.581344</td>\n",
       "      <td>2.690605</td>\n",
       "      <td>0.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>[ -4.19200003e-01   5.06489992e-01  -3.4162998...</td>\n",
       "      <td>5.495713</td>\n",
       "      <td>4.284846e-01</td>\n",
       "      <td>0.574359</td>\n",
       "      <td>0.580354</td>\n",
       "      <td>2.551049</td>\n",
       "      <td>0.689253</td>\n",
       "      <td>0.772741</td>\n",
       "      <td>2.544828</td>\n",
       "      <td>0.680357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely ? How can I solv...</td>\n",
       "      <td>Find the remainder when math 23 24 math is div...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.39732999 -0.13798     0.21100999  0.237990...</td>\n",
       "      <td>4.781672</td>\n",
       "      <td>8.190252e-01</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.458929</td>\n",
       "      <td>2.206308</td>\n",
       "      <td>0.556602</td>\n",
       "      <td>0.381349</td>\n",
       "      <td>2.509163</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quickly sugar , sa...</td>\n",
       "      <td>Which fish would survive in salt water ?</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.064408   -0.073121   -0.25946999  0.15424 ...</td>\n",
       "      <td>6.219034</td>\n",
       "      <td>8.441631e-01</td>\n",
       "      <td>0.554004</td>\n",
       "      <td>0.518265</td>\n",
       "      <td>2.391333</td>\n",
       "      <td>0.653720</td>\n",
       "      <td>0.628441</td>\n",
       "      <td>2.481718</td>\n",
       "      <td>0.670232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  qid1  qid2  \\\n",
       "0           1   0     1     2   \n",
       "1           2   1     3     4   \n",
       "2           3   2     5     6   \n",
       "3           4   3     7     8   \n",
       "4           5   4     9    10   \n",
       "\n",
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor Koh - i - Noor D...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely ? How can I solv...   \n",
       "4  Which one dissolve in water quickly sugar , sa...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when math 23 24 math is div...             0   \n",
       "4          Which fish would survive in salt water ?              0   \n",
       "\n",
       "   len_intersection  len_intersection_sw  num_words_q1       ...        \\\n",
       "0                10                    5            14       ...         \n",
       "1                 2                    0            10       ...         \n",
       "2                 3                    2            14       ...         \n",
       "3                 0                    0            11       ...         \n",
       "4                 3                    2            13       ...         \n",
       "\n",
       "                             q2_max_tf_idf_embedding  \\\n",
       "0  [-0.028461   -0.34314999  0.23021001 -0.063865...   \n",
       "1  [  3.72870006e-02  -3.17849994e-01  -2.3973000...   \n",
       "2  [ -4.19200003e-01   5.06489992e-01  -3.4162998...   \n",
       "3  [-0.39732999 -0.13798     0.21100999  0.237990...   \n",
       "4  [-0.064408   -0.073121   -0.25946999  0.15424 ...   \n",
       "\n",
       "   euclidean_distance_max_tfidf_word  cosine_distance_max_tfidf_word  \\\n",
       "0                           0.000000                    3.250036e-08   \n",
       "1                           0.000000                    4.103800e-08   \n",
       "2                           5.495713                    4.284846e-01   \n",
       "3                           4.781672                    8.190252e-01   \n",
       "4                           6.219034                    8.441631e-01   \n",
       "\n",
       "   path_similarity  path_similarity_idf  lch_similarity_idf  \\\n",
       "0         0.933333             0.929129            3.348972   \n",
       "1         0.582143             0.534293            2.575241   \n",
       "2         0.574359             0.580354            2.551049   \n",
       "3         0.612500             0.458929            2.206308   \n",
       "4         0.554004             0.518265            2.391333   \n",
       "\n",
       "   wup_similarity_idf  embedding_similarity_score_idf  lch_similarity  \\\n",
       "0            0.933559                        0.951548        3.326693   \n",
       "1            0.666331                        0.581344        2.690605   \n",
       "2            0.689253                        0.772741        2.544828   \n",
       "3            0.556602                        0.381349        2.509163   \n",
       "4            0.653720                        0.628441        2.481718   \n",
       "\n",
       "   wup_similarity  \n",
       "0        0.937500  \n",
       "1        0.702500  \n",
       "2        0.680357  \n",
       "3        0.683333  \n",
       "4        0.670232  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Fill NA's in generated columns with zero\n",
    "\n",
    "Very few missing values, so this shouldn't add too much noise. Plus, if value is na, it means that there were no words that overlap of the same part-of-speach, so maybe they are indeed very dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[[u'path_similarity',\n",
    "       u'path_similarity_idf', u'lch_similarity_idf', u'wup_similarity_idf',\n",
    "       u'embedding_similarity_score_idf', u'lch_similarity',\n",
    "       u'wup_similarity']] = data[[u'path_similarity',\n",
    "       u'path_similarity_idf', u'lch_similarity_idf', u'wup_similarity_idf',\n",
    "       u'embedding_similarity_score_idf', u'lch_similarity',\n",
    "       u'wup_similarity']].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2e8a30690>"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAHlCAYAAADGGOnUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e8a30050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = data[[u'path_similarity',\n",
    "       u'path_similarity_idf', u'lch_similarity_idf', u'wup_similarity_idf',\n",
    "       u'embedding_similarity_score_idf', u'lch_similarity',\n",
    "       u'wup_similarity']].corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Write Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[[col for col in data.columns if col not in [u'q1_embedding', u'q2_embedding']]].to_csv(\"../data/features.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
