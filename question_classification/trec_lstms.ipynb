{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.text import one_hot\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Flatten\n",
    "# from keras.layers import Embedding\n",
    "# from keras.layers import LSTM\n",
    "# from keras.layers import Dropout\n",
    "# from keras.layers import Activation\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# import h5py\n",
    "\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Classification\n",
    "\n",
    "In this notebook I build a series of deep learning models that are used to classify questions based on their answer type and their detailed answer type. \n",
    "\n",
    "These models will initially be evaluated using a developement/test set, but then the entire dataset will be used for training, and the models will be evaluated by the utility of their predictions for the main downstream task - deduplicating question intent. \n",
    "\n",
    "Thank you Dr. Jason Brownlee for [this great post](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/) on how to use pretrained word embeddings!\n",
    "\n",
    "### Contents\n",
    "\n",
    "#### 0. Load `TREC` dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trec_train = pd.read_csv(\"../data/TREC/processed/train.csv\")\n",
    "trec_test = pd.read_csv(\"../data/TREC/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>extended_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>DESC:manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>ENTY:cremat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How can I find a list of celebrities' real nam...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>DESC:manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>ENTY:animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What is the full form of . com ?</td>\n",
       "      <td>ABBR</td>\n",
       "      <td>ABBR:exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>What contemptible scoundrel stole the cork fro...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM:ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>What team did baseball's St . Louis Browns bec...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM:gr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>What is the oldest profession ?</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM:title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>What are liver enzymes ?</td>\n",
       "      <td>DESC</td>\n",
       "      <td>DESC:def</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Name the scar - faced bounty hunter of The Old...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM:ind</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           question label  \\\n",
       "0           1  How did serfdom develop in and then leave Russ...  DESC   \n",
       "1           2  What films featured the character Popeye Doyle ?   ENTY   \n",
       "2           3  How can I find a list of celebrities' real nam...  DESC   \n",
       "3           4  What fowl grabs the spotlight after the Chines...  ENTY   \n",
       "4           5                  What is the full form of . com ?   ABBR   \n",
       "5           6  What contemptible scoundrel stole the cork fro...   HUM   \n",
       "6           7  What team did baseball's St . Louis Browns bec...   HUM   \n",
       "7           8                   What is the oldest profession ?    HUM   \n",
       "8           9                          What are liver enzymes ?   DESC   \n",
       "9          10  Name the scar - faced bounty hunter of The Old...   HUM   \n",
       "\n",
       "  extended_label  \n",
       "0    DESC:manner  \n",
       "1    ENTY:cremat  \n",
       "2    DESC:manner  \n",
       "3    ENTY:animal  \n",
       "4       ABBR:exp  \n",
       "5        HUM:ind  \n",
       "6         HUM:gr  \n",
       "7      HUM:title  \n",
       "8       DESC:def  \n",
       "9        HUM:ind  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Shuffle TREC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trec_train = trec_train.sample(frac=1.0, random_state = 550)\n",
    "trec_test = trec_test.sample(frac = 1.0, random_state = 550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Prepare document encoding \n",
    "\n",
    "Using Kera's `Tokenizer` class, create a dictionary of all the types in both the quora and the TREC datasets. Encode each document as a vector of indecies of the corresponding types in the dictionary. \n",
    "\n",
    "#### 1.1  `Quora` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_quora = pd.read_csv(\"../data/processed/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor Koh - i - Noor D...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely ? How can I solv...</td>\n",
       "      <td>Find the remainder when math 23 24 math is div...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quickly sugar , sa...</td>\n",
       "      <td>Which fish would survive in salt water ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  qid1  qid2  \\\n",
       "0           1   0     1     2   \n",
       "1           2   1     3     4   \n",
       "2           3   2     5     6   \n",
       "3           4   3     7     8   \n",
       "4           5   4     9    10   \n",
       "\n",
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor Koh - i - Noor D...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely ? How can I solv...   \n",
       "4  Which one dissolve in water quickly sugar , sa...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when math 23 24 math is div...             0  \n",
       "4          Which fish would survive in salt water ?              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_quora.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Get all the questions, from both datasets, in Numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_questions = trec_train['question'].append(trec_test['question']).append(\n",
    "    train_quora['question1']).append(train_quora['question2']).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who comprised the now - defunct comic book team known as the Champions ? '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare tokenizer \n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for all the quesions in the joint datasets\n",
    "tokenizer.fit_on_texts(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93261"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of types in the joint datset\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# integer encode the questions\n",
    "encoded_questions = tokenizer.texts_to_sequences(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who comprised the now - defunct comic book team known as the Champions ? \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[39, 28303, 1, 165, 35455, 3838, 161, 765, 582, 46, 1, 7202]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of a question integer embedding\n",
    "print(all_questions[0])\n",
    "encoded_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the wingspan of a condor ? \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 54958, 10, 6, 41954]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example. Notice how the word `how` is consistently endoded as the integer 5. \n",
    "print(all_questions[2])\n",
    "encoded_questions[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Save the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../models/tokenizer.pickle', 'wb') as handle:\n",
    "    pkl.dump(tokenizer, handle, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Prepare the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the pretrained fasttext embeddings (this takes a while)\n",
    "embedding_model = KeyedVectors.load_word2vec_format('../data/embeddings/wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each row in the matrix is the embedding of one word in the joint datasets. \n",
    "# The row index corresponds to the integer ecoding of that word. \n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in embedding_model:\n",
    "        embedding_matrix[i] = embedding_model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. An LSTM experiment parameter class\n",
    "\n",
    "I will likely exeperiment with many different models, with many different hyperparameters. It will be useful to keep all those parameters contained in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LstmParams(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 sequence_length = 10, \n",
    "                 labels = \"basic\"):\n",
    "        \n",
    "        # Number of tokens to include in each sequence\n",
    "        self.sequence_length = sequence_length\n",
    "        # whether to use basic or extended labels\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Keep track of the raw training and test questions\n",
    "        self.train_questions = trec_train['question'].values\n",
    "        self.test_questions = trec_test['question'].values\n",
    "        \n",
    "        # Encode the training and test quetions\n",
    "        self.train_questions_encoded = tokenizer.texts_to_sequences(self.train_questions)\n",
    "        self.test_questions_encoded = tokenizer.texts_to_sequences(self.test_questions)\n",
    "        \n",
    "        # Store a label encode for each class. This will be particular to label type of the problem\n",
    "        self.label_encoder = self.get_encoder()\n",
    "        \n",
    "       \n",
    "    \n",
    "    '''\n",
    "    Fit a label encoder to the data, to represent the labels as one-hot vectors\n",
    "    The dimension of these vectors will depend on if the user decided to predict\n",
    "    basic or extended labels, which have 6 and 50 potential categories, respectively. \n",
    "    '''\n",
    "    def get_encoder(self):\n",
    "        # get all the labels, train and test sets, in one list\n",
    "        all_labels = self.get_train_labels().tolist() + self.get_test_labels().tolist()\n",
    "        # fit a label encoder to those labelse\n",
    "        encoder = LabelEncoder().fit(np.array(all_labels))\n",
    "        return(encoder)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Decode one-hot representation of labels back to regular labels.\n",
    "    input: an array of one-hot label arrays. \n",
    "    outpu: an array of regular character labels. \n",
    "    '''\n",
    "    def decode_labels(self,onehot_labels):\n",
    "        # first, get the labels as integers\n",
    "        integer_labels = [np.where(r==1)[0][0] for r in onehot_labels]\n",
    "        # now, return the decoded label\n",
    "        return(self.label_encoder.inverse_transform(integer_labels))\n",
    "    \n",
    "    '''\n",
    "    Return the labels as a numpy array of strings \n",
    "    Which labels to return depends on the objects 'labels' parameter. \n",
    "    '''\n",
    "    def get_train_labels(self):\n",
    "        if self.labels == \"basic\":\n",
    "            return(trec_train['label'].values)\n",
    "        elif self.labels == \"extended\":\n",
    "            return(trec_train[\"extended_label\"].values)\n",
    "        else:\n",
    "            print(\"Invalid `labels` parameter '%s'. Returning basic labels.\") % (self.labels)\n",
    "            return(trec_train['label'].values)\n",
    "        \n",
    "    def get_test_labels(self):\n",
    "        if self.labels == \"basic\":\n",
    "            return(trec_test['label'].values)\n",
    "        elif self.labels == \"extended\":\n",
    "            return(trec_test[\"extended_label\"].values)\n",
    "        else:\n",
    "            print(\"Invalid `labels` parameter '%s'. Returning basic labels.\") % (self.labels)\n",
    "            return(trec_test['label'].values)\n",
    "        \n",
    "\n",
    "    '''\n",
    "    Return the labels of the as a numpy ndarray, using one-hot encoding. \n",
    "    This is for transparency in my Neural Network archetecture. \n",
    "    '''\n",
    "    def get_train_labels_onehot(self):\n",
    "        return(to_categorical(self.label_encoder.transform(self.get_train_labels())))\n",
    "\n",
    "        \n",
    "    def get_test_labels_onehot(self):\n",
    "        return(to_categorical(self.label_encoder.transform(self.get_test_labels())))\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    Return the encoded questions after padding. \n",
    "    Padding (or truncating) amount depends on attribute `self.sequence_length`\n",
    "    '''\n",
    "    def get_train_padded(self):\n",
    "        padded = pad_sequences(self.train_questions_encoded, \n",
    "                              maxlen = self.sequence_length,\n",
    "                              padding = \"post\", \n",
    "                              truncating = \"post\")\n",
    "        return(padded)\n",
    "    \n",
    "    def get_test_padded(self):\n",
    "        padded = pad_sequences(self.test_questions_encoded, \n",
    "                              maxlen = self.sequence_length,\n",
    "                              padding = \"post\", \n",
    "                              truncating = \"post\")\n",
    "        return(padded)\n",
    "    \n",
    "    '''\n",
    "    Get the number of classes (output layer dimension). \n",
    "    This is the number of unique classes. \n",
    "    '''\n",
    "    def get_num_classes(self):\n",
    "        n_unique = len(np.unique(self.get_test_labels().tolist() + self.get_train_labels().tolist()))\n",
    "        return(n_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 4. A first model for the basic labels\n",
    "\n",
    "First, I'll try the most basic vanilla LSTM for the TREC classification problem (simple labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.0 Initialize Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params1 = LstmParams(sequence_length = 10,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Add layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model1.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params1.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model1.add(LSTM(params1.get_num_classes(), activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 300)           27978300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 6)                 7368      \n",
      "=================================================================\n",
      "Total params: 27,985,668\n",
      "Trainable params: 7,368\n",
      "Non-trainable params: 27,978,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5452/5452 [==============================] - 4s 725us/step - loss: 1.7305 - acc: 0.2500\n",
      "Epoch 2/200\n",
      "5452/5452 [==============================] - 2s 441us/step - loss: 1.6726 - acc: 0.2705\n",
      "Epoch 3/200\n",
      "5452/5452 [==============================] - 4s 660us/step - loss: 1.6501 - acc: 0.2977\n",
      "Epoch 4/200\n",
      "5452/5452 [==============================] - 3s 544us/step - loss: 1.6371 - acc: 0.3045\n",
      "Epoch 5/200\n",
      "5452/5452 [==============================] - 3s 465us/step - loss: 1.6254 - acc: 0.3100\n",
      "Epoch 6/200\n",
      "5452/5452 [==============================] - 2s 438us/step - loss: 1.6100 - acc: 0.3247\n",
      "Epoch 7/200\n",
      "5452/5452 [==============================] - 3s 546us/step - loss: 1.5794 - acc: 0.3474\n",
      "Epoch 8/200\n",
      "5452/5452 [==============================] - 3s 477us/step - loss: 1.5340 - acc: 0.4021\n",
      "Epoch 9/200\n",
      "5452/5452 [==============================] - 3s 489us/step - loss: 1.4622 - acc: 0.4563\n",
      "Epoch 10/200\n",
      "5452/5452 [==============================] - 3s 568us/step - loss: 1.3931 - acc: 0.4780\n",
      "Epoch 11/200\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 1.3412 - acc: 0.4963\n",
      "Epoch 12/200\n",
      "5452/5452 [==============================] - 3s 619us/step - loss: 1.2919 - acc: 0.5147\n",
      "Epoch 13/200\n",
      "5452/5452 [==============================] - 4s 654us/step - loss: 1.2435 - acc: 0.5345\n",
      "Epoch 14/200\n",
      "5452/5452 [==============================] - 3s 570us/step - loss: 1.1967 - acc: 0.5493\n",
      "Epoch 15/200\n",
      "5452/5452 [==============================] - 3s 496us/step - loss: 1.1543 - acc: 0.5647\n",
      "Epoch 16/200\n",
      "5452/5452 [==============================] - 3s 510us/step - loss: 1.1159 - acc: 0.5829\n",
      "Epoch 17/200\n",
      "5452/5452 [==============================] - 3s 560us/step - loss: 1.0806 - acc: 0.5992\n",
      "Epoch 18/200\n",
      "5452/5452 [==============================] - 3s 550us/step - loss: 1.0471 - acc: 0.6220\n",
      "Epoch 19/200\n",
      "5452/5452 [==============================] - 3s 580us/step - loss: 1.0154 - acc: 0.6381\n",
      "Epoch 20/200\n",
      "5452/5452 [==============================] - 3s 494us/step - loss: 0.9849 - acc: 0.6555\n",
      "Epoch 21/200\n",
      "5452/5452 [==============================] - 3s 488us/step - loss: 0.9569 - acc: 0.6638\n",
      "Epoch 22/200\n",
      "5452/5452 [==============================] - 3s 465us/step - loss: 0.9301 - acc: 0.6794\n",
      "Epoch 23/200\n",
      "5452/5452 [==============================] - 3s 487us/step - loss: 0.9052 - acc: 0.6882\n",
      "Epoch 24/200\n",
      "5452/5452 [==============================] - 2s 455us/step - loss: 0.8835 - acc: 0.6975\n",
      "Epoch 25/200\n",
      "5452/5452 [==============================] - 3s 464us/step - loss: 0.8615 - acc: 0.7089\n",
      "Epoch 26/200\n",
      "5452/5452 [==============================] - 2s 453us/step - loss: 0.8436 - acc: 0.7100\n",
      "Epoch 27/200\n",
      "5452/5452 [==============================] - 3s 494us/step - loss: 0.8256 - acc: 0.7188\n",
      "Epoch 28/200\n",
      "5452/5452 [==============================] - 2s 439us/step - loss: 0.8087 - acc: 0.7243\n",
      "Epoch 29/200\n",
      "5452/5452 [==============================] - 3s 460us/step - loss: 0.7940 - acc: 0.7284\n",
      "Epoch 30/200\n",
      "5452/5452 [==============================] - 3s 468us/step - loss: 0.7807 - acc: 0.7337\n",
      "Epoch 31/200\n",
      "5452/5452 [==============================] - 2s 451us/step - loss: 0.7685 - acc: 0.7370\n",
      "Epoch 32/200\n",
      "5452/5452 [==============================] - 2s 455us/step - loss: 0.7566 - acc: 0.7406\n",
      "Epoch 33/200\n",
      "5452/5452 [==============================] - 2s 453us/step - loss: 0.7459 - acc: 0.7434\n",
      "Epoch 34/200\n",
      "5452/5452 [==============================] - 3s 480us/step - loss: 0.7372 - acc: 0.7461\n",
      "Epoch 35/200\n",
      "5452/5452 [==============================] - 3s 470us/step - loss: 0.7275 - acc: 0.7500\n",
      "Epoch 36/200\n",
      "5452/5452 [==============================] - 2s 436us/step - loss: 0.7190 - acc: 0.7520\n",
      "Epoch 37/200\n",
      "5452/5452 [==============================] - 3s 471us/step - loss: 0.7106 - acc: 0.7555\n",
      "Epoch 38/200\n",
      "5452/5452 [==============================] - 3s 468us/step - loss: 0.7031 - acc: 0.7572\n",
      "Epoch 39/200\n",
      "5452/5452 [==============================] - 3s 470us/step - loss: 0.6949 - acc: 0.7599\n",
      "Epoch 40/200\n",
      "5452/5452 [==============================] - 3s 460us/step - loss: 0.6886 - acc: 0.7636\n",
      "Epoch 41/200\n",
      "5452/5452 [==============================] - 3s 471us/step - loss: 0.6817 - acc: 0.7656\n",
      "Epoch 42/200\n",
      "5452/5452 [==============================] - 2s 443us/step - loss: 0.6767 - acc: 0.7691\n",
      "Epoch 43/200\n",
      "5452/5452 [==============================] - 2s 396us/step - loss: 0.6695 - acc: 0.7715\n",
      "Epoch 44/200\n",
      "5452/5452 [==============================] - 2s 442us/step - loss: 0.6648 - acc: 0.7707\n",
      "Epoch 45/200\n",
      "5452/5452 [==============================] - 2s 433us/step - loss: 0.6585 - acc: 0.7757\n",
      "Epoch 46/200\n",
      "5452/5452 [==============================] - 2s 447us/step - loss: 0.6525 - acc: 0.7762\n",
      "Epoch 47/200\n",
      "5452/5452 [==============================] - 2s 420us/step - loss: 0.6481 - acc: 0.7814\n",
      "Epoch 48/200\n",
      "5452/5452 [==============================] - 2s 409us/step - loss: 0.6428 - acc: 0.7797\n",
      "Epoch 49/200\n",
      "5452/5452 [==============================] - 2s 394us/step - loss: 0.6381 - acc: 0.7821\n",
      "Epoch 50/200\n",
      "5452/5452 [==============================] - 2s 443us/step - loss: 0.6330 - acc: 0.7883\n",
      "Epoch 51/200\n",
      "5452/5452 [==============================] - 3s 520us/step - loss: 0.6288 - acc: 0.7880\n",
      "Epoch 52/200\n",
      "5452/5452 [==============================] - 3s 480us/step - loss: 0.6237 - acc: 0.7891\n",
      "Epoch 53/200\n",
      "5452/5452 [==============================] - 3s 477us/step - loss: 0.6186 - acc: 0.7931\n",
      "Epoch 54/200\n",
      "5452/5452 [==============================] - 2s 425us/step - loss: 0.6154 - acc: 0.7929\n",
      "Epoch 55/200\n",
      "5452/5452 [==============================] - 3s 507us/step - loss: 0.6109 - acc: 0.7951\n",
      "Epoch 56/200\n",
      "5452/5452 [==============================] - 2s 440us/step - loss: 0.6067 - acc: 0.7975\n",
      "Epoch 57/200\n",
      "5452/5452 [==============================] - 3s 484us/step - loss: 0.6040 - acc: 0.7979\n",
      "Epoch 58/200\n",
      "5452/5452 [==============================] - 2s 445us/step - loss: 0.6000 - acc: 0.8012\n",
      "Epoch 59/200\n",
      "5452/5452 [==============================] - 2s 441us/step - loss: 0.5960 - acc: 0.8028\n",
      "Epoch 60/200\n",
      "5452/5452 [==============================] - 2s 408us/step - loss: 0.5914 - acc: 0.8026\n",
      "Epoch 61/200\n",
      "5452/5452 [==============================] - 2s 425us/step - loss: 0.5887 - acc: 0.8047\n",
      "Epoch 62/200\n",
      "5452/5452 [==============================] - 3s 463us/step - loss: 0.5850 - acc: 0.8076\n",
      "Epoch 63/200\n",
      "5452/5452 [==============================] - 3s 462us/step - loss: 0.5835 - acc: 0.8059\n",
      "Epoch 64/200\n",
      "5452/5452 [==============================] - 2s 443us/step - loss: 0.5798 - acc: 0.8076\n",
      "Epoch 65/200\n",
      "5452/5452 [==============================] - 2s 445us/step - loss: 0.5770 - acc: 0.8109\n",
      "Epoch 66/200\n",
      "5452/5452 [==============================] - 2s 432us/step - loss: 0.5734 - acc: 0.8096\n",
      "Epoch 67/200\n",
      "5452/5452 [==============================] - 3s 471us/step - loss: 0.5698 - acc: 0.8113\n",
      "Epoch 68/200\n",
      "5452/5452 [==============================] - 3s 463us/step - loss: 0.5670 - acc: 0.8111\n",
      "Epoch 69/200\n",
      "5452/5452 [==============================] - 2s 444us/step - loss: 0.5642 - acc: 0.8146\n",
      "Epoch 70/200\n",
      "5452/5452 [==============================] - 3s 476us/step - loss: 0.5616 - acc: 0.8146\n",
      "Epoch 71/200\n",
      "5452/5452 [==============================] - 2s 439us/step - loss: 0.5584 - acc: 0.8173\n",
      "Epoch 72/200\n",
      "5452/5452 [==============================] - 2s 447us/step - loss: 0.5560 - acc: 0.8169\n",
      "Epoch 73/200\n",
      "5452/5452 [==============================] - 2s 444us/step - loss: 0.5543 - acc: 0.8184\n",
      "Epoch 74/200\n",
      "5452/5452 [==============================] - 2s 446us/step - loss: 0.5508 - acc: 0.8169\n",
      "Epoch 75/200\n",
      "5452/5452 [==============================] - 3s 486us/step - loss: 0.5483 - acc: 0.8225\n",
      "Epoch 76/200\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 0.5458 - acc: 0.8197\n",
      "Epoch 77/200\n",
      "5452/5452 [==============================] - 3s 523us/step - loss: 0.5431 - acc: 0.8212\n",
      "Epoch 78/200\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 0.5409 - acc: 0.8230\n",
      "Epoch 79/200\n",
      "5452/5452 [==============================] - 3s 513us/step - loss: 0.5390 - acc: 0.8210\n",
      "Epoch 80/200\n",
      "5452/5452 [==============================] - 3s 472us/step - loss: 0.5363 - acc: 0.8239\n",
      "Epoch 81/200\n",
      "5452/5452 [==============================] - 3s 467us/step - loss: 0.5340 - acc: 0.8248\n",
      "Epoch 82/200\n",
      "5452/5452 [==============================] - 3s 479us/step - loss: 0.5324 - acc: 0.8256\n",
      "Epoch 83/200\n",
      "5452/5452 [==============================] - 2s 401us/step - loss: 0.5289 - acc: 0.8258\n",
      "Epoch 84/200\n",
      "5452/5452 [==============================] - 2s 428us/step - loss: 0.5279 - acc: 0.8292\n",
      "Epoch 85/200\n",
      "5452/5452 [==============================] - 2s 370us/step - loss: 0.5255 - acc: 0.8276\n",
      "Epoch 86/200\n",
      "5452/5452 [==============================] - 2s 425us/step - loss: 0.5226 - acc: 0.8298\n",
      "Epoch 87/200\n",
      "5452/5452 [==============================] - 2s 408us/step - loss: 0.5212 - acc: 0.8283\n",
      "Epoch 88/200\n",
      "5452/5452 [==============================] - 2s 424us/step - loss: 0.5197 - acc: 0.8292\n",
      "Epoch 89/200\n",
      "5452/5452 [==============================] - 2s 447us/step - loss: 0.5170 - acc: 0.8303\n",
      "Epoch 90/200\n",
      "5452/5452 [==============================] - 2s 401us/step - loss: 0.5146 - acc: 0.8314\n",
      "Epoch 91/200\n",
      "5452/5452 [==============================] - 2s 405us/step - loss: 0.5136 - acc: 0.8329\n",
      "Epoch 92/200\n",
      "5452/5452 [==============================] - 2s 403us/step - loss: 0.5113 - acc: 0.8331\n",
      "Epoch 93/200\n",
      "5452/5452 [==============================] - 2s 428us/step - loss: 0.5092 - acc: 0.8336\n",
      "Epoch 94/200\n",
      "5452/5452 [==============================] - 2s 406us/step - loss: 0.5072 - acc: 0.8322\n",
      "Epoch 95/200\n",
      "5452/5452 [==============================] - 2s 410us/step - loss: 0.5046 - acc: 0.8368\n",
      "Epoch 96/200\n",
      "5452/5452 [==============================] - 2s 417us/step - loss: 0.5044 - acc: 0.8369\n",
      "Epoch 97/200\n",
      "5452/5452 [==============================] - 2s 422us/step - loss: 0.5014 - acc: 0.8371\n",
      "Epoch 98/200\n",
      "5452/5452 [==============================] - 2s 432us/step - loss: 0.5009 - acc: 0.8369\n",
      "Epoch 99/200\n",
      "5452/5452 [==============================] - 2s 416us/step - loss: 0.4993 - acc: 0.8366\n",
      "Epoch 100/200\n",
      "5452/5452 [==============================] - 2s 428us/step - loss: 0.4966 - acc: 0.8406\n",
      "Epoch 101/200\n",
      "5452/5452 [==============================] - 2s 402us/step - loss: 0.4954 - acc: 0.8388\n",
      "Epoch 102/200\n",
      "5452/5452 [==============================] - 2s 373us/step - loss: 0.4925 - acc: 0.8401\n",
      "Epoch 103/200\n",
      "5452/5452 [==============================] - 2s 415us/step - loss: 0.4908 - acc: 0.8408\n",
      "Epoch 104/200\n",
      "5452/5452 [==============================] - 2s 423us/step - loss: 0.4900 - acc: 0.8404\n",
      "Epoch 105/200\n",
      "5452/5452 [==============================] - 2s 413us/step - loss: 0.4867 - acc: 0.8432\n",
      "Epoch 106/200\n",
      "5452/5452 [==============================] - 2s 412us/step - loss: 0.4864 - acc: 0.8445\n",
      "Epoch 107/200\n",
      "5452/5452 [==============================] - 2s 406us/step - loss: 0.4863 - acc: 0.8417\n",
      "Epoch 108/200\n",
      "5452/5452 [==============================] - 2s 426us/step - loss: 0.4830 - acc: 0.8441\n",
      "Epoch 109/200\n",
      "5452/5452 [==============================] - 2s 430us/step - loss: 0.4811 - acc: 0.8430\n",
      "Epoch 110/200\n",
      "5452/5452 [==============================] - 2s 422us/step - loss: 0.4788 - acc: 0.8448\n",
      "Epoch 111/200\n",
      "5452/5452 [==============================] - 2s 413us/step - loss: 0.4761 - acc: 0.8434\n",
      "Epoch 112/200\n",
      "5452/5452 [==============================] - 3s 480us/step - loss: 0.4742 - acc: 0.8459\n",
      "Epoch 113/200\n",
      "5452/5452 [==============================] - 3s 481us/step - loss: 0.4723 - acc: 0.8479\n",
      "Epoch 114/200\n",
      "5452/5452 [==============================] - 3s 489us/step - loss: 0.4704 - acc: 0.8461\n",
      "Epoch 115/200\n",
      "5452/5452 [==============================] - 2s 456us/step - loss: 0.4685 - acc: 0.8467\n",
      "Epoch 116/200\n",
      "5452/5452 [==============================] - 2s 403us/step - loss: 0.4671 - acc: 0.8474\n",
      "Epoch 117/200\n",
      "5452/5452 [==============================] - 2s 424us/step - loss: 0.4651 - acc: 0.8468\n",
      "Epoch 118/200\n",
      "5452/5452 [==============================] - 2s 403us/step - loss: 0.4632 - acc: 0.8479\n",
      "Epoch 119/200\n",
      "5452/5452 [==============================] - 2s 388us/step - loss: 0.4624 - acc: 0.8498\n",
      "Epoch 120/200\n",
      "5452/5452 [==============================] - 2s 441us/step - loss: 0.4598 - acc: 0.8478\n",
      "Epoch 121/200\n",
      "5452/5452 [==============================] - 2s 441us/step - loss: 0.4576 - acc: 0.8496\n",
      "Epoch 122/200\n",
      "5452/5452 [==============================] - 2s 434us/step - loss: 0.4569 - acc: 0.8501\n",
      "Epoch 123/200\n",
      "5452/5452 [==============================] - 2s 436us/step - loss: 0.4550 - acc: 0.8505\n",
      "Epoch 124/200\n",
      "5452/5452 [==============================] - 2s 430us/step - loss: 0.4541 - acc: 0.8490\n",
      "Epoch 125/200\n",
      "5452/5452 [==============================] - 2s 453us/step - loss: 0.4517 - acc: 0.8501\n",
      "Epoch 126/200\n",
      "5452/5452 [==============================] - 2s 430us/step - loss: 0.4508 - acc: 0.8509\n",
      "Epoch 127/200\n",
      "5452/5452 [==============================] - 2s 419us/step - loss: 0.4501 - acc: 0.8514\n",
      "Epoch 128/200\n",
      "5452/5452 [==============================] - 2s 444us/step - loss: 0.4483 - acc: 0.8533\n",
      "Epoch 129/200\n",
      "5452/5452 [==============================] - 3s 466us/step - loss: 0.4455 - acc: 0.8536\n",
      "Epoch 130/200\n",
      "5452/5452 [==============================] - 2s 402us/step - loss: 0.4444 - acc: 0.8540\n",
      "Epoch 131/200\n",
      "5452/5452 [==============================] - 2s 406us/step - loss: 0.4431 - acc: 0.8540\n",
      "Epoch 132/200\n",
      "5452/5452 [==============================] - 2s 398us/step - loss: 0.4407 - acc: 0.8562\n",
      "Epoch 133/200\n",
      "5452/5452 [==============================] - 2s 392us/step - loss: 0.4393 - acc: 0.8538\n",
      "Epoch 134/200\n",
      "5452/5452 [==============================] - 2s 392us/step - loss: 0.4363 - acc: 0.8551\n",
      "Epoch 135/200\n",
      "5452/5452 [==============================] - 2s 387us/step - loss: 0.4336 - acc: 0.8556\n",
      "Epoch 136/200\n",
      "5452/5452 [==============================] - 2s 385us/step - loss: 0.4319 - acc: 0.8564\n",
      "Epoch 137/200\n",
      "5452/5452 [==============================] - 2s 393us/step - loss: 0.4313 - acc: 0.8579\n",
      "Epoch 138/200\n",
      "5452/5452 [==============================] - 2s 399us/step - loss: 0.4286 - acc: 0.8606\n",
      "Epoch 139/200\n",
      "5452/5452 [==============================] - 2s 420us/step - loss: 0.4270 - acc: 0.8595\n",
      "Epoch 140/200\n",
      "5452/5452 [==============================] - 2s 401us/step - loss: 0.4250 - acc: 0.8602\n",
      "Epoch 141/200\n",
      "5452/5452 [==============================] - 2s 418us/step - loss: 0.4246 - acc: 0.8591\n",
      "Epoch 142/200\n",
      "5452/5452 [==============================] - 2s 456us/step - loss: 0.4228 - acc: 0.8591\n",
      "Epoch 143/200\n",
      "5452/5452 [==============================] - 2s 391us/step - loss: 0.4216 - acc: 0.8602\n",
      "Epoch 144/200\n",
      "5452/5452 [==============================] - 2s 416us/step - loss: 0.4203 - acc: 0.8608\n",
      "Epoch 145/200\n",
      "5452/5452 [==============================] - 2s 395us/step - loss: 0.4201 - acc: 0.8604\n",
      "Epoch 146/200\n",
      "5452/5452 [==============================] - 2s 406us/step - loss: 0.4175 - acc: 0.8641\n",
      "Epoch 147/200\n",
      "5452/5452 [==============================] - 2s 415us/step - loss: 0.4167 - acc: 0.8610\n",
      "Epoch 148/200\n",
      "5452/5452 [==============================] - 2s 387us/step - loss: 0.4145 - acc: 0.8632\n",
      "Epoch 149/200\n",
      "5452/5452 [==============================] - 2s 386us/step - loss: 0.4134 - acc: 0.8641\n",
      "Epoch 150/200\n",
      "5452/5452 [==============================] - 2s 412us/step - loss: 0.4120 - acc: 0.8637\n",
      "Epoch 151/200\n",
      "5452/5452 [==============================] - 2s 405us/step - loss: 0.4102 - acc: 0.8650\n",
      "Epoch 152/200\n",
      "5452/5452 [==============================] - 2s 449us/step - loss: 0.4091 - acc: 0.8661\n",
      "Epoch 153/200\n",
      "5452/5452 [==============================] - 2s 430us/step - loss: 0.4088 - acc: 0.8676\n",
      "Epoch 154/200\n",
      "5452/5452 [==============================] - 2s 419us/step - loss: 0.4060 - acc: 0.8679\n",
      "Epoch 155/200\n",
      "5452/5452 [==============================] - 2s 405us/step - loss: 0.4073 - acc: 0.8674\n",
      "Epoch 156/200\n",
      "5452/5452 [==============================] - 2s 403us/step - loss: 0.4040 - acc: 0.8690\n",
      "Epoch 157/200\n",
      "5452/5452 [==============================] - 2s 366us/step - loss: 0.4022 - acc: 0.8692\n",
      "Epoch 158/200\n",
      "5452/5452 [==============================] - 2s 384us/step - loss: 0.4013 - acc: 0.8701\n",
      "Epoch 159/200\n",
      "5452/5452 [==============================] - 2s 379us/step - loss: 0.3992 - acc: 0.8707\n",
      "Epoch 160/200\n",
      "5452/5452 [==============================] - 2s 367us/step - loss: 0.3999 - acc: 0.8687\n",
      "Epoch 161/200\n",
      "5452/5452 [==============================] - 2s 418us/step - loss: 0.3967 - acc: 0.8700\n",
      "Epoch 162/200\n",
      "5452/5452 [==============================] - 2s 383us/step - loss: 0.3967 - acc: 0.8720\n",
      "Epoch 163/200\n",
      "5452/5452 [==============================] - 2s 439us/step - loss: 0.3949 - acc: 0.8723\n",
      "Epoch 164/200\n",
      "5452/5452 [==============================] - 2s 423us/step - loss: 0.3926 - acc: 0.8720\n",
      "Epoch 165/200\n",
      "5452/5452 [==============================] - 2s 415us/step - loss: 0.3908 - acc: 0.8747\n",
      "Epoch 166/200\n",
      "5452/5452 [==============================] - 2s 413us/step - loss: 0.3895 - acc: 0.8749\n",
      "Epoch 167/200\n",
      "5452/5452 [==============================] - 3s 460us/step - loss: 0.3881 - acc: 0.8740\n",
      "Epoch 168/200\n",
      "5452/5452 [==============================] - 2s 427us/step - loss: 0.3873 - acc: 0.8744\n",
      "Epoch 169/200\n",
      "5452/5452 [==============================] - 2s 403us/step - loss: 0.3841 - acc: 0.8745\n",
      "Epoch 170/200\n",
      "5452/5452 [==============================] - 2s 454us/step - loss: 0.3824 - acc: 0.8758\n",
      "Epoch 171/200\n",
      "5452/5452 [==============================] - 2s 433us/step - loss: 0.3813 - acc: 0.8777\n",
      "Epoch 172/200\n",
      "5452/5452 [==============================] - 3s 467us/step - loss: 0.3787 - acc: 0.8789\n",
      "Epoch 173/200\n",
      "5452/5452 [==============================] - 2s 393us/step - loss: 0.3773 - acc: 0.8799\n",
      "Epoch 174/200\n",
      "5452/5452 [==============================] - 2s 413us/step - loss: 0.3750 - acc: 0.8822\n",
      "Epoch 175/200\n",
      "5452/5452 [==============================] - 2s 433us/step - loss: 0.3758 - acc: 0.8826\n",
      "Epoch 176/200\n",
      "5452/5452 [==============================] - 2s 384us/step - loss: 0.3734 - acc: 0.8841\n",
      "Epoch 177/200\n",
      "5452/5452 [==============================] - 2s 384us/step - loss: 0.3711 - acc: 0.8833\n",
      "Epoch 178/200\n",
      "5452/5452 [==============================] - 2s 392us/step - loss: 0.3699 - acc: 0.8835\n",
      "Epoch 179/200\n",
      "5452/5452 [==============================] - 2s 397us/step - loss: 0.3697 - acc: 0.8828\n",
      "Epoch 180/200\n",
      "5452/5452 [==============================] - 2s 392us/step - loss: 0.3680 - acc: 0.8855\n",
      "Epoch 181/200\n",
      "5452/5452 [==============================] - 2s 391us/step - loss: 0.3666 - acc: 0.8874\n",
      "Epoch 182/200\n",
      "5452/5452 [==============================] - 2s 371us/step - loss: 0.3657 - acc: 0.8861\n",
      "Epoch 183/200\n",
      "5452/5452 [==============================] - 2s 406us/step - loss: 0.3646 - acc: 0.8866\n",
      "Epoch 184/200\n",
      "5452/5452 [==============================] - 2s 377us/step - loss: 0.3618 - acc: 0.8863\n",
      "Epoch 185/200\n",
      "5452/5452 [==============================] - 2s 395us/step - loss: 0.3626 - acc: 0.8876\n",
      "Epoch 186/200\n",
      "5452/5452 [==============================] - 2s 391us/step - loss: 0.3613 - acc: 0.8899\n",
      "Epoch 187/200\n",
      "5452/5452 [==============================] - 2s 387us/step - loss: 0.3595 - acc: 0.8883\n",
      "Epoch 188/200\n",
      "5452/5452 [==============================] - 2s 396us/step - loss: 0.3587 - acc: 0.8887\n",
      "Epoch 189/200\n",
      "5452/5452 [==============================] - 2s 390us/step - loss: 0.3574 - acc: 0.8890\n",
      "Epoch 190/200\n",
      "5452/5452 [==============================] - 2s 382us/step - loss: 0.3562 - acc: 0.8899\n",
      "Epoch 191/200\n",
      "5452/5452 [==============================] - 2s 393us/step - loss: 0.3552 - acc: 0.8905\n",
      "Epoch 192/200\n",
      "5452/5452 [==============================] - 2s 440us/step - loss: 0.3544 - acc: 0.8914\n",
      "Epoch 193/200\n",
      "5452/5452 [==============================] - 3s 488us/step - loss: 0.3521 - acc: 0.8920\n",
      "Epoch 194/200\n",
      "5452/5452 [==============================] - 2s 442us/step - loss: 0.3514 - acc: 0.8927\n",
      "Epoch 195/200\n",
      "5452/5452 [==============================] - 2s 448us/step - loss: 0.3503 - acc: 0.8945\n",
      "Epoch 196/200\n",
      "5452/5452 [==============================] - 2s 391us/step - loss: 0.3491 - acc: 0.8933\n",
      "Epoch 197/200\n",
      "5452/5452 [==============================] - 2s 393us/step - loss: 0.3479 - acc: 0.8934\n",
      "Epoch 198/200\n",
      "5452/5452 [==============================] - 2s 380us/step - loss: 0.3457 - acc: 0.8931\n",
      "Epoch 199/200\n",
      "5452/5452 [==============================] - 2s 395us/step - loss: 0.3444 - acc: 0.8951\n",
      "Epoch 200/200\n",
      "5452/5452 [==============================] - 2s 385us/step - loss: 0.3440 - acc: 0.8958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23b269b90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model1.fit(x = params1.get_train_padded(),\n",
    "           y = params1.get_train_labels_onehot(),\n",
    "           epochs = 200\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 81.0000000477%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model1.evaluate(params1.get_test_padded(),\n",
    "                params1.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. A first model for the extended labels\n",
    "\n",
    "On the very first try, I got a test set accuracy of 80% when predicting the basic labels. Although without the use of a developmement set and without using proper experimentation I can't claim anything about the generalizability of this first classifier, it does show me that the proble of prdicting the basic label is a very tractable one. \n",
    "\n",
    "Now, I'll train this same basic classifier to predict the extended label to see how much harder that problem is, if at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params2 = LstmParams(sequence_length = 10, labels=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model2.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params2.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model2.add(LSTM(params2.get_num_classes(), activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 10, 300)           27936600  \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 50)                70200     \n",
      "=================================================================\n",
      "Total params: 28,006,800\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 27,936,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5452/5452 [==============================] - 5s 952us/step - loss: 3.7752 - acc: 0.1097\n",
      "Epoch 2/200\n",
      "5452/5452 [==============================] - 3s 639us/step - loss: 3.4962 - acc: 0.1669\n",
      "Epoch 3/200\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 3.3841 - acc: 0.1942\n",
      "Epoch 4/200\n",
      "5452/5452 [==============================] - 3s 588us/step - loss: 3.3287 - acc: 0.2164\n",
      "Epoch 5/200\n",
      "5452/5452 [==============================] - 3s 636us/step - loss: 3.2167 - acc: 0.2570\n",
      "Epoch 6/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 3.1223 - acc: 0.2680\n",
      "Epoch 7/200\n",
      "5452/5452 [==============================] - 3s 630us/step - loss: 3.0692 - acc: 0.2729\n",
      "Epoch 8/200\n",
      "5452/5452 [==============================] - 3s 592us/step - loss: 3.0181 - acc: 0.2771\n",
      "Epoch 9/200\n",
      "5452/5452 [==============================] - 3s 581us/step - loss: 2.9705 - acc: 0.2799\n",
      "Epoch 10/200\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 2.9258 - acc: 0.2904\n",
      "Epoch 11/200\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 2.8764 - acc: 0.3025\n",
      "Epoch 12/200\n",
      "5452/5452 [==============================] - 3s 626us/step - loss: 2.8159 - acc: 0.3173\n",
      "Epoch 13/200\n",
      "5452/5452 [==============================] - 4s 674us/step - loss: 2.7756 - acc: 0.3230\n",
      "Epoch 14/200\n",
      "5452/5452 [==============================] - 3s 618us/step - loss: 2.7373 - acc: 0.3346\n",
      "Epoch 15/200\n",
      "5452/5452 [==============================] - 3s 577us/step - loss: 2.7029 - acc: 0.3467\n",
      "Epoch 16/200\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 2.6693 - acc: 0.3538\n",
      "Epoch 17/200\n",
      "5452/5452 [==============================] - 3s 641us/step - loss: 2.6387 - acc: 0.3624\n",
      "Epoch 18/200\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 2.6122 - acc: 0.3639\n",
      "Epoch 19/200\n",
      "5452/5452 [==============================] - 3s 602us/step - loss: 2.5881 - acc: 0.3733\n",
      "Epoch 20/200\n",
      "5452/5452 [==============================] - 3s 570us/step - loss: 2.5647 - acc: 0.3778\n",
      "Epoch 21/200\n",
      "5452/5452 [==============================] - 3s 617us/step - loss: 2.5429 - acc: 0.3844\n",
      "Epoch 22/200\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 2.5215 - acc: 0.3916\n",
      "Epoch 23/200\n",
      "5452/5452 [==============================] - 3s 578us/step - loss: 2.5027 - acc: 0.3949\n",
      "Epoch 24/200\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 2.4845 - acc: 0.4037\n",
      "Epoch 25/200\n",
      "5452/5452 [==============================] - 3s 602us/step - loss: 2.4672 - acc: 0.4055\n",
      "Epoch 26/200\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 2.4506 - acc: 0.4103\n",
      "Epoch 27/200\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 2.4355 - acc: 0.4142\n",
      "Epoch 28/200\n",
      "5452/5452 [==============================] - 3s 605us/step - loss: 2.4207 - acc: 0.4142\n",
      "Epoch 29/200\n",
      "5452/5452 [==============================] - 3s 625us/step - loss: 2.4074 - acc: 0.4182\n",
      "Epoch 30/200\n",
      "5452/5452 [==============================] - 3s 598us/step - loss: 2.3938 - acc: 0.4209\n",
      "Epoch 31/200\n",
      "5452/5452 [==============================] - 3s 622us/step - loss: 2.3804 - acc: 0.4248\n",
      "Epoch 32/200\n",
      "5452/5452 [==============================] - 3s 624us/step - loss: 2.3678 - acc: 0.4299\n",
      "Epoch 33/200\n",
      "5452/5452 [==============================] - 3s 577us/step - loss: 2.3556 - acc: 0.4316\n",
      "Epoch 34/200\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 2.3443 - acc: 0.4356\n",
      "Epoch 35/200\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 2.3288 - acc: 0.4376\n",
      "Epoch 36/200\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 2.3130 - acc: 0.4420\n",
      "Epoch 37/200\n",
      "5452/5452 [==============================] - 3s 576us/step - loss: 2.3010 - acc: 0.4417\n",
      "Epoch 38/200\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 2.2896 - acc: 0.4450\n",
      "Epoch 39/200\n",
      "5452/5452 [==============================] - 3s 569us/step - loss: 2.2794 - acc: 0.4448\n",
      "Epoch 40/200\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 2.2673 - acc: 0.4486\n",
      "Epoch 41/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 2.2565 - acc: 0.4518\n",
      "Epoch 42/200\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 2.2467 - acc: 0.4536\n",
      "Epoch 43/200\n",
      "5452/5452 [==============================] - 3s 563us/step - loss: 2.2367 - acc: 0.4560\n",
      "Epoch 44/200\n",
      "5452/5452 [==============================] - 3s 581us/step - loss: 2.2268 - acc: 0.4573\n",
      "Epoch 45/200\n",
      "5452/5452 [==============================] - 3s 554us/step - loss: 2.2164 - acc: 0.4589\n",
      "Epoch 46/200\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 2.2063 - acc: 0.4631\n",
      "Epoch 47/200\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 2.1967 - acc: 0.4640\n",
      "Epoch 48/200\n",
      "5452/5452 [==============================] - 3s 564us/step - loss: 2.1881 - acc: 0.4666\n",
      "Epoch 49/200\n",
      "5452/5452 [==============================] - 3s 537us/step - loss: 2.1794 - acc: 0.4675\n",
      "Epoch 50/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 2.1702 - acc: 0.4686\n",
      "Epoch 51/200\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 2.1618 - acc: 0.4705\n",
      "Epoch 52/200\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 2.1532 - acc: 0.4716\n",
      "Epoch 53/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 2.1451 - acc: 0.4729\n",
      "Epoch 54/200\n",
      "5452/5452 [==============================] - 3s 604us/step - loss: 2.1364 - acc: 0.4756\n",
      "Epoch 55/200\n",
      "5452/5452 [==============================] - 3s 556us/step - loss: 2.1283 - acc: 0.4760\n",
      "Epoch 56/200\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 2.1200 - acc: 0.4780\n",
      "Epoch 57/200\n",
      "5452/5452 [==============================] - 3s 609us/step - loss: 2.1113 - acc: 0.4809\n",
      "Epoch 58/200\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 2.1035 - acc: 0.4807\n",
      "Epoch 59/200\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 2.0950 - acc: 0.4835\n",
      "Epoch 60/200\n",
      "5452/5452 [==============================] - 3s 521us/step - loss: 2.0866 - acc: 0.4868\n",
      "Epoch 61/200\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 2.0777 - acc: 0.4894\n",
      "Epoch 62/200\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 2.0679 - acc: 0.4917\n",
      "Epoch 63/200\n",
      "5452/5452 [==============================] - 3s 634us/step - loss: 2.0585 - acc: 0.4943\n",
      "Epoch 64/200\n",
      "5452/5452 [==============================] - 3s 484us/step - loss: 2.0504 - acc: 0.4939\n",
      "Epoch 65/200\n",
      "5452/5452 [==============================] - 3s 633us/step - loss: 2.0429 - acc: 0.4976\n",
      "Epoch 66/200\n",
      "5452/5452 [==============================] - 3s 577us/step - loss: 2.0356 - acc: 0.4998\n",
      "Epoch 67/200\n",
      "5452/5452 [==============================] - 3s 521us/step - loss: 2.0281 - acc: 0.5028\n",
      "Epoch 68/200\n",
      "5452/5452 [==============================] - 3s 548us/step - loss: 2.0209 - acc: 0.5064\n",
      "Epoch 69/200\n",
      "5452/5452 [==============================] - 3s 640us/step - loss: 2.0131 - acc: 0.5101\n",
      "Epoch 70/200\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 2.0064 - acc: 0.5114\n",
      "Epoch 71/200\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 1.9997 - acc: 0.5136\n",
      "Epoch 72/200\n",
      "5452/5452 [==============================] - 4s 656us/step - loss: 1.9924 - acc: 0.5152\n",
      "Epoch 73/200\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 1.9843 - acc: 0.5189\n",
      "Epoch 74/200\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 1.9768 - acc: 0.5200\n",
      "Epoch 75/200\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 1.9702 - acc: 0.5244\n",
      "Epoch 76/200\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 1.9642 - acc: 0.5244\n",
      "Epoch 77/200\n",
      "5452/5452 [==============================] - 3s 584us/step - loss: 1.9584 - acc: 0.5279\n",
      "Epoch 78/200\n",
      "5452/5452 [==============================] - 3s 592us/step - loss: 1.9518 - acc: 0.5286\n",
      "Epoch 79/200\n",
      "5452/5452 [==============================] - 3s 507us/step - loss: 1.9459 - acc: 0.5325\n",
      "Epoch 80/200\n",
      "5452/5452 [==============================] - 3s 507us/step - loss: 1.9397 - acc: 0.5321\n",
      "Epoch 81/200\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 1.9341 - acc: 0.5332\n",
      "Epoch 82/200\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 1.9283 - acc: 0.5352\n",
      "Epoch 83/200\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 1.9227 - acc: 0.5360\n",
      "Epoch 84/200\n",
      "5452/5452 [==============================] - 3s 632us/step - loss: 1.9170 - acc: 0.5396\n",
      "Epoch 85/200\n",
      "5452/5452 [==============================] - 3s 603us/step - loss: 1.9118 - acc: 0.5396\n",
      "Epoch 86/200\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 1.9067 - acc: 0.5383\n",
      "Epoch 87/200\n",
      "5452/5452 [==============================] - 4s 645us/step - loss: 1.9014 - acc: 0.5418\n",
      "Epoch 88/200\n",
      "5452/5452 [==============================] - 3s 575us/step - loss: 1.8959 - acc: 0.5429\n",
      "Epoch 89/200\n",
      "5452/5452 [==============================] - 3s 522us/step - loss: 1.8914 - acc: 0.5442\n",
      "Epoch 90/200\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 1.8863 - acc: 0.5451\n",
      "Epoch 91/200\n",
      "5452/5452 [==============================] - 3s 539us/step - loss: 1.8816 - acc: 0.5471\n",
      "Epoch 92/200\n",
      "5452/5452 [==============================] - 3s 591us/step - loss: 1.8769 - acc: 0.5503\n",
      "Epoch 93/200\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 1.8718 - acc: 0.5512\n",
      "Epoch 94/200\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 1.8669 - acc: 0.5530\n",
      "Epoch 95/200\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 1.8619 - acc: 0.5539\n",
      "Epoch 96/200\n",
      "5452/5452 [==============================] - 3s 591us/step - loss: 1.8564 - acc: 0.5552\n",
      "Epoch 97/200\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 1.8512 - acc: 0.5550\n",
      "Epoch 98/200\n",
      "5452/5452 [==============================] - 3s 525us/step - loss: 1.8458 - acc: 0.5583\n",
      "Epoch 99/200\n",
      "5452/5452 [==============================] - 3s 554us/step - loss: 1.8412 - acc: 0.5592\n",
      "Epoch 100/200\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 1.8364 - acc: 0.5618\n",
      "Epoch 101/200\n",
      "5452/5452 [==============================] - 3s 579us/step - loss: 1.8312 - acc: 0.5616\n",
      "Epoch 102/200\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 1.8264 - acc: 0.5613\n",
      "Epoch 103/200\n",
      "5452/5452 [==============================] - 3s 565us/step - loss: 1.8215 - acc: 0.5640\n",
      "Epoch 104/200\n",
      "5452/5452 [==============================] - 3s 558us/step - loss: 1.8177 - acc: 0.5642\n",
      "Epoch 105/200\n",
      "5452/5452 [==============================] - 3s 547us/step - loss: 1.8126 - acc: 0.5668\n",
      "Epoch 106/200\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 1.8079 - acc: 0.5699\n",
      "Epoch 107/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 1.8030 - acc: 0.5680\n",
      "Epoch 108/200\n",
      "5452/5452 [==============================] - 3s 567us/step - loss: 1.7979 - acc: 0.5702\n",
      "Epoch 109/200\n",
      "5452/5452 [==============================] - 3s 571us/step - loss: 1.7938 - acc: 0.5725\n",
      "Epoch 110/200\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 1.7889 - acc: 0.5706\n",
      "Epoch 111/200\n",
      "5452/5452 [==============================] - 3s 591us/step - loss: 1.7846 - acc: 0.5745\n",
      "Epoch 112/200\n",
      "5452/5452 [==============================] - 3s 610us/step - loss: 1.7805 - acc: 0.5743\n",
      "Epoch 113/200\n",
      "5452/5452 [==============================] - 3s 619us/step - loss: 1.7762 - acc: 0.5759\n",
      "Epoch 114/200\n",
      "5452/5452 [==============================] - 3s 571us/step - loss: 1.7719 - acc: 0.5774\n",
      "Epoch 115/200\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 1.7681 - acc: 0.5791\n",
      "Epoch 116/200\n",
      "5452/5452 [==============================] - 3s 545us/step - loss: 1.7642 - acc: 0.5800\n",
      "Epoch 117/200\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 1.7601 - acc: 0.5813\n",
      "Epoch 118/200\n",
      "5452/5452 [==============================] - 3s 576us/step - loss: 1.7555 - acc: 0.5816\n",
      "Epoch 119/200\n",
      "5452/5452 [==============================] - 3s 532us/step - loss: 1.7521 - acc: 0.5829\n",
      "Epoch 120/200\n",
      "5452/5452 [==============================] - 3s 580us/step - loss: 1.7477 - acc: 0.5835\n",
      "Epoch 121/200\n",
      "5452/5452 [==============================] - 3s 534us/step - loss: 1.7440 - acc: 0.5829\n",
      "Epoch 122/200\n",
      "5452/5452 [==============================] - 3s 565us/step - loss: 1.7400 - acc: 0.5864\n",
      "Epoch 123/200\n",
      "5452/5452 [==============================] - 3s 549us/step - loss: 1.7367 - acc: 0.5877\n",
      "Epoch 124/200\n",
      "5452/5452 [==============================] - 3s 566us/step - loss: 1.7327 - acc: 0.5886\n",
      "Epoch 125/200\n",
      "5452/5452 [==============================] - 3s 559us/step - loss: 1.7282 - acc: 0.5908\n",
      "Epoch 126/200\n",
      "5452/5452 [==============================] - 3s 567us/step - loss: 1.7247 - acc: 0.5880\n",
      "Epoch 127/200\n",
      "5452/5452 [==============================] - 3s 586us/step - loss: 1.7210 - acc: 0.5921\n",
      "Epoch 128/200\n",
      "5452/5452 [==============================] - 3s 567us/step - loss: 1.7170 - acc: 0.5939\n",
      "Epoch 129/200\n",
      "5452/5452 [==============================] - 3s 530us/step - loss: 1.7127 - acc: 0.5932\n",
      "Epoch 130/200\n",
      "5452/5452 [==============================] - 3s 563us/step - loss: 1.7093 - acc: 0.5945\n",
      "Epoch 131/200\n",
      "5452/5452 [==============================] - 3s 561us/step - loss: 1.7057 - acc: 0.5952\n",
      "Epoch 132/200\n",
      "5452/5452 [==============================] - 3s 590us/step - loss: 1.7015 - acc: 0.5974\n",
      "Epoch 133/200\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 1.6975 - acc: 0.5979\n",
      "Epoch 134/200\n",
      "5452/5452 [==============================] - 3s 563us/step - loss: 1.6939 - acc: 0.5985\n",
      "Epoch 135/200\n",
      "5452/5452 [==============================] - 3s 596us/step - loss: 1.6898 - acc: 0.5990\n",
      "Epoch 136/200\n",
      "5452/5452 [==============================] - 3s 584us/step - loss: 1.6853 - acc: 0.5990\n",
      "Epoch 137/200\n",
      "5452/5452 [==============================] - 3s 528us/step - loss: 1.6818 - acc: 0.6003\n",
      "Epoch 138/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 1.6776 - acc: 0.6012\n",
      "Epoch 139/200\n",
      "5452/5452 [==============================] - 3s 567us/step - loss: 1.6734 - acc: 0.6014\n",
      "Epoch 140/200\n",
      "5452/5452 [==============================] - 3s 555us/step - loss: 1.6694 - acc: 0.6016\n",
      "Epoch 141/200\n",
      "5452/5452 [==============================] - 3s 604us/step - loss: 1.6659 - acc: 0.6020\n",
      "Epoch 142/200\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 1.6615 - acc: 0.6031\n",
      "Epoch 143/200\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 1.6580 - acc: 0.6042\n",
      "Epoch 144/200\n",
      "5452/5452 [==============================] - 3s 479us/step - loss: 1.6542 - acc: 0.6056\n",
      "Epoch 145/200\n",
      "5452/5452 [==============================] - 3s 542us/step - loss: 1.6506 - acc: 0.6045\n",
      "Epoch 146/200\n",
      "5452/5452 [==============================] - 3s 588us/step - loss: 1.6468 - acc: 0.6079\n",
      "Epoch 147/200\n",
      "5452/5452 [==============================] - 3s 547us/step - loss: 1.6431 - acc: 0.6053\n",
      "Epoch 148/200\n",
      "5452/5452 [==============================] - 3s 548us/step - loss: 1.6398 - acc: 0.6082\n",
      "Epoch 149/200\n",
      "5452/5452 [==============================] - 3s 535us/step - loss: 1.6356 - acc: 0.6084\n",
      "Epoch 150/200\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 1.6318 - acc: 0.6084\n",
      "Epoch 151/200\n",
      "5452/5452 [==============================] - 3s 597us/step - loss: 1.6283 - acc: 0.6095\n",
      "Epoch 152/200\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 1.6246 - acc: 0.6090\n",
      "Epoch 153/200\n",
      "5452/5452 [==============================] - 3s 552us/step - loss: 1.6210 - acc: 0.6101\n",
      "Epoch 154/200\n",
      "5452/5452 [==============================] - 3s 584us/step - loss: 1.6178 - acc: 0.6112\n",
      "Epoch 155/200\n",
      "5452/5452 [==============================] - 3s 548us/step - loss: 1.6137 - acc: 0.6113\n",
      "Epoch 156/200\n",
      "5452/5452 [==============================] - 3s 540us/step - loss: 1.6097 - acc: 0.6123\n",
      "Epoch 157/200\n",
      "5452/5452 [==============================] - 3s 529us/step - loss: 1.6045 - acc: 0.6145\n",
      "Epoch 158/200\n",
      "5452/5452 [==============================] - 3s 549us/step - loss: 1.5986 - acc: 0.6143\n",
      "Epoch 159/200\n",
      "5452/5452 [==============================] - 3s 534us/step - loss: 1.5938 - acc: 0.6148\n",
      "Epoch 160/200\n",
      "5452/5452 [==============================] - 3s 570us/step - loss: 1.5899 - acc: 0.6168\n",
      "Epoch 161/200\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 1.5863 - acc: 0.6185\n",
      "Epoch 162/200\n",
      "5452/5452 [==============================] - 3s 560us/step - loss: 1.5826 - acc: 0.6179\n",
      "Epoch 163/200\n",
      "5452/5452 [==============================] - 3s 555us/step - loss: 1.5792 - acc: 0.6185\n",
      "Epoch 164/200\n",
      "5452/5452 [==============================] - 3s 550us/step - loss: 1.5756 - acc: 0.6190\n",
      "Epoch 165/200\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 1.5720 - acc: 0.6190\n",
      "Epoch 166/200\n",
      "5452/5452 [==============================] - 3s 576us/step - loss: 1.5689 - acc: 0.6196\n",
      "Epoch 167/200\n",
      "5452/5452 [==============================] - 3s 537us/step - loss: 1.5656 - acc: 0.6227\n",
      "Epoch 168/200\n",
      "5452/5452 [==============================] - 3s 555us/step - loss: 1.5623 - acc: 0.6218\n",
      "Epoch 169/200\n",
      "5452/5452 [==============================] - 3s 579us/step - loss: 1.5587 - acc: 0.6225\n",
      "Epoch 170/200\n",
      "5452/5452 [==============================] - 3s 580us/step - loss: 1.5556 - acc: 0.6247\n",
      "Epoch 171/200\n",
      "5452/5452 [==============================] - 3s 547us/step - loss: 1.5524 - acc: 0.6242\n",
      "Epoch 172/200\n",
      "5452/5452 [==============================] - 3s 572us/step - loss: 1.5484 - acc: 0.6264\n",
      "Epoch 173/200\n",
      "5452/5452 [==============================] - 3s 569us/step - loss: 1.5460 - acc: 0.6278\n",
      "Epoch 174/200\n",
      "5452/5452 [==============================] - 3s 552us/step - loss: 1.5426 - acc: 0.6282\n",
      "Epoch 175/200\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 1.5391 - acc: 0.6275\n",
      "Epoch 176/200\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 1.5364 - acc: 0.6289\n",
      "Epoch 177/200\n",
      "5452/5452 [==============================] - 3s 575us/step - loss: 1.5327 - acc: 0.6299\n",
      "Epoch 178/200\n",
      "5452/5452 [==============================] - 3s 543us/step - loss: 1.5298 - acc: 0.6302\n",
      "Epoch 179/200\n",
      "5452/5452 [==============================] - 3s 581us/step - loss: 1.5261 - acc: 0.6310\n",
      "Epoch 180/200\n",
      "5452/5452 [==============================] - 3s 548us/step - loss: 1.5235 - acc: 0.6311\n",
      "Epoch 181/200\n",
      "5452/5452 [==============================] - 3s 550us/step - loss: 1.5200 - acc: 0.6322\n",
      "Epoch 182/200\n",
      "5452/5452 [==============================] - 3s 569us/step - loss: 1.5165 - acc: 0.6335\n",
      "Epoch 183/200\n",
      "5452/5452 [==============================] - 3s 570us/step - loss: 1.5140 - acc: 0.6335\n",
      "Epoch 184/200\n",
      "5452/5452 [==============================] - 3s 572us/step - loss: 1.5107 - acc: 0.6330\n",
      "Epoch 185/200\n",
      "5452/5452 [==============================] - 3s 570us/step - loss: 1.5075 - acc: 0.6352\n",
      "Epoch 186/200\n",
      "5452/5452 [==============================] - 3s 567us/step - loss: 1.5042 - acc: 0.6366\n",
      "Epoch 187/200\n",
      "5452/5452 [==============================] - 3s 569us/step - loss: 1.5010 - acc: 0.6379\n",
      "Epoch 188/200\n",
      "5452/5452 [==============================] - 3s 556us/step - loss: 1.4978 - acc: 0.6381\n",
      "Epoch 189/200\n",
      "5452/5452 [==============================] - 3s 554us/step - loss: 1.4952 - acc: 0.6374\n",
      "Epoch 190/200\n",
      "5452/5452 [==============================] - 3s 520us/step - loss: 1.4921 - acc: 0.6398\n",
      "Epoch 191/200\n",
      "5452/5452 [==============================] - 3s 544us/step - loss: 1.4890 - acc: 0.6399\n",
      "Epoch 192/200\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 1.4857 - acc: 0.6399\n",
      "Epoch 193/200\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 1.4826 - acc: 0.6420\n",
      "Epoch 194/200\n",
      "5452/5452 [==============================] - 3s 539us/step - loss: 1.4800 - acc: 0.6403\n",
      "Epoch 195/200\n",
      "5452/5452 [==============================] - 3s 558us/step - loss: 1.4767 - acc: 0.6421\n",
      "Epoch 196/200\n",
      "5452/5452 [==============================] - 3s 552us/step - loss: 1.4735 - acc: 0.6427\n",
      "Epoch 197/200\n",
      "5452/5452 [==============================] - 3s 580us/step - loss: 1.4704 - acc: 0.6442\n",
      "Epoch 198/200\n",
      "5452/5452 [==============================] - 3s 577us/step - loss: 1.4672 - acc: 0.6429\n",
      "Epoch 199/200\n",
      "5452/5452 [==============================] - 3s 530us/step - loss: 1.4638 - acc: 0.6449\n",
      "Epoch 200/200\n",
      "5452/5452 [==============================] - 3s 582us/step - loss: 1.4609 - acc: 0.6456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d238ce10>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model2.fit(x = params2.get_train_padded(),\n",
    "           y = params2.get_train_labels_onehot(),\n",
    "           epochs = 200\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 55.8000000477%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model2.evaluate(params2.get_test_padded(),\n",
    "                params2.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, predicting the fine grained label is a much harder problem. The training accuracy never exceeds 65%, which indicates that we may need to include a wider window, and perhaps a more complex archetecture. \n",
    "\n",
    "Further, we can already see signs of overfitting, as the test error is much higher than the training error. If we add more complex layers, it may be worth adding some dropout units in order to combat overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Adding a dense layer when predicting basic labels\n",
    "\n",
    "It might be worth adding a dense layer to the basic label classifier to learn more complex functions. If there are signs of overfitting, then I'll add some dropout units to the LSTM cells.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params3 = LstmParams(sequence_length = 10,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Add embedding and LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model3.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params3.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use a dense hidden layer of 50 nodes after the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model3.add(LSTM(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a Dense laer, and apply the softmax activation on their outputs. \n",
    "model3.add(Dense(params3.get_num_classes(), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 300)           27978300  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 28,048,806\n",
      "Trainable params: 70,506\n",
      "Non-trainable params: 27,978,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Train that model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5452/5452 [==============================] - 5s 831us/step - loss: 1.5908 - acc: 0.3072\n",
      "Epoch 2/100\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.9598 - acc: 0.6388\n",
      "Epoch 3/100\n",
      "5452/5452 [==============================] - 3s 597us/step - loss: 0.7802 - acc: 0.6968\n",
      "Epoch 4/100\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.7076 - acc: 0.7313\n",
      "Epoch 5/100\n",
      "5452/5452 [==============================] - 3s 605us/step - loss: 0.6633 - acc: 0.7496\n",
      "Epoch 6/100\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 0.6161 - acc: 0.7689\n",
      "Epoch 7/100\n",
      "5452/5452 [==============================] - 4s 651us/step - loss: 0.5905 - acc: 0.7793\n",
      "Epoch 8/100\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 0.5665 - acc: 0.7922\n",
      "Epoch 9/100\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 0.5445 - acc: 0.7975\n",
      "Epoch 10/100\n",
      "5452/5452 [==============================] - 3s 568us/step - loss: 0.5193 - acc: 0.8136\n",
      "Epoch 11/100\n",
      "5452/5452 [==============================] - 3s 559us/step - loss: 0.5030 - acc: 0.8168\n",
      "Epoch 12/100\n",
      "5452/5452 [==============================] - 3s 551us/step - loss: 0.4903 - acc: 0.8217\n",
      "Epoch 13/100\n",
      "5452/5452 [==============================] - 3s 570us/step - loss: 0.4767 - acc: 0.8303\n",
      "Epoch 14/100\n",
      "5452/5452 [==============================] - 3s 563us/step - loss: 0.4591 - acc: 0.8386\n",
      "Epoch 15/100\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 0.4519 - acc: 0.8384\n",
      "Epoch 16/100\n",
      "5452/5452 [==============================] - 3s 542us/step - loss: 0.4438 - acc: 0.8421\n",
      "Epoch 17/100\n",
      "5452/5452 [==============================] - 3s 549us/step - loss: 0.4263 - acc: 0.8467\n",
      "Epoch 18/100\n",
      "5452/5452 [==============================] - 3s 550us/step - loss: 0.4169 - acc: 0.8516\n",
      "Epoch 19/100\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 0.4057 - acc: 0.8545\n",
      "Epoch 20/100\n",
      "5452/5452 [==============================] - 3s 571us/step - loss: 0.3882 - acc: 0.8579\n",
      "Epoch 21/100\n",
      "5452/5452 [==============================] - 3s 579us/step - loss: 0.3928 - acc: 0.8604\n",
      "Epoch 22/100\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 0.3824 - acc: 0.8694\n",
      "Epoch 23/100\n",
      "5452/5452 [==============================] - 3s 563us/step - loss: 0.3721 - acc: 0.8700\n",
      "Epoch 24/100\n",
      "5452/5452 [==============================] - 3s 584us/step - loss: 0.3475 - acc: 0.8800\n",
      "Epoch 25/100\n",
      "5452/5452 [==============================] - 3s 551us/step - loss: 0.3489 - acc: 0.8788\n",
      "Epoch 26/100\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 0.3322 - acc: 0.8879\n",
      "Epoch 27/100\n",
      "5452/5452 [==============================] - 3s 559us/step - loss: 0.3313 - acc: 0.8813\n",
      "Epoch 28/100\n",
      "5452/5452 [==============================] - 3s 539us/step - loss: 0.3094 - acc: 0.8907\n",
      "Epoch 29/100\n",
      "5452/5452 [==============================] - 3s 554us/step - loss: 0.3064 - acc: 0.8945\n",
      "Epoch 30/100\n",
      "5452/5452 [==============================] - 3s 604us/step - loss: 0.2951 - acc: 0.8944\n",
      "Epoch 31/100\n",
      "5452/5452 [==============================] - 3s 554us/step - loss: 0.2994 - acc: 0.8936\n",
      "Epoch 32/100\n",
      "5452/5452 [==============================] - 3s 571us/step - loss: 0.2776 - acc: 0.9032\n",
      "Epoch 33/100\n",
      "5452/5452 [==============================] - 3s 530us/step - loss: 0.2709 - acc: 0.9077\n",
      "Epoch 34/100\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 0.2619 - acc: 0.9057\n",
      "Epoch 35/100\n",
      "5452/5452 [==============================] - 3s 566us/step - loss: 0.2599 - acc: 0.9121\n",
      "Epoch 36/100\n",
      "5452/5452 [==============================] - 3s 529us/step - loss: 0.2460 - acc: 0.9151\n",
      "Epoch 37/100\n",
      "5452/5452 [==============================] - 3s 530us/step - loss: 0.2451 - acc: 0.9165\n",
      "Epoch 38/100\n",
      "5452/5452 [==============================] - 3s 550us/step - loss: 0.2332 - acc: 0.9186\n",
      "Epoch 39/100\n",
      "5452/5452 [==============================] - 3s 559us/step - loss: 0.2136 - acc: 0.9257\n",
      "Epoch 40/100\n",
      "5452/5452 [==============================] - 3s 560us/step - loss: 0.2115 - acc: 0.9277\n",
      "Epoch 41/100\n",
      "5452/5452 [==============================] - 3s 559us/step - loss: 0.1931 - acc: 0.9351\n",
      "Epoch 42/100\n",
      "5452/5452 [==============================] - 3s 565us/step - loss: 0.2015 - acc: 0.9287\n",
      "Epoch 43/100\n",
      "5452/5452 [==============================] - 3s 561us/step - loss: 0.1767 - acc: 0.9411\n",
      "Epoch 44/100\n",
      "5452/5452 [==============================] - 3s 547us/step - loss: 0.1720 - acc: 0.9391\n",
      "Epoch 45/100\n",
      "5452/5452 [==============================] - 3s 554us/step - loss: 0.1687 - acc: 0.9455\n",
      "Epoch 46/100\n",
      "5452/5452 [==============================] - 3s 565us/step - loss: 0.1640 - acc: 0.9428\n",
      "Epoch 47/100\n",
      "5452/5452 [==============================] - 3s 565us/step - loss: 0.1597 - acc: 0.9450\n",
      "Epoch 48/100\n",
      "5452/5452 [==============================] - 3s 561us/step - loss: 0.1559 - acc: 0.9507\n",
      "Epoch 49/100\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 0.1330 - acc: 0.9573\n",
      "Epoch 50/100\n",
      "5452/5452 [==============================] - 3s 551us/step - loss: 0.1225 - acc: 0.9639\n",
      "Epoch 51/100\n",
      "5452/5452 [==============================] - 3s 558us/step - loss: 0.1146 - acc: 0.9663\n",
      "Epoch 52/100\n",
      "5452/5452 [==============================] - 3s 535us/step - loss: 0.1172 - acc: 0.9624\n",
      "Epoch 53/100\n",
      "5452/5452 [==============================] - 3s 558us/step - loss: 0.1174 - acc: 0.9607\n",
      "Epoch 54/100\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 0.1088 - acc: 0.9650\n",
      "Epoch 55/100\n",
      "5452/5452 [==============================] - 3s 566us/step - loss: 0.0897 - acc: 0.9738\n",
      "Epoch 56/100\n",
      "5452/5452 [==============================] - 3s 569us/step - loss: 0.0899 - acc: 0.9740\n",
      "Epoch 57/100\n",
      "5452/5452 [==============================] - 3s 567us/step - loss: 0.1095 - acc: 0.9644\n",
      "Epoch 58/100\n",
      "5452/5452 [==============================] - 3s 565us/step - loss: 0.1028 - acc: 0.9655\n",
      "Epoch 59/100\n",
      "5452/5452 [==============================] - 3s 564us/step - loss: 0.0944 - acc: 0.9694\n",
      "Epoch 60/100\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 0.0736 - acc: 0.9778\n",
      "Epoch 61/100\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 0.0786 - acc: 0.9756\n",
      "Epoch 62/100\n",
      "5452/5452 [==============================] - 3s 588us/step - loss: 0.0696 - acc: 0.9804\n",
      "Epoch 63/100\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 0.0583 - acc: 0.9828\n",
      "Epoch 64/100\n",
      "5452/5452 [==============================] - 3s 584us/step - loss: 0.0535 - acc: 0.9859\n",
      "Epoch 65/100\n",
      "5452/5452 [==============================] - 3s 596us/step - loss: 0.0629 - acc: 0.9815\n",
      "Epoch 66/100\n",
      "5452/5452 [==============================] - 3s 610us/step - loss: 0.0568 - acc: 0.9828\n",
      "Epoch 67/100\n",
      "5452/5452 [==============================] - 3s 578us/step - loss: 0.0877 - acc: 0.9705\n",
      "Epoch 68/100\n",
      "5452/5452 [==============================] - 3s 576us/step - loss: 0.0911 - acc: 0.9730\n",
      "Epoch 69/100\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 0.0584 - acc: 0.9844\n",
      "Epoch 70/100\n",
      "5452/5452 [==============================] - 3s 565us/step - loss: 0.0382 - acc: 0.9905\n",
      "Epoch 71/100\n",
      "5452/5452 [==============================] - 3s 553us/step - loss: 0.0383 - acc: 0.9910\n",
      "Epoch 72/100\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 0.0382 - acc: 0.9903\n",
      "Epoch 73/100\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 0.0336 - acc: 0.9916\n",
      "Epoch 74/100\n",
      "5452/5452 [==============================] - 3s 594us/step - loss: 0.0287 - acc: 0.9932\n",
      "Epoch 75/100\n",
      "5452/5452 [==============================] - 3s 572us/step - loss: 0.0400 - acc: 0.9906\n",
      "Epoch 76/100\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 0.0673 - acc: 0.9802\n",
      "Epoch 77/100\n",
      "5452/5452 [==============================] - 3s 547us/step - loss: 0.0821 - acc: 0.9754\n",
      "Epoch 78/100\n",
      "5452/5452 [==============================] - 3s 602us/step - loss: 0.0248 - acc: 0.9950\n",
      "Epoch 79/100\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 0.0209 - acc: 0.9960\n",
      "Epoch 80/100\n",
      "5452/5452 [==============================] - 3s 609us/step - loss: 0.0154 - acc: 0.9980\n",
      "Epoch 81/100\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 0.0126 - acc: 0.9980\n",
      "Epoch 82/100\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 0.0158 - acc: 0.9965\n",
      "Epoch 83/100\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.1320 - acc: 0.9635\n",
      "Epoch 84/100\n",
      "5452/5452 [==============================] - 3s 582us/step - loss: 0.0703 - acc: 0.9778\n",
      "Epoch 85/100\n",
      "5452/5452 [==============================] - 3s 563us/step - loss: 0.0480 - acc: 0.9850\n",
      "Epoch 86/100\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 0.0449 - acc: 0.9862\n",
      "Epoch 87/100\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.0232 - acc: 0.9949\n",
      "Epoch 88/100\n",
      "5452/5452 [==============================] - 3s 571us/step - loss: 0.0217 - acc: 0.9945\n",
      "Epoch 89/100\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 0.0111 - acc: 0.9982\n",
      "Epoch 90/100\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 0.0083 - acc: 0.9989\n",
      "Epoch 91/100\n",
      "5452/5452 [==============================] - 3s 584us/step - loss: 0.0075 - acc: 0.9989\n",
      "Epoch 92/100\n",
      "5452/5452 [==============================] - 3s 591us/step - loss: 0.0061 - acc: 0.9991\n",
      "Epoch 93/100\n",
      "5452/5452 [==============================] - 3s 596us/step - loss: 0.0059 - acc: 0.9991\n",
      "Epoch 94/100\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 0.0248 - acc: 0.9945\n",
      "Epoch 95/100\n",
      "5452/5452 [==============================] - 3s 590us/step - loss: 0.1056 - acc: 0.9668\n",
      "Epoch 96/100\n",
      "5452/5452 [==============================] - 3s 596us/step - loss: 0.0592 - acc: 0.9826\n",
      "Epoch 97/100\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 0.0327 - acc: 0.9905\n",
      "Epoch 98/100\n",
      "5452/5452 [==============================] - 3s 574us/step - loss: 0.0276 - acc: 0.9928\n",
      "Epoch 99/100\n",
      "5452/5452 [==============================] - 3s 555us/step - loss: 0.0138 - acc: 0.9974\n",
      "Epoch 100/100\n",
      "5452/5452 [==============================] - 4s 693us/step - loss: 0.0065 - acc: 0.9993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2c7cf10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model3.fit(x = params3.get_train_padded(),\n",
    "           y = params3.get_train_labels_onehot(),\n",
    "           epochs = 100\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 83.0%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model3.evaluate(params3.get_test_padded(),\n",
    "                params3.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this network is overfit. WE achieve almost perfect accuracy on the training set, but the test accuracy is no better than the simple LSTM modle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Adding a dropout layer\n",
    "\n",
    "I'll re-use the same archetecutre, but this time use dropout layers between the embedding and LSTM layers, as well as between the LSTM and dense layers.\n",
    "\n",
    "Later, it might be worth using Within-cell recurrent dropout, provided via the `Keras` interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params4 = LstmParams(sequence_length = 10,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model4.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params4.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a droput layer\n",
    "model4.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model4.add(LSTM(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# another dropout before a dense layer\n",
    "model4.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a Dense layer, and apply the softmax activation on their outputs. \n",
    "model4.add(Dense(params4.get_num_classes(), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 300)           27978300  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 28,048,806\n",
      "Trainable params: 70,506\n",
      "Non-trainable params: 27,978,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "5452/5452 [==============================] - 5s 870us/step - loss: 1.6017 - acc: 0.3085\n",
      "Epoch 2/125\n",
      "5452/5452 [==============================] - 3s 629us/step - loss: 1.1103 - acc: 0.5616\n",
      "Epoch 3/125\n",
      "5452/5452 [==============================] - 4s 657us/step - loss: 0.9020 - acc: 0.6572\n",
      "Epoch 4/125\n",
      "5452/5452 [==============================] - 3s 547us/step - loss: 0.8252 - acc: 0.6810\n",
      "Epoch 5/125\n",
      "5452/5452 [==============================] - 4s 647us/step - loss: 0.7787 - acc: 0.7047\n",
      "Epoch 6/125\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.7418 - acc: 0.7181\n",
      "Epoch 7/125\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 0.7252 - acc: 0.7296\n",
      "Epoch 8/125\n",
      "5452/5452 [==============================] - 4s 687us/step - loss: 0.7018 - acc: 0.7421\n",
      "Epoch 9/125\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 0.6917 - acc: 0.7410\n",
      "Epoch 10/125\n",
      "5452/5452 [==============================] - 3s 582us/step - loss: 0.6833 - acc: 0.7436\n",
      "Epoch 11/125\n",
      "5452/5452 [==============================] - 3s 592us/step - loss: 0.6575 - acc: 0.7588\n",
      "Epoch 12/125\n",
      "5452/5452 [==============================] - 3s 545us/step - loss: 0.6399 - acc: 0.7678\n",
      "Epoch 13/125\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 0.6226 - acc: 0.7702\n",
      "Epoch 14/125\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 0.6195 - acc: 0.7724\n",
      "Epoch 15/125\n",
      "5452/5452 [==============================] - 3s 576us/step - loss: 0.6175 - acc: 0.7674\n",
      "Epoch 16/125\n",
      "5452/5452 [==============================] - 3s 639us/step - loss: 0.6055 - acc: 0.7742\n",
      "Epoch 17/125\n",
      "5452/5452 [==============================] - 3s 636us/step - loss: 0.5889 - acc: 0.7775\n",
      "Epoch 18/125\n",
      "5452/5452 [==============================] - 3s 617us/step - loss: 0.5743 - acc: 0.7937\n",
      "Epoch 19/125\n",
      "5452/5452 [==============================] - 3s 582us/step - loss: 0.5562 - acc: 0.7896\n",
      "Epoch 20/125\n",
      "5452/5452 [==============================] - 3s 592us/step - loss: 0.5624 - acc: 0.7920\n",
      "Epoch 21/125\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 0.5414 - acc: 0.7982\n",
      "Epoch 22/125\n",
      "5452/5452 [==============================] - 4s 644us/step - loss: 0.5393 - acc: 0.8030\n",
      "Epoch 23/125\n",
      "5452/5452 [==============================] - 3s 597us/step - loss: 0.5321 - acc: 0.8041\n",
      "Epoch 24/125\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 0.5127 - acc: 0.8109\n",
      "Epoch 25/125\n",
      "5452/5452 [==============================] - 3s 576us/step - loss: 0.5151 - acc: 0.8085\n",
      "Epoch 26/125\n",
      "5452/5452 [==============================] - 3s 591us/step - loss: 0.5035 - acc: 0.8160\n",
      "Epoch 27/125\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 0.4943 - acc: 0.8197\n",
      "Epoch 28/125\n",
      "5452/5452 [==============================] - 3s 582us/step - loss: 0.4905 - acc: 0.8160\n",
      "Epoch 29/125\n",
      "5452/5452 [==============================] - 3s 540us/step - loss: 0.4818 - acc: 0.8236\n",
      "Epoch 30/125\n",
      "5452/5452 [==============================] - 3s 544us/step - loss: 0.4682 - acc: 0.8316\n",
      "Epoch 31/125\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 0.4696 - acc: 0.8289\n",
      "Epoch 32/125\n",
      "5452/5452 [==============================] - 3s 569us/step - loss: 0.4583 - acc: 0.8300\n",
      "Epoch 33/125\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.4509 - acc: 0.8305\n",
      "Epoch 34/125\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 0.4446 - acc: 0.8369\n",
      "Epoch 35/125\n",
      "5452/5452 [==============================] - 3s 582us/step - loss: 0.4308 - acc: 0.8410\n",
      "Epoch 36/125\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 0.4127 - acc: 0.8461\n",
      "Epoch 37/125\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 0.4041 - acc: 0.8485\n",
      "Epoch 38/125\n",
      "5452/5452 [==============================] - 3s 576us/step - loss: 0.3997 - acc: 0.8525\n",
      "Epoch 39/125\n",
      "5452/5452 [==============================] - 3s 561us/step - loss: 0.4031 - acc: 0.8542\n",
      "Epoch 40/125\n",
      "5452/5452 [==============================] - 3s 563us/step - loss: 0.3847 - acc: 0.8577\n",
      "Epoch 41/125\n",
      "5452/5452 [==============================] - 3s 631us/step - loss: 0.3821 - acc: 0.8564\n",
      "Epoch 42/125\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 0.3759 - acc: 0.8615\n",
      "Epoch 43/125\n",
      "5452/5452 [==============================] - 3s 550us/step - loss: 0.3706 - acc: 0.8637\n",
      "Epoch 44/125\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 0.3679 - acc: 0.8685\n",
      "Epoch 45/125\n",
      "5452/5452 [==============================] - 3s 591us/step - loss: 0.3507 - acc: 0.8692\n",
      "Epoch 46/125\n",
      "5452/5452 [==============================] - 4s 665us/step - loss: 0.3510 - acc: 0.8705\n",
      "Epoch 47/125\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 0.3396 - acc: 0.8744\n",
      "Epoch 48/125\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.3375 - acc: 0.8712\n",
      "Epoch 49/125\n",
      "5452/5452 [==============================] - 3s 554us/step - loss: 0.3313 - acc: 0.8784\n",
      "Epoch 50/125\n",
      "5452/5452 [==============================] - 3s 629us/step - loss: 0.3177 - acc: 0.8824\n",
      "Epoch 51/125\n",
      "5452/5452 [==============================] - 3s 603us/step - loss: 0.3127 - acc: 0.8819\n",
      "Epoch 52/125\n",
      "5452/5452 [==============================] - 3s 631us/step - loss: 0.3087 - acc: 0.8868\n",
      "Epoch 53/125\n",
      "5452/5452 [==============================] - 4s 677us/step - loss: 0.3044 - acc: 0.8841\n",
      "Epoch 54/125\n",
      "5452/5452 [==============================] - 3s 626us/step - loss: 0.2863 - acc: 0.8947\n",
      "Epoch 55/125\n",
      "5452/5452 [==============================] - 3s 592us/step - loss: 0.3098 - acc: 0.8890\n",
      "Epoch 56/125\n",
      "5452/5452 [==============================] - 3s 609us/step - loss: 0.2894 - acc: 0.8951\n",
      "Epoch 57/125\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 0.2815 - acc: 0.8956\n",
      "Epoch 58/125\n",
      "5452/5452 [==============================] - 3s 561us/step - loss: 0.2827 - acc: 0.8934\n",
      "Epoch 59/125\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 0.2570 - acc: 0.9015\n",
      "Epoch 60/125\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.2604 - acc: 0.9081\n",
      "Epoch 61/125\n",
      "5452/5452 [==============================] - 3s 591us/step - loss: 0.2572 - acc: 0.9043\n",
      "Epoch 62/125\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 0.2534 - acc: 0.9096\n",
      "Epoch 63/125\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 0.2466 - acc: 0.9074\n",
      "Epoch 64/125\n",
      "5452/5452 [==============================] - 3s 632us/step - loss: 0.2419 - acc: 0.9120\n",
      "Epoch 65/125\n",
      "5452/5452 [==============================] - 3s 554us/step - loss: 0.2336 - acc: 0.9132\n",
      "Epoch 66/125\n",
      "5452/5452 [==============================] - 3s 629us/step - loss: 0.2292 - acc: 0.9167\n",
      "Epoch 67/125\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.2321 - acc: 0.9138\n",
      "Epoch 68/125\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.2176 - acc: 0.9189\n",
      "Epoch 69/125\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 0.2256 - acc: 0.9189\n",
      "Epoch 70/125\n",
      "5452/5452 [==============================] - 3s 585us/step - loss: 0.2118 - acc: 0.9204\n",
      "Epoch 71/125\n",
      "5452/5452 [==============================] - 4s 661us/step - loss: 0.2029 - acc: 0.9239\n",
      "Epoch 72/125\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.2065 - acc: 0.9242\n",
      "Epoch 73/125\n",
      "5452/5452 [==============================] - 3s 635us/step - loss: 0.1877 - acc: 0.9301\n",
      "Epoch 74/125\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 0.1912 - acc: 0.9298\n",
      "Epoch 75/125\n",
      "5452/5452 [==============================] - 3s 609us/step - loss: 0.1964 - acc: 0.9316\n",
      "Epoch 76/125\n",
      "5452/5452 [==============================] - 4s 657us/step - loss: 0.1854 - acc: 0.9299\n",
      "Epoch 77/125\n",
      "5452/5452 [==============================] - 4s 732us/step - loss: 0.1933 - acc: 0.9310\n",
      "Epoch 78/125\n",
      "5452/5452 [==============================] - 4s 661us/step - loss: 0.1770 - acc: 0.9358\n",
      "Epoch 79/125\n",
      "5452/5452 [==============================] - 3s 642us/step - loss: 0.1860 - acc: 0.9342\n",
      "Epoch 80/125\n",
      "5452/5452 [==============================] - 3s 617us/step - loss: 0.1798 - acc: 0.9371\n",
      "Epoch 81/125\n",
      "5452/5452 [==============================] - 3s 630us/step - loss: 0.1662 - acc: 0.9448\n",
      "Epoch 82/125\n",
      "5452/5452 [==============================] - 4s 665us/step - loss: 0.1695 - acc: 0.9378\n",
      "Epoch 83/125\n",
      "5452/5452 [==============================] - 3s 618us/step - loss: 0.1682 - acc: 0.9419\n",
      "Epoch 84/125\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 0.1593 - acc: 0.9424\n",
      "Epoch 85/125\n",
      "5452/5452 [==============================] - 4s 657us/step - loss: 0.1579 - acc: 0.9453\n",
      "Epoch 86/125\n",
      "5452/5452 [==============================] - 4s 642us/step - loss: 0.1565 - acc: 0.9433\n",
      "Epoch 87/125\n",
      "5452/5452 [==============================] - 3s 638us/step - loss: 0.1421 - acc: 0.9486\n",
      "Epoch 88/125\n",
      "5452/5452 [==============================] - 3s 617us/step - loss: 0.1522 - acc: 0.9470\n",
      "Epoch 89/125\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.1329 - acc: 0.9508\n",
      "Epoch 90/125\n",
      "5452/5452 [==============================] - 3s 641us/step - loss: 0.1538 - acc: 0.9452\n",
      "Epoch 91/125\n",
      "5452/5452 [==============================] - 4s 643us/step - loss: 0.1505 - acc: 0.9452\n",
      "Epoch 92/125\n",
      "5452/5452 [==============================] - 3s 626us/step - loss: 0.1327 - acc: 0.9552\n",
      "Epoch 93/125\n",
      "5452/5452 [==============================] - 4s 649us/step - loss: 0.1362 - acc: 0.9514\n",
      "Epoch 94/125\n",
      "5452/5452 [==============================] - 3s 634us/step - loss: 0.1463 - acc: 0.9508\n",
      "Epoch 95/125\n",
      "5452/5452 [==============================] - 3s 622us/step - loss: 0.1283 - acc: 0.9538\n",
      "Epoch 96/125\n",
      "5452/5452 [==============================] - 3s 635us/step - loss: 0.1269 - acc: 0.9587\n",
      "Epoch 97/125\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 0.1186 - acc: 0.9595\n",
      "Epoch 98/125\n",
      "5452/5452 [==============================] - 3s 581us/step - loss: 0.1085 - acc: 0.9615\n",
      "Epoch 99/125\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 0.1267 - acc: 0.9536\n",
      "Epoch 100/125\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.1290 - acc: 0.9551\n",
      "Epoch 101/125\n",
      "5452/5452 [==============================] - 3s 605us/step - loss: 0.1164 - acc: 0.9628\n",
      "Epoch 102/125\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 0.1176 - acc: 0.9602\n",
      "Epoch 103/125\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.1027 - acc: 0.9631\n",
      "Epoch 104/125\n",
      "5452/5452 [==============================] - 3s 626us/step - loss: 0.1178 - acc: 0.9595\n",
      "Epoch 105/125\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 0.1116 - acc: 0.9629\n",
      "Epoch 106/125\n",
      "5452/5452 [==============================] - 3s 634us/step - loss: 0.1040 - acc: 0.9629\n",
      "Epoch 107/125\n",
      "5452/5452 [==============================] - 3s 624us/step - loss: 0.1056 - acc: 0.9640\n",
      "Epoch 108/125\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 0.1030 - acc: 0.9648\n",
      "Epoch 109/125\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 0.0978 - acc: 0.9697\n",
      "Epoch 110/125\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.1007 - acc: 0.9661\n",
      "Epoch 111/125\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 0.0947 - acc: 0.9685\n",
      "Epoch 112/125\n",
      "5452/5452 [==============================] - 3s 633us/step - loss: 0.0998 - acc: 0.9640\n",
      "Epoch 113/125\n",
      "5452/5452 [==============================] - 3s 630us/step - loss: 0.1030 - acc: 0.9664\n",
      "Epoch 114/125\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 0.0823 - acc: 0.9705\n",
      "Epoch 115/125\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 0.1006 - acc: 0.9648\n",
      "Epoch 116/125\n",
      "5452/5452 [==============================] - 3s 633us/step - loss: 0.0862 - acc: 0.9692\n",
      "Epoch 117/125\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 0.0882 - acc: 0.9692\n",
      "Epoch 118/125\n",
      "5452/5452 [==============================] - 3s 641us/step - loss: 0.0909 - acc: 0.9663\n",
      "Epoch 119/125\n",
      "5452/5452 [==============================] - 3s 625us/step - loss: 0.0962 - acc: 0.9677\n",
      "Epoch 120/125\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 0.0788 - acc: 0.9754\n",
      "Epoch 121/125\n",
      "5452/5452 [==============================] - 3s 618us/step - loss: 0.0809 - acc: 0.9730\n",
      "Epoch 122/125\n",
      "5452/5452 [==============================] - 3s 631us/step - loss: 0.0830 - acc: 0.9714\n",
      "Epoch 123/125\n",
      "5452/5452 [==============================] - 4s 661us/step - loss: 0.0935 - acc: 0.9699\n",
      "Epoch 124/125\n",
      "5452/5452 [==============================] - 3s 602us/step - loss: 0.0758 - acc: 0.9765\n",
      "Epoch 125/125\n",
      "5452/5452 [==============================] - 3s 604us/step - loss: 0.0741 - acc: 0.9749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2ced290>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model4.fit(x = params4.get_train_padded(),\n",
    "           y = params4.get_train_labels_onehot(),\n",
    "           epochs = 125\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 86.4000000477%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model4.evaluate(params4.get_test_padded(),\n",
    "                params4.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the model for later use\n",
    "model4.save(\"../models/trec_lstm1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the associated encoder\n",
    "with open('../models/encoder1.pickle', 'wb') as handle:\n",
    "    pkl.dump(params4.label_encoder, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Trying dropout and recurrent dropout within the LSTM cells\n",
    "\n",
    "This will mask some of the time-specific idiosyncracies in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params5 = LstmParams(sequence_length = 10,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model5 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model5.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params5.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model5.add(LSTM(50,dropout=0.2, recurrent_dropout=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a Dense layer, and apply the softmax activation on their outputs. \n",
    "model5.add(Dense(params5.get_num_classes(), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model5.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 10, 300)           27978300  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 28,048,806\n",
      "Trainable params: 70,506\n",
      "Non-trainable params: 27,978,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "5452/5452 [==============================] - 5s 854us/step - loss: 1.6134 - acc: 0.2982\n",
      "Epoch 2/150\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 1.1286 - acc: 0.5429\n",
      "Epoch 3/150\n",
      "5452/5452 [==============================] - 3s 594us/step - loss: 0.9212 - acc: 0.6467\n",
      "Epoch 4/150\n",
      "5452/5452 [==============================] - 3s 635us/step - loss: 0.8352 - acc: 0.6875\n",
      "Epoch 5/150\n",
      "5452/5452 [==============================] - 3s 631us/step - loss: 0.7908 - acc: 0.6986\n",
      "Epoch 6/150\n",
      "5452/5452 [==============================] - 3s 622us/step - loss: 0.7501 - acc: 0.7137\n",
      "Epoch 7/150\n",
      "5452/5452 [==============================] - 4s 722us/step - loss: 0.7351 - acc: 0.7207\n",
      "Epoch 8/150\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.7058 - acc: 0.7333\n",
      "Epoch 9/150\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.6942 - acc: 0.7379\n",
      "Epoch 10/150\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 0.6837 - acc: 0.7458\n",
      "Epoch 11/150\n",
      "5452/5452 [==============================] - 4s 681us/step - loss: 0.6754 - acc: 0.7496\n",
      "Epoch 12/150\n",
      "5452/5452 [==============================] - 3s 641us/step - loss: 0.6496 - acc: 0.7605\n",
      "Epoch 13/150\n",
      "5452/5452 [==============================] - 3s 596us/step - loss: 0.6577 - acc: 0.7540\n",
      "Epoch 14/150\n",
      "5452/5452 [==============================] - 3s 631us/step - loss: 0.6519 - acc: 0.7625\n",
      "Epoch 15/150\n",
      "5452/5452 [==============================] - 3s 625us/step - loss: 0.6346 - acc: 0.7647\n",
      "Epoch 16/150\n",
      "5452/5452 [==============================] - 3s 617us/step - loss: 0.6284 - acc: 0.7731\n",
      "Epoch 17/150\n",
      "5452/5452 [==============================] - 3s 619us/step - loss: 0.6034 - acc: 0.7760\n",
      "Epoch 18/150\n",
      "5452/5452 [==============================] - 3s 602us/step - loss: 0.6096 - acc: 0.7738\n",
      "Epoch 19/150\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 0.5951 - acc: 0.7817\n",
      "Epoch 20/150\n",
      "5452/5452 [==============================] - 3s 598us/step - loss: 0.5903 - acc: 0.7792\n",
      "Epoch 21/150\n",
      "5452/5452 [==============================] - 3s 594us/step - loss: 0.5774 - acc: 0.7883\n",
      "Epoch 22/150\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.5770 - acc: 0.7867\n",
      "Epoch 23/150\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 0.5644 - acc: 0.7905\n",
      "Epoch 24/150\n",
      "5452/5452 [==============================] - 3s 625us/step - loss: 0.5664 - acc: 0.7894\n",
      "Epoch 25/150\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 0.5517 - acc: 0.7944\n",
      "Epoch 26/150\n",
      "5452/5452 [==============================] - 4s 647us/step - loss: 0.5467 - acc: 0.7975\n",
      "Epoch 27/150\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 0.5398 - acc: 0.8006\n",
      "Epoch 28/150\n",
      "5452/5452 [==============================] - 3s 596us/step - loss: 0.5355 - acc: 0.8019\n",
      "Epoch 29/150\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 0.5283 - acc: 0.8017\n",
      "Epoch 30/150\n",
      "5452/5452 [==============================] - 3s 624us/step - loss: 0.5122 - acc: 0.8151\n",
      "Epoch 31/150\n",
      "5452/5452 [==============================] - 3s 629us/step - loss: 0.5051 - acc: 0.8142\n",
      "Epoch 32/150\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 0.5005 - acc: 0.8131\n",
      "Epoch 33/150\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 0.4991 - acc: 0.8173\n",
      "Epoch 34/150\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.4794 - acc: 0.8245\n",
      "Epoch 35/150\n",
      "5452/5452 [==============================] - 3s 636us/step - loss: 0.4787 - acc: 0.8245\n",
      "Epoch 36/150\n",
      "5452/5452 [==============================] - 3s 610us/step - loss: 0.4765 - acc: 0.8212\n",
      "Epoch 37/150\n",
      "5452/5452 [==============================] - 3s 584us/step - loss: 0.4767 - acc: 0.8226\n",
      "Epoch 38/150\n",
      "5452/5452 [==============================] - 3s 626us/step - loss: 0.4627 - acc: 0.8357\n",
      "Epoch 39/150\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.4599 - acc: 0.8289\n",
      "Epoch 40/150\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.4429 - acc: 0.8335\n",
      "Epoch 41/150\n",
      "5452/5452 [==============================] - 3s 638us/step - loss: 0.4468 - acc: 0.8335\n",
      "Epoch 42/150\n",
      "5452/5452 [==============================] - 3s 632us/step - loss: 0.4403 - acc: 0.8347\n",
      "Epoch 43/150\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.4339 - acc: 0.8408\n",
      "Epoch 44/150\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 0.4243 - acc: 0.8382\n",
      "Epoch 45/150\n",
      "5452/5452 [==============================] - 3s 618us/step - loss: 0.4129 - acc: 0.8463\n",
      "Epoch 46/150\n",
      "5452/5452 [==============================] - 3s 604us/step - loss: 0.4175 - acc: 0.8478\n",
      "Epoch 47/150\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 0.4006 - acc: 0.8527\n",
      "Epoch 48/150\n",
      "5452/5452 [==============================] - 3s 617us/step - loss: 0.4047 - acc: 0.8435\n",
      "Epoch 49/150\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 0.3942 - acc: 0.8591\n",
      "Epoch 50/150\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 0.3926 - acc: 0.8527\n",
      "Epoch 51/150\n",
      "5452/5452 [==============================] - 3s 627us/step - loss: 0.3855 - acc: 0.8580\n",
      "Epoch 52/150\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.3810 - acc: 0.8650\n",
      "Epoch 53/150\n",
      "5452/5452 [==============================] - 4s 646us/step - loss: 0.3723 - acc: 0.8654\n",
      "Epoch 54/150\n",
      "5452/5452 [==============================] - 4s 647us/step - loss: 0.3700 - acc: 0.8582\n",
      "Epoch 55/150\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 0.3669 - acc: 0.8621\n",
      "Epoch 56/150\n",
      "5452/5452 [==============================] - 4s 651us/step - loss: 0.3625 - acc: 0.8657\n",
      "Epoch 57/150\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.3433 - acc: 0.8736\n",
      "Epoch 58/150\n",
      "5452/5452 [==============================] - 4s 655us/step - loss: 0.3310 - acc: 0.8777\n",
      "Epoch 59/150\n",
      "5452/5452 [==============================] - 4s 678us/step - loss: 0.3324 - acc: 0.8784\n",
      "Epoch 60/150\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.3216 - acc: 0.8832\n",
      "Epoch 61/150\n",
      "5452/5452 [==============================] - 4s 665us/step - loss: 0.3271 - acc: 0.8786\n",
      "Epoch 62/150\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 0.3231 - acc: 0.8784\n",
      "Epoch 63/150\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 0.3055 - acc: 0.8912\n",
      "Epoch 64/150\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.3045 - acc: 0.8920\n",
      "Epoch 65/150\n",
      "5452/5452 [==============================] - 3s 622us/step - loss: 0.3028 - acc: 0.8936\n",
      "Epoch 66/150\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.3028 - acc: 0.8914\n",
      "Epoch 67/150\n",
      "5452/5452 [==============================] - 4s 676us/step - loss: 0.2859 - acc: 0.8947\n",
      "Epoch 68/150\n",
      "5452/5452 [==============================] - 4s 683us/step - loss: 0.2842 - acc: 0.8953\n",
      "Epoch 69/150\n",
      "5452/5452 [==============================] - 4s 668us/step - loss: 0.2797 - acc: 0.8966\n",
      "Epoch 70/150\n",
      "5452/5452 [==============================] - 4s 672us/step - loss: 0.2838 - acc: 0.8966\n",
      "Epoch 71/150\n",
      "5452/5452 [==============================] - 4s 671us/step - loss: 0.2890 - acc: 0.8955\n",
      "Epoch 72/150\n",
      "5452/5452 [==============================] - 4s 649us/step - loss: 0.2703 - acc: 0.9002\n",
      "Epoch 73/150\n",
      "5452/5452 [==============================] - 4s 663us/step - loss: 0.2632 - acc: 0.9033\n",
      "Epoch 74/150\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.2728 - acc: 0.9010\n",
      "Epoch 75/150\n",
      "5452/5452 [==============================] - 4s 676us/step - loss: 0.2510 - acc: 0.9110\n",
      "Epoch 76/150\n",
      "5452/5452 [==============================] - 4s 682us/step - loss: 0.2652 - acc: 0.9043\n",
      "Epoch 77/150\n",
      "5452/5452 [==============================] - 4s 677us/step - loss: 0.2483 - acc: 0.9087\n",
      "Epoch 78/150\n",
      "5452/5452 [==============================] - 4s 677us/step - loss: 0.2503 - acc: 0.9110\n",
      "Epoch 79/150\n",
      "5452/5452 [==============================] - 4s 663us/step - loss: 0.2281 - acc: 0.9191\n",
      "Epoch 80/150\n",
      "5452/5452 [==============================] - 4s 674us/step - loss: 0.2326 - acc: 0.9176\n",
      "Epoch 81/150\n",
      "5452/5452 [==============================] - 4s 666us/step - loss: 0.2248 - acc: 0.9211\n",
      "Epoch 82/150\n",
      "5452/5452 [==============================] - 4s 664us/step - loss: 0.2244 - acc: 0.9197\n",
      "Epoch 83/150\n",
      "5452/5452 [==============================] - 4s 674us/step - loss: 0.2209 - acc: 0.9180\n",
      "Epoch 84/150\n",
      "5452/5452 [==============================] - 4s 707us/step - loss: 0.2204 - acc: 0.9202\n",
      "Epoch 85/150\n",
      "5452/5452 [==============================] - 4s 651us/step - loss: 0.2144 - acc: 0.9252\n",
      "Epoch 86/150\n",
      "5452/5452 [==============================] - 4s 672us/step - loss: 0.2072 - acc: 0.9250\n",
      "Epoch 87/150\n",
      "5452/5452 [==============================] - 4s 659us/step - loss: 0.2234 - acc: 0.9208\n",
      "Epoch 88/150\n",
      "5452/5452 [==============================] - 4s 687us/step - loss: 0.2056 - acc: 0.9226\n",
      "Epoch 89/150\n",
      "5452/5452 [==============================] - 4s 681us/step - loss: 0.2097 - acc: 0.9255\n",
      "Epoch 90/150\n",
      "5452/5452 [==============================] - 4s 662us/step - loss: 0.2090 - acc: 0.9281\n",
      "Epoch 91/150\n",
      "5452/5452 [==============================] - 4s 664us/step - loss: 0.2010 - acc: 0.9285\n",
      "Epoch 92/150\n",
      "5452/5452 [==============================] - 4s 670us/step - loss: 0.1925 - acc: 0.9296\n",
      "Epoch 93/150\n",
      "5452/5452 [==============================] - 4s 657us/step - loss: 0.1983 - acc: 0.9296\n",
      "Epoch 94/150\n",
      "5452/5452 [==============================] - 4s 686us/step - loss: 0.1852 - acc: 0.9321\n",
      "Epoch 95/150\n",
      "5452/5452 [==============================] - 4s 684us/step - loss: 0.1769 - acc: 0.9369\n",
      "Epoch 96/150\n",
      "5452/5452 [==============================] - 4s 649us/step - loss: 0.1872 - acc: 0.9318\n",
      "Epoch 97/150\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.1809 - acc: 0.9369\n",
      "Epoch 98/150\n",
      "5452/5452 [==============================] - 3s 630us/step - loss: 0.1771 - acc: 0.9369\n",
      "Epoch 99/150\n",
      "5452/5452 [==============================] - 4s 663us/step - loss: 0.1707 - acc: 0.9397\n",
      "Epoch 100/150\n",
      "5452/5452 [==============================] - 4s 667us/step - loss: 0.1834 - acc: 0.9382\n",
      "Epoch 101/150\n",
      "5452/5452 [==============================] - 4s 662us/step - loss: 0.1707 - acc: 0.9373\n",
      "Epoch 102/150\n",
      "5452/5452 [==============================] - 4s 642us/step - loss: 0.1549 - acc: 0.9461\n",
      "Epoch 103/150\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 0.1625 - acc: 0.9387\n",
      "Epoch 104/150\n",
      "5452/5452 [==============================] - 4s 653us/step - loss: 0.1663 - acc: 0.9369\n",
      "Epoch 105/150\n",
      "5452/5452 [==============================] - 3s 635us/step - loss: 0.1512 - acc: 0.9444\n",
      "Epoch 106/150\n",
      "5452/5452 [==============================] - 4s 648us/step - loss: 0.1595 - acc: 0.9439\n",
      "Epoch 107/150\n",
      "5452/5452 [==============================] - 3s 633us/step - loss: 0.1556 - acc: 0.9463\n",
      "Epoch 108/150\n",
      "5452/5452 [==============================] - 4s 659us/step - loss: 0.1559 - acc: 0.9457\n",
      "Epoch 109/150\n",
      "5452/5452 [==============================] - 4s 669us/step - loss: 0.1529 - acc: 0.9492\n",
      "Epoch 110/150\n",
      "5452/5452 [==============================] - 4s 652us/step - loss: 0.1535 - acc: 0.9466\n",
      "Epoch 111/150\n",
      "5452/5452 [==============================] - 4s 647us/step - loss: 0.1495 - acc: 0.9481\n",
      "Epoch 112/150\n",
      "5452/5452 [==============================] - 4s 670us/step - loss: 0.1490 - acc: 0.9477\n",
      "Epoch 113/150\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 0.1430 - acc: 0.9519\n",
      "Epoch 114/150\n",
      "5452/5452 [==============================] - 4s 656us/step - loss: 0.1353 - acc: 0.9538\n",
      "Epoch 115/150\n",
      "5452/5452 [==============================] - 4s 668us/step - loss: 0.1368 - acc: 0.9529\n",
      "Epoch 116/150\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.1384 - acc: 0.9525\n",
      "Epoch 117/150\n",
      "5452/5452 [==============================] - 4s 671us/step - loss: 0.1374 - acc: 0.9530\n",
      "Epoch 118/150\n",
      "5452/5452 [==============================] - 4s 663us/step - loss: 0.1403 - acc: 0.9496\n",
      "Epoch 119/150\n",
      "5452/5452 [==============================] - 4s 666us/step - loss: 0.1330 - acc: 0.9565\n",
      "Epoch 120/150\n",
      "5452/5452 [==============================] - 4s 659us/step - loss: 0.1261 - acc: 0.9565\n",
      "Epoch 121/150\n",
      "5452/5452 [==============================] - 4s 663us/step - loss: 0.1241 - acc: 0.9576\n",
      "Epoch 122/150\n",
      "5452/5452 [==============================] - 4s 644us/step - loss: 0.1248 - acc: 0.9551\n",
      "Epoch 123/150\n",
      "5452/5452 [==============================] - 4s 655us/step - loss: 0.1239 - acc: 0.9558\n",
      "Epoch 124/150\n",
      "5452/5452 [==============================] - 4s 669us/step - loss: 0.1147 - acc: 0.9615\n",
      "Epoch 125/150\n",
      "5452/5452 [==============================] - 4s 673us/step - loss: 0.1253 - acc: 0.9576\n",
      "Epoch 126/150\n",
      "5452/5452 [==============================] - 4s 646us/step - loss: 0.1092 - acc: 0.9631\n",
      "Epoch 127/150\n",
      "5452/5452 [==============================] - 4s 659us/step - loss: 0.1137 - acc: 0.9635\n",
      "Epoch 128/150\n",
      "5452/5452 [==============================] - 4s 658us/step - loss: 0.1174 - acc: 0.9595\n",
      "Epoch 129/150\n",
      "5452/5452 [==============================] - 4s 670us/step - loss: 0.1135 - acc: 0.9617\n",
      "Epoch 130/150\n",
      "5452/5452 [==============================] - 4s 661us/step - loss: 0.1113 - acc: 0.9637\n",
      "Epoch 131/150\n",
      "5452/5452 [==============================] - 3s 620us/step - loss: 0.1060 - acc: 0.9668\n",
      "Epoch 132/150\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.1068 - acc: 0.9652\n",
      "Epoch 133/150\n",
      "5452/5452 [==============================] - 4s 654us/step - loss: 0.1069 - acc: 0.9635\n",
      "Epoch 134/150\n",
      "5452/5452 [==============================] - 3s 633us/step - loss: 0.1122 - acc: 0.9653\n",
      "Epoch 135/150\n",
      "5452/5452 [==============================] - 3s 639us/step - loss: 0.1088 - acc: 0.9640\n",
      "Epoch 136/150\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.0997 - acc: 0.9664\n",
      "Epoch 137/150\n",
      "5452/5452 [==============================] - 4s 672us/step - loss: 0.0920 - acc: 0.9699\n",
      "Epoch 138/150\n",
      "5452/5452 [==============================] - 4s 649us/step - loss: 0.0985 - acc: 0.9650\n",
      "Epoch 139/150\n",
      "5452/5452 [==============================] - 4s 662us/step - loss: 0.0897 - acc: 0.9718\n",
      "Epoch 140/150\n",
      "5452/5452 [==============================] - 4s 663us/step - loss: 0.1066 - acc: 0.9644\n",
      "Epoch 141/150\n",
      "5452/5452 [==============================] - 4s 675us/step - loss: 0.0914 - acc: 0.9701\n",
      "Epoch 142/150\n",
      "5452/5452 [==============================] - 4s 670us/step - loss: 0.1006 - acc: 0.9648\n",
      "Epoch 143/150\n",
      "5452/5452 [==============================] - 3s 638us/step - loss: 0.0890 - acc: 0.9707\n",
      "Epoch 144/150\n",
      "5452/5452 [==============================] - 4s 651us/step - loss: 0.0904 - acc: 0.9690\n",
      "Epoch 145/150\n",
      "5452/5452 [==============================] - 3s 634us/step - loss: 0.0850 - acc: 0.9730\n",
      "Epoch 146/150\n",
      "5452/5452 [==============================] - 3s 634us/step - loss: 0.0876 - acc: 0.9697\n",
      "Epoch 147/150\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 0.0880 - acc: 0.9705\n",
      "Epoch 148/150\n",
      "5452/5452 [==============================] - 4s 657us/step - loss: 0.0892 - acc: 0.9708\n",
      "Epoch 149/150\n",
      "5452/5452 [==============================] - 4s 653us/step - loss: 0.0890 - acc: 0.9696\n",
      "Epoch 150/150\n",
      "5452/5452 [==============================] - 3s 609us/step - loss: 0.0901 - acc: 0.9703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2f6e310>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model5.fit(x = params5.get_train_padded(),\n",
    "           y = params5.get_train_labels_onehot(),\n",
    "           epochs = 150\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 88.1999999046%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model5.evaluate(params5.get_test_padded(),\n",
    "                params5.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no telling whether this increase in performance really means that using internal dropout leads to a better classifier - I'm certainly overfitting to the test data by tuning my hyperparameters according to the test error. \n",
    "\n",
    "It does show, however, that internal droput seems to combat overfitting reasonably effectively. Still not enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model5.save(\"../models/trec_lstm2.h5\")\n",
    "\n",
    "#also write associated encoder\n",
    "with open(\"../models/encoder2.pickle\", \"wb\") as handle:\n",
    "    pkl.dump(params5.label_encoder, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 expanding the word context window. \n",
    "\n",
    "So far, I've been working with a context window of 10 words. Perhaps 15 will be better? I'll try it with my two most trecent model configurations, `model4` and `model5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params6 = LstmParams(sequence_length = 15,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model6 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model6.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params6.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a dropout layer\n",
    "model6.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model6.add(LSTM(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model6.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a Dense layer, and apply the softmax activation on their outputs. \n",
    "model6.add(Dense(params6.get_num_classes(), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model6.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 15, 300)           27978300  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 28,048,806\n",
      "Trainable params: 70,506\n",
      "Non-trainable params: 27,978,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5452/5452 [==============================] - 10s 2ms/step - loss: 1.6449 - acc: 0.2742\n",
      "Epoch 2/100\n",
      "5452/5452 [==============================] - 5s 846us/step - loss: 1.2574 - acc: 0.4545\n",
      "Epoch 3/100\n",
      "5452/5452 [==============================] - 4s 779us/step - loss: 1.1335 - acc: 0.5216\n",
      "Epoch 4/100\n",
      "5452/5452 [==============================] - 4s 802us/step - loss: 1.0463 - acc: 0.5743\n",
      "Epoch 5/100\n",
      "5452/5452 [==============================] - 4s 730us/step - loss: 0.9044 - acc: 0.6554\n",
      "Epoch 6/100\n",
      "5452/5452 [==============================] - 4s 745us/step - loss: 0.8194 - acc: 0.6911\n",
      "Epoch 7/100\n",
      "5452/5452 [==============================] - 4s 799us/step - loss: 0.7748 - acc: 0.7133\n",
      "Epoch 8/100\n",
      "5452/5452 [==============================] - 5s 840us/step - loss: 0.7357 - acc: 0.7197\n",
      "Epoch 9/100\n",
      "5452/5452 [==============================] - 5s 832us/step - loss: 0.7207 - acc: 0.7337\n",
      "Epoch 10/100\n",
      "5452/5452 [==============================] - 5s 843us/step - loss: 0.7001 - acc: 0.7383\n",
      "Epoch 11/100\n",
      "5452/5452 [==============================] - 4s 750us/step - loss: 0.6882 - acc: 0.7469\n",
      "Epoch 12/100\n",
      "5452/5452 [==============================] - 4s 720us/step - loss: 0.6606 - acc: 0.7592\n",
      "Epoch 13/100\n",
      "5452/5452 [==============================] - 4s 823us/step - loss: 0.6519 - acc: 0.7590\n",
      "Epoch 14/100\n",
      "5452/5452 [==============================] - 4s 792us/step - loss: 0.6462 - acc: 0.7645\n",
      "Epoch 15/100\n",
      "5452/5452 [==============================] - 4s 792us/step - loss: 0.6348 - acc: 0.7674\n",
      "Epoch 16/100\n",
      "5452/5452 [==============================] - 4s 822us/step - loss: 0.6255 - acc: 0.7748\n",
      "Epoch 17/100\n",
      "5452/5452 [==============================] - 4s 794us/step - loss: 0.6117 - acc: 0.7841\n",
      "Epoch 18/100\n",
      "5452/5452 [==============================] - 4s 783us/step - loss: 0.5868 - acc: 0.7909\n",
      "Epoch 19/100\n",
      "5452/5452 [==============================] - 4s 819us/step - loss: 0.5951 - acc: 0.7843\n",
      "Epoch 20/100\n",
      "5452/5452 [==============================] - 4s 799us/step - loss: 0.5742 - acc: 0.7905\n",
      "Epoch 21/100\n",
      "5452/5452 [==============================] - 4s 784us/step - loss: 0.5708 - acc: 0.7970\n",
      "Epoch 22/100\n",
      "5452/5452 [==============================] - 5s 833us/step - loss: 0.5685 - acc: 0.7948\n",
      "Epoch 23/100\n",
      "5452/5452 [==============================] - 4s 825us/step - loss: 0.5564 - acc: 0.8021\n",
      "Epoch 24/100\n",
      "5452/5452 [==============================] - 4s 819us/step - loss: 0.5498 - acc: 0.8045\n",
      "Epoch 25/100\n",
      "5452/5452 [==============================] - 4s 771us/step - loss: 0.5385 - acc: 0.8043\n",
      "Epoch 26/100\n",
      "5452/5452 [==============================] - 5s 831us/step - loss: 0.5263 - acc: 0.8091\n",
      "Epoch 27/100\n",
      "5452/5452 [==============================] - 4s 774us/step - loss: 0.5277 - acc: 0.8147\n",
      "Epoch 28/100\n",
      "5452/5452 [==============================] - 4s 822us/step - loss: 0.5126 - acc: 0.8164\n",
      "Epoch 29/100\n",
      "5452/5452 [==============================] - 4s 766us/step - loss: 0.5033 - acc: 0.8190\n",
      "Epoch 30/100\n",
      "5452/5452 [==============================] - 4s 802us/step - loss: 0.5092 - acc: 0.8136\n",
      "Epoch 31/100\n",
      "5452/5452 [==============================] - 4s 825us/step - loss: 0.4868 - acc: 0.8269\n",
      "Epoch 32/100\n",
      "5452/5452 [==============================] - 4s 719us/step - loss: 0.4813 - acc: 0.8303\n",
      "Epoch 33/100\n",
      "5452/5452 [==============================] - 4s 820us/step - loss: 0.4804 - acc: 0.8228\n",
      "Epoch 34/100\n",
      "5452/5452 [==============================] - 4s 763us/step - loss: 0.4663 - acc: 0.8336\n",
      "Epoch 35/100\n",
      "5452/5452 [==============================] - 4s 765us/step - loss: 0.4729 - acc: 0.8333\n",
      "Epoch 36/100\n",
      "5452/5452 [==============================] - 5s 844us/step - loss: 0.4571 - acc: 0.8391\n",
      "Epoch 37/100\n",
      "5452/5452 [==============================] - 4s 799us/step - loss: 0.4577 - acc: 0.8415\n",
      "Epoch 38/100\n",
      "5452/5452 [==============================] - 5s 841us/step - loss: 0.4362 - acc: 0.8489\n",
      "Epoch 39/100\n",
      "5452/5452 [==============================] - 5s 834us/step - loss: 0.4319 - acc: 0.8479\n",
      "Epoch 40/100\n",
      "5452/5452 [==============================] - 4s 748us/step - loss: 0.4281 - acc: 0.8505\n",
      "Epoch 41/100\n",
      "5452/5452 [==============================] - 4s 742us/step - loss: 0.4234 - acc: 0.8538\n",
      "Epoch 42/100\n",
      "5452/5452 [==============================] - 4s 759us/step - loss: 0.4192 - acc: 0.8496\n",
      "Epoch 43/100\n",
      "5452/5452 [==============================] - 4s 764us/step - loss: 0.4163 - acc: 0.8538\n",
      "Epoch 44/100\n",
      "5452/5452 [==============================] - 4s 783us/step - loss: 0.3952 - acc: 0.8602\n",
      "Epoch 45/100\n",
      "5452/5452 [==============================] - 4s 766us/step - loss: 0.3918 - acc: 0.8602\n",
      "Epoch 46/100\n",
      "5452/5452 [==============================] - 4s 786us/step - loss: 0.3864 - acc: 0.8637\n",
      "Epoch 47/100\n",
      "5452/5452 [==============================] - 4s 759us/step - loss: 0.3813 - acc: 0.8637\n",
      "Epoch 48/100\n",
      "5452/5452 [==============================] - 4s 762us/step - loss: 0.3705 - acc: 0.8718\n",
      "Epoch 49/100\n",
      "5452/5452 [==============================] - 4s 814us/step - loss: 0.3698 - acc: 0.8670\n",
      "Epoch 50/100\n",
      "5452/5452 [==============================] - 4s 788us/step - loss: 0.3696 - acc: 0.8705\n",
      "Epoch 51/100\n",
      "5452/5452 [==============================] - 4s 767us/step - loss: 0.3512 - acc: 0.8740\n",
      "Epoch 52/100\n",
      "5452/5452 [==============================] - 4s 809us/step - loss: 0.3417 - acc: 0.8854\n",
      "Epoch 53/100\n",
      "5452/5452 [==============================] - 4s 774us/step - loss: 0.3336 - acc: 0.8866\n",
      "Epoch 54/100\n",
      "5452/5452 [==============================] - 4s 794us/step - loss: 0.3230 - acc: 0.8857\n",
      "Epoch 55/100\n",
      "5452/5452 [==============================] - 4s 735us/step - loss: 0.3346 - acc: 0.8835\n",
      "Epoch 56/100\n",
      "5452/5452 [==============================] - 4s 780us/step - loss: 0.3262 - acc: 0.8901\n",
      "Epoch 57/100\n",
      "5452/5452 [==============================] - 4s 759us/step - loss: 0.2995 - acc: 0.8964\n",
      "Epoch 58/100\n",
      "5452/5452 [==============================] - 4s 762us/step - loss: 0.3078 - acc: 0.8912\n",
      "Epoch 59/100\n",
      "5452/5452 [==============================] - 4s 772us/step - loss: 0.3091 - acc: 0.8901\n",
      "Epoch 60/100\n",
      "5452/5452 [==============================] - 4s 740us/step - loss: 0.3012 - acc: 0.8903\n",
      "Epoch 61/100\n",
      "5452/5452 [==============================] - 4s 762us/step - loss: 0.3048 - acc: 0.8945\n",
      "Epoch 62/100\n",
      "5452/5452 [==============================] - 4s 728us/step - loss: 0.2990 - acc: 0.8967\n",
      "Epoch 63/100\n",
      "5452/5452 [==============================] - 4s 706us/step - loss: 0.2746 - acc: 0.9037\n",
      "Epoch 64/100\n",
      "5452/5452 [==============================] - 4s 789us/step - loss: 0.2809 - acc: 0.9000\n",
      "Epoch 65/100\n",
      "5452/5452 [==============================] - 4s 796us/step - loss: 0.2761 - acc: 0.8993\n",
      "Epoch 66/100\n",
      "5452/5452 [==============================] - 4s 784us/step - loss: 0.2765 - acc: 0.9044\n",
      "Epoch 67/100\n",
      "5452/5452 [==============================] - 4s 750us/step - loss: 0.2660 - acc: 0.9046\n",
      "Epoch 68/100\n",
      "5452/5452 [==============================] - 4s 753us/step - loss: 0.2502 - acc: 0.9114\n",
      "Epoch 69/100\n",
      "5452/5452 [==============================] - 4s 756us/step - loss: 0.2646 - acc: 0.9070\n",
      "Epoch 70/100\n",
      "5452/5452 [==============================] - 4s 779us/step - loss: 0.2474 - acc: 0.9134\n",
      "Epoch 71/100\n",
      "5452/5452 [==============================] - 4s 744us/step - loss: 0.2257 - acc: 0.9244\n",
      "Epoch 72/100\n",
      "5452/5452 [==============================] - 4s 787us/step - loss: 0.2274 - acc: 0.9198\n",
      "Epoch 73/100\n",
      "5452/5452 [==============================] - 4s 797us/step - loss: 0.2362 - acc: 0.9158\n",
      "Epoch 74/100\n",
      "5452/5452 [==============================] - 4s 777us/step - loss: 0.2296 - acc: 0.9215\n",
      "Epoch 75/100\n",
      "5452/5452 [==============================] - 4s 797us/step - loss: 0.2322 - acc: 0.9215\n",
      "Epoch 76/100\n",
      "5452/5452 [==============================] - 4s 757us/step - loss: 0.2071 - acc: 0.9261\n",
      "Epoch 77/100\n",
      "5452/5452 [==============================] - 4s 759us/step - loss: 0.2145 - acc: 0.9239\n",
      "Epoch 78/100\n",
      "5452/5452 [==============================] - 4s 789us/step - loss: 0.2153 - acc: 0.9222\n",
      "Epoch 79/100\n",
      "5452/5452 [==============================] - 4s 812us/step - loss: 0.2205 - acc: 0.9215\n",
      "Epoch 80/100\n",
      "5452/5452 [==============================] - 4s 803us/step - loss: 0.2118 - acc: 0.9264\n",
      "Epoch 81/100\n",
      "5452/5452 [==============================] - 5s 866us/step - loss: 0.2072 - acc: 0.9264\n",
      "Epoch 82/100\n",
      "5452/5452 [==============================] - 5s 885us/step - loss: 0.2102 - acc: 0.9224\n",
      "Epoch 83/100\n",
      "5452/5452 [==============================] - 4s 771us/step - loss: 0.1912 - acc: 0.9331\n",
      "Epoch 84/100\n",
      "5452/5452 [==============================] - 4s 775us/step - loss: 0.1990 - acc: 0.9285\n",
      "Epoch 85/100\n",
      "5452/5452 [==============================] - 4s 806us/step - loss: 0.1920 - acc: 0.9338\n",
      "Epoch 86/100\n",
      "5452/5452 [==============================] - 4s 760us/step - loss: 0.1776 - acc: 0.9398\n",
      "Epoch 87/100\n",
      "5452/5452 [==============================] - 4s 795us/step - loss: 0.1904 - acc: 0.9338\n",
      "Epoch 88/100\n",
      "5452/5452 [==============================] - 4s 778us/step - loss: 0.1850 - acc: 0.9349\n",
      "Epoch 89/100\n",
      "5452/5452 [==============================] - 4s 771us/step - loss: 0.1718 - acc: 0.9422\n",
      "Epoch 90/100\n",
      "5452/5452 [==============================] - 4s 748us/step - loss: 0.1813 - acc: 0.9369\n",
      "Epoch 91/100\n",
      "5452/5452 [==============================] - 4s 754us/step - loss: 0.1622 - acc: 0.9455\n",
      "Epoch 92/100\n",
      "5452/5452 [==============================] - 4s 750us/step - loss: 0.1664 - acc: 0.9426\n",
      "Epoch 93/100\n",
      "5452/5452 [==============================] - 4s 799us/step - loss: 0.1523 - acc: 0.9441\n",
      "Epoch 94/100\n",
      "5452/5452 [==============================] - 4s 804us/step - loss: 0.1571 - acc: 0.9446\n",
      "Epoch 95/100\n",
      "5452/5452 [==============================] - 4s 792us/step - loss: 0.1638 - acc: 0.9486\n",
      "Epoch 96/100\n",
      "5452/5452 [==============================] - 4s 778us/step - loss: 0.1534 - acc: 0.9474\n",
      "Epoch 97/100\n",
      "5452/5452 [==============================] - 4s 763us/step - loss: 0.1329 - acc: 0.9530\n",
      "Epoch 98/100\n",
      "5452/5452 [==============================] - 4s 785us/step - loss: 0.1451 - acc: 0.9525\n",
      "Epoch 99/100\n",
      "5452/5452 [==============================] - 4s 763us/step - loss: 0.1450 - acc: 0.9512\n",
      "Epoch 100/100\n",
      "5452/5452 [==============================] - 4s 754us/step - loss: 0.1526 - acc: 0.9486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23cb1c2d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model6.fit(x = params6.get_train_padded(),\n",
    "           y = params6.get_train_labels_onehot(),\n",
    "           epochs = 100\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 85.8000000477%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model6.evaluate(params6.get_test_padded(),\n",
    "                params6.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes no obvious difference. \n",
    "\n",
    "Because the data is so small, and I know my LSTM has a propensity to overfit, I prefer a smaller context window over a larger one (Occam's razor). \n",
    "\n",
    "Intuitively, it would make sense that the neccessary window for learning the broad categorization of sentence will be small, becuas seeing the words _What is the..._ versus _Who is the..._ might already tell you that the first question is a entity, while the second is an human. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Stacked LSTMs\n",
    "\n",
    "I'll now try stacking two LSTM's on top of one another, and using recurrent dropout. Hopefully, the second LSTM will learn some other (more intersting) features than the dense layer did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params7 = LstmParams(sequence_length = 10,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model7 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model7.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params7.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model7.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model7.add(LSTM(params7.get_num_classes(), dropout=0.2, recurrent_dropout=0.2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model7.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 10, 300)           27978300  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 10, 100)           160400    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 6)                 2568      \n",
      "=================================================================\n",
      "Total params: 28,141,268\n",
      "Trainable params: 162,968\n",
      "Non-trainable params: 27,978,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5452/5452 [==============================] - 11s 2ms/step - loss: 1.6728 - acc: 0.3472\n",
      "Epoch 2/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 1.2092 - acc: 0.5813\n",
      "Epoch 3/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 1.0563 - acc: 0.6526\n",
      "Epoch 4/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.9350 - acc: 0.6897\n",
      "Epoch 5/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.8127 - acc: 0.7049\n",
      "Epoch 6/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.8017 - acc: 0.7029\n",
      "Epoch 7/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.7702 - acc: 0.7131\n",
      "Epoch 8/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.7627 - acc: 0.7146\n",
      "Epoch 9/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.7407 - acc: 0.7322\n",
      "Epoch 10/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.7262 - acc: 0.7351\n",
      "Epoch 11/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.7726 - acc: 0.7129\n",
      "Epoch 12/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.7456 - acc: 0.7247\n",
      "Epoch 13/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.7185 - acc: 0.7372\n",
      "Epoch 14/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.7138 - acc: 0.7372\n",
      "Epoch 15/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.7146 - acc: 0.7383\n",
      "Epoch 16/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6817 - acc: 0.7504\n",
      "Epoch 17/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6765 - acc: 0.7550\n",
      "Epoch 18/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.6576 - acc: 0.7597\n",
      "Epoch 19/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6531 - acc: 0.7621\n",
      "Epoch 20/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6359 - acc: 0.7643\n",
      "Epoch 21/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6384 - acc: 0.7671\n",
      "Epoch 22/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6408 - acc: 0.7614\n",
      "Epoch 23/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6609 - acc: 0.7562\n",
      "Epoch 24/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6272 - acc: 0.7770\n",
      "Epoch 25/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6464 - acc: 0.7674\n",
      "Epoch 26/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6520 - acc: 0.7641\n",
      "Epoch 27/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6141 - acc: 0.7744\n",
      "Epoch 28/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6016 - acc: 0.7863\n",
      "Epoch 29/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5925 - acc: 0.7828\n",
      "Epoch 30/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5950 - acc: 0.7836\n",
      "Epoch 31/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5948 - acc: 0.7830\n",
      "Epoch 32/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5769 - acc: 0.7876\n",
      "Epoch 33/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5622 - acc: 0.7926\n",
      "Epoch 34/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5463 - acc: 0.8012\n",
      "Epoch 35/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5537 - acc: 0.7966\n",
      "Epoch 36/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5460 - acc: 0.7942\n",
      "Epoch 37/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5372 - acc: 0.7997\n",
      "Epoch 38/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5417 - acc: 0.8032\n",
      "Epoch 39/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5375 - acc: 0.8045\n",
      "Epoch 40/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5269 - acc: 0.8054\n",
      "Epoch 41/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5294 - acc: 0.8059\n",
      "Epoch 42/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5156 - acc: 0.8153\n",
      "Epoch 43/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5023 - acc: 0.8114\n",
      "Epoch 44/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4990 - acc: 0.8175\n",
      "Epoch 45/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4733 - acc: 0.8221\n",
      "Epoch 46/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4836 - acc: 0.8173\n",
      "Epoch 47/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4838 - acc: 0.8217\n",
      "Epoch 48/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4942 - acc: 0.8243\n",
      "Epoch 49/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4940 - acc: 0.8261\n",
      "Epoch 50/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4651 - acc: 0.8303\n",
      "Epoch 51/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.4558 - acc: 0.8379\n",
      "Epoch 52/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4485 - acc: 0.8413\n",
      "Epoch 53/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4504 - acc: 0.8340\n",
      "Epoch 54/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4214 - acc: 0.8457\n",
      "Epoch 55/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4191 - acc: 0.8432\n",
      "Epoch 56/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4215 - acc: 0.8489\n",
      "Epoch 57/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4129 - acc: 0.8501\n",
      "Epoch 58/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3990 - acc: 0.8562\n",
      "Epoch 59/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4127 - acc: 0.8481\n",
      "Epoch 60/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4184 - acc: 0.8492\n",
      "Epoch 61/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4082 - acc: 0.8525\n",
      "Epoch 62/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3906 - acc: 0.8577\n",
      "Epoch 63/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3669 - acc: 0.8630\n",
      "Epoch 64/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3799 - acc: 0.8621\n",
      "Epoch 65/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3706 - acc: 0.8670\n",
      "Epoch 66/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3741 - acc: 0.8613\n",
      "Epoch 67/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3650 - acc: 0.8698\n",
      "Epoch 68/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3444 - acc: 0.8712\n",
      "Epoch 69/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3488 - acc: 0.8711\n",
      "Epoch 70/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3303 - acc: 0.8782\n",
      "Epoch 71/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3380 - acc: 0.8775\n",
      "Epoch 72/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3267 - acc: 0.8775\n",
      "Epoch 73/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3351 - acc: 0.8802\n",
      "Epoch 74/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3155 - acc: 0.8855\n",
      "Epoch 75/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3063 - acc: 0.8879\n",
      "Epoch 76/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2946 - acc: 0.8898\n",
      "Epoch 77/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2985 - acc: 0.8920\n",
      "Epoch 78/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3219 - acc: 0.8824\n",
      "Epoch 79/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2950 - acc: 0.8914\n",
      "Epoch 80/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2905 - acc: 0.8934\n",
      "Epoch 81/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2790 - acc: 0.8989\n",
      "Epoch 82/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2657 - acc: 0.9041\n",
      "Epoch 83/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2669 - acc: 0.9006\n",
      "Epoch 84/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2713 - acc: 0.9048\n",
      "Epoch 85/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2478 - acc: 0.9090\n",
      "Epoch 86/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2557 - acc: 0.9052\n",
      "Epoch 87/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2718 - acc: 0.9004\n",
      "Epoch 88/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2566 - acc: 0.9088\n",
      "Epoch 89/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2543 - acc: 0.9105\n",
      "Epoch 90/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2277 - acc: 0.9131\n",
      "Epoch 91/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2216 - acc: 0.9156\n",
      "Epoch 92/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2343 - acc: 0.9142\n",
      "Epoch 93/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2248 - acc: 0.9213\n",
      "Epoch 94/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2232 - acc: 0.9200\n",
      "Epoch 95/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2124 - acc: 0.9200\n",
      "Epoch 96/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2131 - acc: 0.9253\n",
      "Epoch 97/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.1920 - acc: 0.9321\n",
      "Epoch 98/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.1965 - acc: 0.9331\n",
      "Epoch 99/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2247 - acc: 0.9224\n",
      "Epoch 100/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.1907 - acc: 0.9331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x245709ad0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model\n",
    "model7.fit(x = params7.get_train_padded(),\n",
    "           y = params7.get_train_labels_onehot(),\n",
    "           epochs = 100\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 87.0%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model7.evaluate(params7.get_test_padded(),\n",
    "                params7.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model7.save(\"../models/trec_lstm3.h5\")\n",
    "\n",
    "#also write associated encoder\n",
    "with open(\"../models/encoder3.pickle\", \"wb\") as handle:\n",
    "    pkl.dump(params7.label_encoder, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. Merge, Re-train and Save\n",
    "\n",
    "As I did not use a validation set, I can not make any claim about the usefulness of the models above. However, there are three that are intuitively best - specifically those that combat overfitting with dropout units. \n",
    "\n",
    "I will merge the training and test datasets into one dataset. Then I will fit each of these model configurations again to the augmented dataset, and save the resulting models. These are the models I will apply to the Quora data later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  7.0 Merge the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All the data in one place\n",
    "trec_merged = pd.concat([trec_train, trec_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Train a label encoder on the merged dataset\n",
    "\n",
    "This will be used to convert labels to one-hot encodings, and back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a label encoder\n",
    "encoder = LabelEncoder().fit(trec_merged['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the label encoder\n",
    "with open(\"../models/trec_label_encoder.pickle\", \"wb\") as handle:\n",
    "    pkl.dump(encoder, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Encode questions as padded index vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function which takes in a numpy array of quesitions (strings)\n",
    "# and returns padded index vectors usable by deep learning models. \n",
    "def encode_and_pad(questions, sequence_length = 10):\n",
    "    # questions encoded as index vectors\n",
    "    encoded = tokenizer.texts_to_sequences(questions)\n",
    "    # padded squences to be of length [sequence_length]\n",
    "    padded = pad_sequences(encoded, \n",
    "                            maxlen = sequence_length,\n",
    "                            padding = \"post\", \n",
    "                            truncating = \"post\")\n",
    "    return(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Convert labels to one-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_labels(labels):\n",
    "    return(to_categorical(encoder.transform(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Retrain and save models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1 : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1_merged = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model1_merged.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = 10,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a droput layer\n",
    "model1_merged.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1_merged.add(LSTM(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a droput layer\n",
    "model1_merged.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a Dense layer, and apply the softmax activation on their outputs. \n",
    "model1_merged.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1_merged.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 10, 300)           27978300  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 28,048,806\n",
      "Trainable params: 70,506\n",
      "Non-trainable params: 27,978,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1_merged.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "5952/5952 [==============================] - 6s 974us/step - loss: 1.4978 - acc: 0.3637\n",
      "Epoch 2/125\n",
      "5952/5952 [==============================] - 4s 619us/step - loss: 1.0390 - acc: 0.5921\n",
      "Epoch 3/125\n",
      "5952/5952 [==============================] - 4s 619us/step - loss: 0.8639 - acc: 0.6658\n",
      "Epoch 4/125\n",
      "5952/5952 [==============================] - 4s 618us/step - loss: 0.7914 - acc: 0.6957\n",
      "Epoch 5/125\n",
      "5952/5952 [==============================] - 4s 752us/step - loss: 0.7552 - acc: 0.7176\n",
      "Epoch 6/125\n",
      "5952/5952 [==============================] - 4s 719us/step - loss: 0.7160 - acc: 0.7293\n",
      "Epoch 7/125\n",
      "5952/5952 [==============================] - 4s 672us/step - loss: 0.6980 - acc: 0.7398\n",
      "Epoch 8/125\n",
      "5952/5952 [==============================] - 4s 605us/step - loss: 0.6764 - acc: 0.7458\n",
      "Epoch 9/125\n",
      "5952/5952 [==============================] - 4s 705us/step - loss: 0.6513 - acc: 0.7535\n",
      "Epoch 10/125\n",
      "5952/5952 [==============================] - 4s 614us/step - loss: 0.6470 - acc: 0.7621\n",
      "Epoch 11/125\n",
      "5952/5952 [==============================] - 4s 604us/step - loss: 0.6330 - acc: 0.7698\n",
      "Epoch 12/125\n",
      "5952/5952 [==============================] - 4s 626us/step - loss: 0.6256 - acc: 0.7660\n",
      "Epoch 13/125\n",
      "5952/5952 [==============================] - 4s 637us/step - loss: 0.6063 - acc: 0.7755\n",
      "Epoch 14/125\n",
      "5952/5952 [==============================] - 4s 631us/step - loss: 0.5989 - acc: 0.7784\n",
      "Epoch 15/125\n",
      "5952/5952 [==============================] - 4s 637us/step - loss: 0.6009 - acc: 0.7765\n",
      "Epoch 16/125\n",
      "5952/5952 [==============================] - 4s 647us/step - loss: 0.5826 - acc: 0.7868\n",
      "Epoch 17/125\n",
      "5952/5952 [==============================] - 4s 609us/step - loss: 0.5730 - acc: 0.7851\n",
      "Epoch 18/125\n",
      "5952/5952 [==============================] - 4s 652us/step - loss: 0.5731 - acc: 0.7883\n",
      "Epoch 19/125\n",
      "5952/5952 [==============================] - 4s 642us/step - loss: 0.5507 - acc: 0.7945\n",
      "Epoch 20/125\n",
      "5952/5952 [==============================] - 4s 646us/step - loss: 0.5493 - acc: 0.7981\n",
      "Epoch 21/125\n",
      "5952/5952 [==============================] - 4s 627us/step - loss: 0.5234 - acc: 0.8070\n",
      "Epoch 22/125\n",
      "5952/5952 [==============================] - 4s 632us/step - loss: 0.5452 - acc: 0.7975\n",
      "Epoch 23/125\n",
      "5952/5952 [==============================] - 4s 611us/step - loss: 0.5226 - acc: 0.8036\n",
      "Epoch 24/125\n",
      "5952/5952 [==============================] - 4s 591us/step - loss: 0.5007 - acc: 0.8150\n",
      "Epoch 25/125\n",
      "5952/5952 [==============================] - 3s 571us/step - loss: 0.4911 - acc: 0.8160\n",
      "Epoch 26/125\n",
      "5952/5952 [==============================] - 4s 619us/step - loss: 0.4969 - acc: 0.8135\n",
      "Epoch 27/125\n",
      "5952/5952 [==============================] - 4s 642us/step - loss: 0.4834 - acc: 0.8189\n",
      "Epoch 28/125\n",
      "5952/5952 [==============================] - 4s 628us/step - loss: 0.4822 - acc: 0.8238\n",
      "Epoch 29/125\n",
      "5952/5952 [==============================] - 4s 646us/step - loss: 0.4681 - acc: 0.8239\n",
      "Epoch 30/125\n",
      "5952/5952 [==============================] - 4s 616us/step - loss: 0.4563 - acc: 0.8288\n",
      "Epoch 31/125\n",
      "5952/5952 [==============================] - 4s 613us/step - loss: 0.4589 - acc: 0.8269\n",
      "Epoch 32/125\n",
      "5952/5952 [==============================] - 4s 623us/step - loss: 0.4417 - acc: 0.8404\n",
      "Epoch 33/125\n",
      "5952/5952 [==============================] - 4s 628us/step - loss: 0.4409 - acc: 0.8379\n",
      "Epoch 34/125\n",
      "5952/5952 [==============================] - 4s 600us/step - loss: 0.4260 - acc: 0.8411\n",
      "Epoch 35/125\n",
      "5952/5952 [==============================] - 4s 625us/step - loss: 0.4212 - acc: 0.8449\n",
      "Epoch 36/125\n",
      "5952/5952 [==============================] - 4s 643us/step - loss: 0.4122 - acc: 0.8466\n",
      "Epoch 37/125\n",
      "5952/5952 [==============================] - 3s 587us/step - loss: 0.4048 - acc: 0.8469\n",
      "Epoch 38/125\n",
      "5952/5952 [==============================] - 4s 634us/step - loss: 0.3993 - acc: 0.8530\n",
      "Epoch 39/125\n",
      "5952/5952 [==============================] - 4s 626us/step - loss: 0.3996 - acc: 0.8495\n",
      "Epoch 40/125\n",
      "5952/5952 [==============================] - 4s 631us/step - loss: 0.3817 - acc: 0.8616\n",
      "Epoch 41/125\n",
      "5952/5952 [==============================] - 4s 617us/step - loss: 0.3835 - acc: 0.8543\n",
      "Epoch 42/125\n",
      "5952/5952 [==============================] - 4s 629us/step - loss: 0.3732 - acc: 0.8617\n",
      "Epoch 43/125\n",
      "5952/5952 [==============================] - 4s 605us/step - loss: 0.3653 - acc: 0.8627\n",
      "Epoch 44/125\n",
      "5952/5952 [==============================] - 4s 598us/step - loss: 0.3573 - acc: 0.8664\n",
      "Epoch 45/125\n",
      "5952/5952 [==============================] - 4s 624us/step - loss: 0.3500 - acc: 0.8678\n",
      "Epoch 46/125\n",
      "5952/5952 [==============================] - 4s 606us/step - loss: 0.3498 - acc: 0.8691\n",
      "Epoch 47/125\n",
      "5952/5952 [==============================] - 4s 611us/step - loss: 0.3434 - acc: 0.8732\n",
      "Epoch 48/125\n",
      "5952/5952 [==============================] - 4s 638us/step - loss: 0.3322 - acc: 0.8774\n",
      "Epoch 49/125\n",
      "5952/5952 [==============================] - 4s 628us/step - loss: 0.3266 - acc: 0.8807\n",
      "Epoch 50/125\n",
      "5952/5952 [==============================] - 4s 617us/step - loss: 0.3276 - acc: 0.8812\n",
      "Epoch 51/125\n",
      "5952/5952 [==============================] - 4s 611us/step - loss: 0.3189 - acc: 0.8816\n",
      "Epoch 52/125\n",
      "5952/5952 [==============================] - 4s 622us/step - loss: 0.3088 - acc: 0.8844\n",
      "Epoch 53/125\n",
      "5952/5952 [==============================] - 4s 611us/step - loss: 0.3050 - acc: 0.8896\n",
      "Epoch 54/125\n",
      "5952/5952 [==============================] - 4s 643us/step - loss: 0.2894 - acc: 0.8901\n",
      "Epoch 55/125\n",
      "5952/5952 [==============================] - 4s 612us/step - loss: 0.2906 - acc: 0.8950\n",
      "Epoch 56/125\n",
      "5952/5952 [==============================] - 3s 587us/step - loss: 0.2821 - acc: 0.8940\n",
      "Epoch 57/125\n",
      "5952/5952 [==============================] - 4s 632us/step - loss: 0.2764 - acc: 0.8985\n",
      "Epoch 58/125\n",
      "5952/5952 [==============================] - 4s 603us/step - loss: 0.2673 - acc: 0.8973\n",
      "Epoch 59/125\n",
      "5952/5952 [==============================] - 4s 614us/step - loss: 0.2666 - acc: 0.9014\n",
      "Epoch 60/125\n",
      "5952/5952 [==============================] - 4s 621us/step - loss: 0.2588 - acc: 0.9062\n",
      "Epoch 61/125\n",
      "5952/5952 [==============================] - 4s 602us/step - loss: 0.2462 - acc: 0.9094\n",
      "Epoch 62/125\n",
      "5952/5952 [==============================] - 4s 609us/step - loss: 0.2516 - acc: 0.9034\n",
      "Epoch 63/125\n",
      "5952/5952 [==============================] - 3s 578us/step - loss: 0.2371 - acc: 0.9103\n",
      "Epoch 64/125\n",
      "5952/5952 [==============================] - 4s 628us/step - loss: 0.2285 - acc: 0.9189\n",
      "Epoch 65/125\n",
      "5952/5952 [==============================] - 4s 636us/step - loss: 0.2346 - acc: 0.9125\n",
      "Epoch 66/125\n",
      "5952/5952 [==============================] - 4s 615us/step - loss: 0.2259 - acc: 0.9170\n",
      "Epoch 67/125\n",
      "5952/5952 [==============================] - 4s 612us/step - loss: 0.2346 - acc: 0.9157\n",
      "Epoch 68/125\n",
      "5952/5952 [==============================] - 4s 644us/step - loss: 0.2166 - acc: 0.9180\n",
      "Epoch 69/125\n",
      "5952/5952 [==============================] - 4s 635us/step - loss: 0.2065 - acc: 0.9247\n",
      "Epoch 70/125\n",
      "5952/5952 [==============================] - 4s 625us/step - loss: 0.2172 - acc: 0.9212\n",
      "Epoch 71/125\n",
      "5952/5952 [==============================] - 4s 608us/step - loss: 0.2104 - acc: 0.9197\n",
      "Epoch 72/125\n",
      "5952/5952 [==============================] - 3s 567us/step - loss: 0.2099 - acc: 0.9210\n",
      "Epoch 73/125\n",
      "5952/5952 [==============================] - 4s 628us/step - loss: 0.1997 - acc: 0.9267\n",
      "Epoch 74/125\n",
      "5952/5952 [==============================] - 4s 614us/step - loss: 0.1978 - acc: 0.9259\n",
      "Epoch 75/125\n",
      "5952/5952 [==============================] - 4s 594us/step - loss: 0.1955 - acc: 0.9266\n",
      "Epoch 76/125\n",
      "5952/5952 [==============================] - 4s 609us/step - loss: 0.1793 - acc: 0.9318\n",
      "Epoch 77/125\n",
      "5952/5952 [==============================] - 4s 669us/step - loss: 0.1767 - acc: 0.9377\n",
      "Epoch 78/125\n",
      "5952/5952 [==============================] - 4s 665us/step - loss: 0.1833 - acc: 0.9382\n",
      "Epoch 79/125\n",
      "5952/5952 [==============================] - 4s 603us/step - loss: 0.1743 - acc: 0.9399\n",
      "Epoch 80/125\n",
      "5952/5952 [==============================] - 4s 614us/step - loss: 0.1751 - acc: 0.9355\n",
      "Epoch 81/125\n",
      "5952/5952 [==============================] - 4s 620us/step - loss: 0.1698 - acc: 0.9378\n",
      "Epoch 82/125\n",
      "5952/5952 [==============================] - 4s 648us/step - loss: 0.1589 - acc: 0.9385\n",
      "Epoch 83/125\n",
      "5952/5952 [==============================] - 5s 786us/step - loss: 0.1536 - acc: 0.9456\n",
      "Epoch 84/125\n",
      "5952/5952 [==============================] - 3s 574us/step - loss: 0.1533 - acc: 0.9432\n",
      "Epoch 85/125\n",
      "5952/5952 [==============================] - 4s 638us/step - loss: 0.1603 - acc: 0.9447\n",
      "Epoch 86/125\n",
      "5952/5952 [==============================] - 4s 732us/step - loss: 0.1486 - acc: 0.9476\n",
      "Epoch 87/125\n",
      "5952/5952 [==============================] - 4s 654us/step - loss: 0.1485 - acc: 0.9484\n",
      "Epoch 88/125\n",
      "5952/5952 [==============================] - 3s 556us/step - loss: 0.1446 - acc: 0.9461\n",
      "Epoch 89/125\n",
      "5952/5952 [==============================] - 3s 554us/step - loss: 0.1529 - acc: 0.9412\n",
      "Epoch 90/125\n",
      "5952/5952 [==============================] - 3s 576us/step - loss: 0.1395 - acc: 0.9513\n",
      "Epoch 91/125\n",
      "5952/5952 [==============================] - 4s 623us/step - loss: 0.1396 - acc: 0.9486\n",
      "Epoch 92/125\n",
      "5952/5952 [==============================] - 3s 561us/step - loss: 0.1318 - acc: 0.9523\n",
      "Epoch 93/125\n",
      "5952/5952 [==============================] - 3s 569us/step - loss: 0.1386 - acc: 0.9535\n",
      "Epoch 94/125\n",
      "5952/5952 [==============================] - 3s 564us/step - loss: 0.1283 - acc: 0.9543\n",
      "Epoch 95/125\n",
      "5952/5952 [==============================] - 4s 611us/step - loss: 0.1353 - acc: 0.9508\n",
      "Epoch 96/125\n",
      "5952/5952 [==============================] - 3s 578us/step - loss: 0.1195 - acc: 0.9558\n",
      "Epoch 97/125\n",
      "5952/5952 [==============================] - 3s 544us/step - loss: 0.1320 - acc: 0.9536\n",
      "Epoch 98/125\n",
      "5952/5952 [==============================] - 4s 640us/step - loss: 0.1089 - acc: 0.9627\n",
      "Epoch 99/125\n",
      "5952/5952 [==============================] - 3s 553us/step - loss: 0.1211 - acc: 0.9573\n",
      "Epoch 100/125\n",
      "5952/5952 [==============================] - 4s 679us/step - loss: 0.1157 - acc: 0.9612\n",
      "Epoch 101/125\n",
      "5952/5952 [==============================] - 4s 748us/step - loss: 0.1110 - acc: 0.9603\n",
      "Epoch 102/125\n",
      "5952/5952 [==============================] - 4s 635us/step - loss: 0.1093 - acc: 0.9615\n",
      "Epoch 103/125\n",
      "5952/5952 [==============================] - 4s 664us/step - loss: 0.1133 - acc: 0.9617\n",
      "Epoch 104/125\n",
      "5952/5952 [==============================] - 3s 582us/step - loss: 0.1117 - acc: 0.9612\n",
      "Epoch 105/125\n",
      "5952/5952 [==============================] - 4s 715us/step - loss: 0.0943 - acc: 0.9688\n",
      "Epoch 106/125\n",
      "5952/5952 [==============================] - 4s 639us/step - loss: 0.1059 - acc: 0.9652\n",
      "Epoch 107/125\n",
      "5952/5952 [==============================] - 4s 613us/step - loss: 0.1041 - acc: 0.9635\n",
      "Epoch 108/125\n",
      "5952/5952 [==============================] - 4s 665us/step - loss: 0.0941 - acc: 0.9677\n",
      "Epoch 109/125\n",
      "5952/5952 [==============================] - 3s 545us/step - loss: 0.0948 - acc: 0.9667\n",
      "Epoch 110/125\n",
      "5952/5952 [==============================] - 3s 556us/step - loss: 0.0973 - acc: 0.9661\n",
      "Epoch 111/125\n",
      "5952/5952 [==============================] - 3s 565us/step - loss: 0.0868 - acc: 0.9699\n",
      "Epoch 112/125\n",
      "5952/5952 [==============================] - 4s 615us/step - loss: 0.0998 - acc: 0.9639\n",
      "Epoch 113/125\n",
      "5952/5952 [==============================] - 3s 582us/step - loss: 0.1039 - acc: 0.9654\n",
      "Epoch 114/125\n",
      "5952/5952 [==============================] - 4s 627us/step - loss: 0.0960 - acc: 0.9667\n",
      "Epoch 115/125\n",
      "5952/5952 [==============================] - 4s 641us/step - loss: 0.0927 - acc: 0.9667\n",
      "Epoch 116/125\n",
      "5952/5952 [==============================] - 4s 596us/step - loss: 0.0925 - acc: 0.9666\n",
      "Epoch 117/125\n",
      "5952/5952 [==============================] - 4s 590us/step - loss: 0.0780 - acc: 0.9735\n",
      "Epoch 118/125\n",
      "5952/5952 [==============================] - 4s 605us/step - loss: 0.0845 - acc: 0.9714\n",
      "Epoch 119/125\n",
      "5952/5952 [==============================] - 3s 549us/step - loss: 0.0799 - acc: 0.9731\n",
      "Epoch 120/125\n",
      "5952/5952 [==============================] - 3s 548us/step - loss: 0.0868 - acc: 0.9693\n",
      "Epoch 121/125\n",
      "5952/5952 [==============================] - 4s 601us/step - loss: 0.0784 - acc: 0.9728\n",
      "Epoch 122/125\n",
      "5952/5952 [==============================] - 3s 516us/step - loss: 0.0688 - acc: 0.9761\n",
      "Epoch 123/125\n",
      "5952/5952 [==============================] - 4s 593us/step - loss: 0.0780 - acc: 0.9711\n",
      "Epoch 124/125\n",
      "5952/5952 [==============================] - 4s 608us/step - loss: 0.0789 - acc: 0.9726\n",
      "Epoch 125/125\n",
      "5952/5952 [==============================] - 4s 629us/step - loss: 0.0694 - acc: 0.9777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d8a2d90>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model1_merged.fit(x = encode_and_pad(trec_merged['question'].values),\n",
    "           y = one_hot_labels(trec_merged['label'].values),\n",
    "           epochs = 125\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model1_merged.save(\"../models/trec_lstm1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2_merged = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model2_merged.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = 10,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model2_merged.add(LSTM(50,dropout=0.2, recurrent_dropout=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a Dense layer, and apply the softmax activation on their outputs. \n",
    "model2_merged.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2_merged.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 10, 300)           27978300  \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 28,048,806\n",
      "Trainable params: 70,506\n",
      "Non-trainable params: 27,978,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2_merged.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "5952/5952 [==============================] - 6s 1ms/step - loss: 1.5400 - acc: 0.3359\n",
      "Epoch 2/150\n",
      "5952/5952 [==============================] - 4s 678us/step - loss: 1.0758 - acc: 0.5719\n",
      "Epoch 3/150\n",
      "5952/5952 [==============================] - 4s 627us/step - loss: 0.9303 - acc: 0.6433\n",
      "Epoch 4/150\n",
      "5952/5952 [==============================] - 4s 631us/step - loss: 0.8575 - acc: 0.6704\n",
      "Epoch 5/150\n",
      "5952/5952 [==============================] - 4s 669us/step - loss: 0.8033 - acc: 0.6930\n",
      "Epoch 6/150\n",
      "5952/5952 [==============================] - 4s 643us/step - loss: 0.7662 - acc: 0.7130\n",
      "Epoch 7/150\n",
      "5952/5952 [==============================] - 4s 636us/step - loss: 0.7430 - acc: 0.7241\n",
      "Epoch 8/150\n",
      "5952/5952 [==============================] - 4s 630us/step - loss: 0.7196 - acc: 0.7292\n",
      "Epoch 9/150\n",
      "5952/5952 [==============================] - 4s 643us/step - loss: 0.7121 - acc: 0.7419\n",
      "Epoch 10/150\n",
      "5952/5952 [==============================] - 4s 645us/step - loss: 0.6802 - acc: 0.7431\n",
      "Epoch 11/150\n",
      "5952/5952 [==============================] - 4s 647us/step - loss: 0.6709 - acc: 0.7500\n",
      "Epoch 12/150\n",
      "5952/5952 [==============================] - 4s 641us/step - loss: 0.6583 - acc: 0.7525\n",
      "Epoch 13/150\n",
      "5952/5952 [==============================] - 4s 635us/step - loss: 0.6537 - acc: 0.7633\n",
      "Epoch 14/150\n",
      "5952/5952 [==============================] - 4s 660us/step - loss: 0.6365 - acc: 0.7660\n",
      "Epoch 15/150\n",
      "5952/5952 [==============================] - 4s 645us/step - loss: 0.6287 - acc: 0.7688\n",
      "Epoch 16/150\n",
      "5952/5952 [==============================] - 4s 654us/step - loss: 0.6209 - acc: 0.7708\n",
      "Epoch 17/150\n",
      "5952/5952 [==============================] - 4s 650us/step - loss: 0.6163 - acc: 0.7718\n",
      "Epoch 18/150\n",
      "5952/5952 [==============================] - 4s 703us/step - loss: 0.6016 - acc: 0.7801\n",
      "Epoch 19/150\n",
      "5952/5952 [==============================] - 4s 686us/step - loss: 0.5895 - acc: 0.7814\n",
      "Epoch 20/150\n",
      "5952/5952 [==============================] - 4s 662us/step - loss: 0.5813 - acc: 0.7828\n",
      "Epoch 21/150\n",
      "5952/5952 [==============================] - 4s 702us/step - loss: 0.5785 - acc: 0.7823\n",
      "Epoch 22/150\n",
      "5952/5952 [==============================] - 4s 745us/step - loss: 0.5666 - acc: 0.7891\n",
      "Epoch 23/150\n",
      "5952/5952 [==============================] - 4s 756us/step - loss: 0.5625 - acc: 0.7900\n",
      "Epoch 24/150\n",
      "5952/5952 [==============================] - 4s 607us/step - loss: 0.5517 - acc: 0.7952\n",
      "Epoch 25/150\n",
      "5952/5952 [==============================] - 4s 643us/step - loss: 0.5416 - acc: 0.7975\n",
      "Epoch 26/150\n",
      "5952/5952 [==============================] - 4s 653us/step - loss: 0.5413 - acc: 0.7965\n",
      "Epoch 27/150\n",
      "5952/5952 [==============================] - 4s 615us/step - loss: 0.5289 - acc: 0.8024\n",
      "Epoch 28/150\n",
      "5952/5952 [==============================] - 4s 600us/step - loss: 0.5250 - acc: 0.8105\n",
      "Epoch 29/150\n",
      "5952/5952 [==============================] - 4s 606us/step - loss: 0.5170 - acc: 0.8083\n",
      "Epoch 30/150\n",
      "5952/5952 [==============================] - 3s 576us/step - loss: 0.4984 - acc: 0.8112\n",
      "Epoch 31/150\n",
      "5952/5952 [==============================] - 4s 594us/step - loss: 0.4947 - acc: 0.8202\n",
      "Epoch 32/150\n",
      "5952/5952 [==============================] - 4s 616us/step - loss: 0.4869 - acc: 0.8187\n",
      "Epoch 33/150\n",
      "5952/5952 [==============================] - 4s 595us/step - loss: 0.4762 - acc: 0.8249\n",
      "Epoch 34/150\n",
      "5952/5952 [==============================] - 4s 606us/step - loss: 0.4748 - acc: 0.8217\n",
      "Epoch 35/150\n",
      "5952/5952 [==============================] - 4s 611us/step - loss: 0.4618 - acc: 0.8256\n",
      "Epoch 36/150\n",
      "5952/5952 [==============================] - 4s 624us/step - loss: 0.4414 - acc: 0.8377\n",
      "Epoch 37/150\n",
      "5952/5952 [==============================] - 4s 609us/step - loss: 0.4436 - acc: 0.8318\n",
      "Epoch 38/150\n",
      "5952/5952 [==============================] - 4s 624us/step - loss: 0.4429 - acc: 0.8359\n",
      "Epoch 39/150\n",
      "5952/5952 [==============================] - 4s 601us/step - loss: 0.4388 - acc: 0.8394\n",
      "Epoch 40/150\n",
      "5952/5952 [==============================] - 4s 593us/step - loss: 0.4194 - acc: 0.8456\n",
      "Epoch 41/150\n",
      "5952/5952 [==============================] - 4s 613us/step - loss: 0.4156 - acc: 0.8444\n",
      "Epoch 42/150\n",
      "5952/5952 [==============================] - 4s 628us/step - loss: 0.4050 - acc: 0.8523\n",
      "Epoch 43/150\n",
      "5952/5952 [==============================] - 4s 613us/step - loss: 0.3982 - acc: 0.8537\n",
      "Epoch 44/150\n",
      "5952/5952 [==============================] - 4s 609us/step - loss: 0.3984 - acc: 0.8533\n",
      "Epoch 45/150\n",
      "5952/5952 [==============================] - 4s 596us/step - loss: 0.3881 - acc: 0.8595\n",
      "Epoch 46/150\n",
      "5952/5952 [==============================] - 4s 614us/step - loss: 0.3854 - acc: 0.8587\n",
      "Epoch 47/150\n",
      "5952/5952 [==============================] - 3s 585us/step - loss: 0.3711 - acc: 0.8616\n",
      "Epoch 48/150\n",
      "5952/5952 [==============================] - 3s 586us/step - loss: 0.3651 - acc: 0.8641\n",
      "Epoch 49/150\n",
      "5952/5952 [==============================] - 4s 606us/step - loss: 0.3673 - acc: 0.8678\n",
      "Epoch 50/150\n",
      "5952/5952 [==============================] - 4s 614us/step - loss: 0.3497 - acc: 0.8738\n",
      "Epoch 51/150\n",
      "5952/5952 [==============================] - 4s 596us/step - loss: 0.3486 - acc: 0.8718\n",
      "Epoch 52/150\n",
      "5952/5952 [==============================] - 4s 631us/step - loss: 0.3453 - acc: 0.8700\n",
      "Epoch 53/150\n",
      "5952/5952 [==============================] - 4s 595us/step - loss: 0.3446 - acc: 0.8720\n",
      "Epoch 54/150\n",
      "5952/5952 [==============================] - 4s 607us/step - loss: 0.3381 - acc: 0.8774\n",
      "Epoch 55/150\n",
      "5952/5952 [==============================] - 4s 607us/step - loss: 0.3279 - acc: 0.8753\n",
      "Epoch 56/150\n",
      "5952/5952 [==============================] - 4s 601us/step - loss: 0.3119 - acc: 0.8901\n",
      "Epoch 57/150\n",
      "5952/5952 [==============================] - 4s 629us/step - loss: 0.3150 - acc: 0.8866\n",
      "Epoch 58/150\n",
      "5952/5952 [==============================] - 4s 596us/step - loss: 0.3157 - acc: 0.8837\n",
      "Epoch 59/150\n",
      "5952/5952 [==============================] - 4s 609us/step - loss: 0.3011 - acc: 0.8883\n",
      "Epoch 60/150\n",
      "5952/5952 [==============================] - 4s 589us/step - loss: 0.2982 - acc: 0.8920\n",
      "Epoch 61/150\n",
      "5952/5952 [==============================] - 4s 589us/step - loss: 0.2976 - acc: 0.8925\n",
      "Epoch 62/150\n",
      "5952/5952 [==============================] - 5s 896us/step - loss: 0.2999 - acc: 0.8925\n",
      "Epoch 63/150\n",
      "5952/5952 [==============================] - 4s 732us/step - loss: 0.2911 - acc: 0.8928\n",
      "Epoch 64/150\n",
      "5952/5952 [==============================] - 4s 653us/step - loss: 0.2750 - acc: 0.9020\n",
      "Epoch 65/150\n",
      "5952/5952 [==============================] - 4s 611us/step - loss: 0.2879 - acc: 0.8925\n",
      "Epoch 66/150\n",
      "5952/5952 [==============================] - 4s 602us/step - loss: 0.2723 - acc: 0.8997\n",
      "Epoch 67/150\n",
      "5952/5952 [==============================] - 3s 584us/step - loss: 0.2673 - acc: 0.9047\n",
      "Epoch 68/150\n",
      "5952/5952 [==============================] - 4s 604us/step - loss: 0.2623 - acc: 0.9062\n",
      "Epoch 69/150\n",
      "5952/5952 [==============================] - 4s 610us/step - loss: 0.2655 - acc: 0.9068\n",
      "Epoch 70/150\n",
      "5952/5952 [==============================] - 4s 615us/step - loss: 0.2442 - acc: 0.9123\n",
      "Epoch 71/150\n",
      "5952/5952 [==============================] - 4s 609us/step - loss: 0.2525 - acc: 0.9079\n",
      "Epoch 72/150\n",
      "5952/5952 [==============================] - 4s 604us/step - loss: 0.2509 - acc: 0.9091\n",
      "Epoch 73/150\n",
      "5952/5952 [==============================] - 4s 633us/step - loss: 0.2438 - acc: 0.9106\n",
      "Epoch 74/150\n",
      "5952/5952 [==============================] - 4s 614us/step - loss: 0.2434 - acc: 0.9113\n",
      "Epoch 75/150\n",
      "5952/5952 [==============================] - 4s 606us/step - loss: 0.2381 - acc: 0.9167\n",
      "Epoch 76/150\n",
      "5952/5952 [==============================] - 4s 616us/step - loss: 0.2422 - acc: 0.9136\n",
      "Epoch 77/150\n",
      "5952/5952 [==============================] - 4s 606us/step - loss: 0.2360 - acc: 0.9130\n",
      "Epoch 78/150\n",
      "5952/5952 [==============================] - 4s 621us/step - loss: 0.2228 - acc: 0.9189\n",
      "Epoch 79/150\n",
      "5952/5952 [==============================] - 4s 598us/step - loss: 0.2137 - acc: 0.9224\n",
      "Epoch 80/150\n",
      "5952/5952 [==============================] - 4s 605us/step - loss: 0.2156 - acc: 0.9227\n",
      "Epoch 81/150\n",
      "5952/5952 [==============================] - 4s 624us/step - loss: 0.2187 - acc: 0.9204\n",
      "Epoch 82/150\n",
      "5952/5952 [==============================] - 4s 599us/step - loss: 0.2134 - acc: 0.9274\n",
      "Epoch 83/150\n",
      "5952/5952 [==============================] - 4s 609us/step - loss: 0.2066 - acc: 0.9274\n",
      "Epoch 84/150\n",
      "5952/5952 [==============================] - 4s 594us/step - loss: 0.2100 - acc: 0.9239\n",
      "Epoch 85/150\n",
      "5952/5952 [==============================] - 3s 585us/step - loss: 0.1982 - acc: 0.9299\n",
      "Epoch 86/150\n",
      "5952/5952 [==============================] - 4s 613us/step - loss: 0.2000 - acc: 0.9269\n",
      "Epoch 87/150\n",
      "5952/5952 [==============================] - 4s 602us/step - loss: 0.1877 - acc: 0.9321\n",
      "Epoch 88/150\n",
      "5952/5952 [==============================] - 4s 601us/step - loss: 0.1984 - acc: 0.9262\n",
      "Epoch 89/150\n",
      "5952/5952 [==============================] - 4s 612us/step - loss: 0.1888 - acc: 0.9294\n",
      "Epoch 90/150\n",
      "5952/5952 [==============================] - 4s 619us/step - loss: 0.1844 - acc: 0.9313\n",
      "Epoch 91/150\n",
      "5952/5952 [==============================] - 4s 618us/step - loss: 0.1782 - acc: 0.9370\n",
      "Epoch 92/150\n",
      "5952/5952 [==============================] - 4s 620us/step - loss: 0.1770 - acc: 0.9392\n",
      "Epoch 93/150\n",
      "5952/5952 [==============================] - 4s 603us/step - loss: 0.1665 - acc: 0.9419\n",
      "Epoch 94/150\n",
      "5952/5952 [==============================] - 4s 614us/step - loss: 0.1722 - acc: 0.9372\n",
      "Epoch 95/150\n",
      "5952/5952 [==============================] - 4s 620us/step - loss: 0.1705 - acc: 0.9409\n",
      "Epoch 96/150\n",
      "5952/5952 [==============================] - 4s 622us/step - loss: 0.1663 - acc: 0.9412\n",
      "Epoch 97/150\n",
      "5952/5952 [==============================] - 4s 605us/step - loss: 0.1655 - acc: 0.9409\n",
      "Epoch 98/150\n",
      "5952/5952 [==============================] - 4s 614us/step - loss: 0.1637 - acc: 0.9430\n",
      "Epoch 99/150\n",
      "5952/5952 [==============================] - 4s 603us/step - loss: 0.1575 - acc: 0.9444\n",
      "Epoch 100/150\n",
      "5952/5952 [==============================] - 4s 610us/step - loss: 0.1624 - acc: 0.9427\n",
      "Epoch 101/150\n",
      "5952/5952 [==============================] - 4s 604us/step - loss: 0.1604 - acc: 0.9417\n",
      "Epoch 102/150\n",
      "5952/5952 [==============================] - 4s 622us/step - loss: 0.1497 - acc: 0.9479\n",
      "Epoch 103/150\n",
      "5952/5952 [==============================] - 4s 632us/step - loss: 0.1540 - acc: 0.9462\n",
      "Epoch 104/150\n",
      "5952/5952 [==============================] - 4s 609us/step - loss: 0.1475 - acc: 0.9489\n",
      "Epoch 105/150\n",
      "5952/5952 [==============================] - 4s 609us/step - loss: 0.1455 - acc: 0.9508\n",
      "Epoch 106/150\n",
      "5952/5952 [==============================] - 4s 652us/step - loss: 0.1351 - acc: 0.9528\n",
      "Epoch 107/150\n",
      "5952/5952 [==============================] - 4s 634us/step - loss: 0.1422 - acc: 0.9525\n",
      "Epoch 108/150\n",
      "5952/5952 [==============================] - 4s 617us/step - loss: 0.1392 - acc: 0.9546\n",
      "Epoch 109/150\n",
      "5952/5952 [==============================] - 4s 615us/step - loss: 0.1317 - acc: 0.9550\n",
      "Epoch 110/150\n",
      "5952/5952 [==============================] - 3s 585us/step - loss: 0.1404 - acc: 0.9496\n",
      "Epoch 111/150\n",
      "5952/5952 [==============================] - 4s 622us/step - loss: 0.1309 - acc: 0.9560\n",
      "Epoch 112/150\n",
      "5952/5952 [==============================] - 4s 611us/step - loss: 0.1216 - acc: 0.9563\n",
      "Epoch 113/150\n",
      "5952/5952 [==============================] - 4s 606us/step - loss: 0.1268 - acc: 0.9556\n",
      "Epoch 114/150\n",
      "5952/5952 [==============================] - 4s 607us/step - loss: 0.1267 - acc: 0.9530\n",
      "Epoch 115/150\n",
      "5952/5952 [==============================] - 4s 593us/step - loss: 0.1166 - acc: 0.9597\n",
      "Epoch 116/150\n",
      "5952/5952 [==============================] - 4s 621us/step - loss: 0.1194 - acc: 0.9619\n",
      "Epoch 117/150\n",
      "5952/5952 [==============================] - 4s 621us/step - loss: 0.1214 - acc: 0.9597\n",
      "Epoch 118/150\n",
      "5952/5952 [==============================] - 4s 594us/step - loss: 0.1197 - acc: 0.9577\n",
      "Epoch 119/150\n",
      "5952/5952 [==============================] - 4s 620us/step - loss: 0.1139 - acc: 0.9605\n",
      "Epoch 120/150\n",
      "5952/5952 [==============================] - 4s 598us/step - loss: 0.1160 - acc: 0.9590\n",
      "Epoch 121/150\n",
      "5952/5952 [==============================] - 4s 627us/step - loss: 0.1088 - acc: 0.9612\n",
      "Epoch 122/150\n",
      "5952/5952 [==============================] - 4s 638us/step - loss: 0.1128 - acc: 0.9602\n",
      "Epoch 123/150\n",
      "5952/5952 [==============================] - 4s 638us/step - loss: 0.1171 - acc: 0.9582\n",
      "Epoch 124/150\n",
      "5952/5952 [==============================] - 5s 784us/step - loss: 0.1041 - acc: 0.9647\n",
      "Epoch 125/150\n",
      "5952/5952 [==============================] - 4s 696us/step - loss: 0.0991 - acc: 0.9624\n",
      "Epoch 126/150\n",
      "5952/5952 [==============================] - 4s 739us/step - loss: 0.1062 - acc: 0.9617\n",
      "Epoch 127/150\n",
      "5952/5952 [==============================] - 4s 609us/step - loss: 0.1083 - acc: 0.9634\n",
      "Epoch 128/150\n",
      "5952/5952 [==============================] - 4s 674us/step - loss: 0.1022 - acc: 0.9681\n",
      "Epoch 129/150\n",
      "5952/5952 [==============================] - 4s 711us/step - loss: 0.1019 - acc: 0.9654\n",
      "Epoch 130/150\n",
      "5952/5952 [==============================] - 4s 634us/step - loss: 0.1040 - acc: 0.9639\n",
      "Epoch 131/150\n",
      "5952/5952 [==============================] - 4s 687us/step - loss: 0.0936 - acc: 0.9682\n",
      "Epoch 132/150\n",
      "5952/5952 [==============================] - 4s 654us/step - loss: 0.0965 - acc: 0.9698\n",
      "Epoch 133/150\n",
      "5952/5952 [==============================] - 4s 619us/step - loss: 0.0959 - acc: 0.9664\n",
      "Epoch 134/150\n",
      "5952/5952 [==============================] - 4s 599us/step - loss: 0.0934 - acc: 0.9666\n",
      "Epoch 135/150\n",
      "5952/5952 [==============================] - 4s 604us/step - loss: 0.0874 - acc: 0.9713\n",
      "Epoch 136/150\n",
      "5952/5952 [==============================] - 4s 591us/step - loss: 0.0900 - acc: 0.9699\n",
      "Epoch 137/150\n",
      "5952/5952 [==============================] - 3s 579us/step - loss: 0.0989 - acc: 0.9647\n",
      "Epoch 138/150\n",
      "5952/5952 [==============================] - 4s 595us/step - loss: 0.0893 - acc: 0.9674\n",
      "Epoch 139/150\n",
      "5952/5952 [==============================] - 3s 585us/step - loss: 0.0903 - acc: 0.9706\n",
      "Epoch 140/150\n",
      "5952/5952 [==============================] - 4s 597us/step - loss: 0.0911 - acc: 0.9694\n",
      "Epoch 141/150\n",
      "5952/5952 [==============================] - 3s 584us/step - loss: 0.0914 - acc: 0.9671\n",
      "Epoch 142/150\n",
      "5952/5952 [==============================] - 4s 622us/step - loss: 0.0807 - acc: 0.9716\n",
      "Epoch 143/150\n",
      "5952/5952 [==============================] - 4s 623us/step - loss: 0.0865 - acc: 0.9708\n",
      "Epoch 144/150\n",
      "5952/5952 [==============================] - 4s 639us/step - loss: 0.0829 - acc: 0.9726\n",
      "Epoch 145/150\n",
      "5952/5952 [==============================] - 4s 612us/step - loss: 0.0825 - acc: 0.9709\n",
      "Epoch 146/150\n",
      "5952/5952 [==============================] - 4s 632us/step - loss: 0.0743 - acc: 0.9726\n",
      "Epoch 147/150\n",
      "5952/5952 [==============================] - 4s 639us/step - loss: 0.0764 - acc: 0.9750\n",
      "Epoch 148/150\n",
      "5952/5952 [==============================] - 4s 595us/step - loss: 0.0758 - acc: 0.9755\n",
      "Epoch 149/150\n",
      "5952/5952 [==============================] - 4s 611us/step - loss: 0.0774 - acc: 0.9736\n",
      "Epoch 150/150\n",
      "5952/5952 [==============================] - 4s 594us/step - loss: 0.0761 - acc: 0.9745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x263a5c9d0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model2_merged.fit(x = encode_and_pad(trec_merged['question'].values),\n",
    "           y = one_hot_labels(trec_merged['label'].values),\n",
    "           epochs = 150\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2_merged.save(\"../models/trec_lstm2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3_merged = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model3_merged.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = 10,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model3_merged.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3_merged.add(LSTM(6, dropout=0.2, recurrent_dropout=0.2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3_merged.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 10, 300)           27978300  \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 10, 100)           160400    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 6)                 2568      \n",
      "=================================================================\n",
      "Total params: 28,141,268\n",
      "Trainable params: 162,968\n",
      "Non-trainable params: 27,978,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3_merged.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "5952/5952 [==============================] - 14s 2ms/step - loss: 1.6497 - acc: 0.3624\n",
      "Epoch 2/125\n",
      "5952/5952 [==============================] - 12s 2ms/step - loss: 1.1105 - acc: 0.5822\n",
      "Epoch 3/125\n",
      "5952/5952 [==============================] - 15s 3ms/step - loss: 0.9058 - acc: 0.6569\n",
      "Epoch 4/125\n",
      "5952/5952 [==============================] - 11s 2ms/step - loss: 0.8606 - acc: 0.6905\n",
      "Epoch 5/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.8237 - acc: 0.7068\n",
      "Epoch 6/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.7666 - acc: 0.7194\n",
      "Epoch 7/125\n",
      "5952/5952 [==============================] - 10s 2ms/step - loss: 0.7621 - acc: 0.7241\n",
      "Epoch 8/125\n",
      "5952/5952 [==============================] - 10s 2ms/step - loss: 0.7327 - acc: 0.7391\n",
      "Epoch 9/125\n",
      "5952/5952 [==============================] - 10s 2ms/step - loss: 0.7248 - acc: 0.7350\n",
      "Epoch 10/125\n",
      "5952/5952 [==============================] - 9s 2ms/step - loss: 0.7002 - acc: 0.7480\n",
      "Epoch 11/125\n",
      "5952/5952 [==============================] - 9s 1ms/step - loss: 0.6997 - acc: 0.7448\n",
      "Epoch 12/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.6807 - acc: 0.7549\n",
      "Epoch 13/125\n",
      "5952/5952 [==============================] - 12s 2ms/step - loss: 0.6675 - acc: 0.7567\n",
      "Epoch 14/125\n",
      "5952/5952 [==============================] - 12s 2ms/step - loss: 0.6554 - acc: 0.7634\n",
      "Epoch 15/125\n",
      "5952/5952 [==============================] - 9s 2ms/step - loss: 0.6435 - acc: 0.7673\n",
      "Epoch 16/125\n",
      "5952/5952 [==============================] - 10s 2ms/step - loss: 0.6372 - acc: 0.7718\n",
      "Epoch 17/125\n",
      "5952/5952 [==============================] - 12s 2ms/step - loss: 0.6246 - acc: 0.7767\n",
      "Epoch 18/125\n",
      "5952/5952 [==============================] - 12s 2ms/step - loss: 0.6269 - acc: 0.7750\n",
      "Epoch 19/125\n",
      "5952/5952 [==============================] - 13s 2ms/step - loss: 0.6169 - acc: 0.7755\n",
      "Epoch 20/125\n",
      "5952/5952 [==============================] - 9s 2ms/step - loss: 0.5958 - acc: 0.7848\n",
      "Epoch 21/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.6179 - acc: 0.7770\n",
      "Epoch 22/125\n",
      "5952/5952 [==============================] - 9s 2ms/step - loss: 0.6382 - acc: 0.7728\n",
      "Epoch 23/125\n",
      "5952/5952 [==============================] - 9s 2ms/step - loss: 0.6117 - acc: 0.7873\n",
      "Epoch 24/125\n",
      "5952/5952 [==============================] - 9s 2ms/step - loss: 0.5834 - acc: 0.7903\n",
      "Epoch 25/125\n",
      "5952/5952 [==============================] - 11s 2ms/step - loss: 0.5755 - acc: 0.7969\n",
      "Epoch 26/125\n",
      "5952/5952 [==============================] - 13s 2ms/step - loss: 0.5631 - acc: 0.7977\n",
      "Epoch 27/125\n",
      "5952/5952 [==============================] - 10s 2ms/step - loss: 0.5505 - acc: 0.8012\n",
      "Epoch 28/125\n",
      "5952/5952 [==============================] - 12s 2ms/step - loss: 0.5479 - acc: 0.8021\n",
      "Epoch 29/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.5416 - acc: 0.8038\n",
      "Epoch 30/125\n",
      "5952/5952 [==============================] - 14s 2ms/step - loss: 0.5281 - acc: 0.8117\n",
      "Epoch 31/125\n",
      "5952/5952 [==============================] - 9s 2ms/step - loss: 0.5179 - acc: 0.8157\n",
      "Epoch 32/125\n",
      "5952/5952 [==============================] - 9s 2ms/step - loss: 0.5109 - acc: 0.8132\n",
      "Epoch 33/125\n",
      "5952/5952 [==============================] - 12s 2ms/step - loss: 0.5114 - acc: 0.8217\n",
      "Epoch 34/125\n",
      "5952/5952 [==============================] - 13s 2ms/step - loss: 0.5048 - acc: 0.8204\n",
      "Epoch 35/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.4972 - acc: 0.8164\n",
      "Epoch 36/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.4936 - acc: 0.8259\n",
      "Epoch 37/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.4837 - acc: 0.8234\n",
      "Epoch 38/125\n",
      "5952/5952 [==============================] - 10s 2ms/step - loss: 0.4724 - acc: 0.8308\n",
      "Epoch 39/125\n",
      "5952/5952 [==============================] - 9s 1ms/step - loss: 0.4635 - acc: 0.8310\n",
      "Epoch 40/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.5058 - acc: 0.8110\n",
      "Epoch 41/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.4749 - acc: 0.8317\n",
      "Epoch 42/125\n",
      "5952/5952 [==============================] - 11s 2ms/step - loss: 0.4547 - acc: 0.8385\n",
      "Epoch 43/125\n",
      "5952/5952 [==============================] - 9s 1ms/step - loss: 0.4357 - acc: 0.8458\n",
      "Epoch 44/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.4305 - acc: 0.8468\n",
      "Epoch 45/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.4275 - acc: 0.8446\n",
      "Epoch 46/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.4296 - acc: 0.8481\n",
      "Epoch 47/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.4057 - acc: 0.8513\n",
      "Epoch 48/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.3978 - acc: 0.8569\n",
      "Epoch 49/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.4106 - acc: 0.8530\n",
      "Epoch 50/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.3884 - acc: 0.8569\n",
      "Epoch 51/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.3741 - acc: 0.8637\n",
      "Epoch 52/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.3784 - acc: 0.8646\n",
      "Epoch 53/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.3704 - acc: 0.8705\n",
      "Epoch 54/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.3679 - acc: 0.8693\n",
      "Epoch 55/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.3462 - acc: 0.8753\n",
      "Epoch 56/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.3654 - acc: 0.8703\n",
      "Epoch 57/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.3504 - acc: 0.8750\n",
      "Epoch 58/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.3351 - acc: 0.8810\n",
      "Epoch 59/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.3385 - acc: 0.8800\n",
      "Epoch 60/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.3233 - acc: 0.8839\n",
      "Epoch 61/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.3074 - acc: 0.8910\n",
      "Epoch 62/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.3236 - acc: 0.8831\n",
      "Epoch 63/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2926 - acc: 0.8980\n",
      "Epoch 64/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.3109 - acc: 0.8883\n",
      "Epoch 65/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.3070 - acc: 0.8955\n",
      "Epoch 66/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.3043 - acc: 0.8943\n",
      "Epoch 67/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2829 - acc: 0.8995\n",
      "Epoch 68/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2781 - acc: 0.9041\n",
      "Epoch 69/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2680 - acc: 0.9073\n",
      "Epoch 70/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2606 - acc: 0.9078\n",
      "Epoch 71/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2630 - acc: 0.9106\n",
      "Epoch 72/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2344 - acc: 0.9162\n",
      "Epoch 73/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2504 - acc: 0.9155\n",
      "Epoch 74/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2548 - acc: 0.9108\n",
      "Epoch 75/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2342 - acc: 0.9172\n",
      "Epoch 76/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2266 - acc: 0.9205\n",
      "Epoch 77/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2498 - acc: 0.9143\n",
      "Epoch 78/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2260 - acc: 0.9219\n",
      "Epoch 79/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2166 - acc: 0.9267\n",
      "Epoch 80/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2287 - acc: 0.9234\n",
      "Epoch 81/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.2163 - acc: 0.9304\n",
      "Epoch 82/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2046 - acc: 0.9286\n",
      "Epoch 83/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.2050 - acc: 0.9315\n",
      "Epoch 84/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.1980 - acc: 0.9343\n",
      "Epoch 85/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.2039 - acc: 0.9318\n",
      "Epoch 86/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.1961 - acc: 0.9301\n",
      "Epoch 87/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.1820 - acc: 0.9393\n",
      "Epoch 88/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1727 - acc: 0.9407\n",
      "Epoch 89/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1819 - acc: 0.9429\n",
      "Epoch 90/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1869 - acc: 0.9387\n",
      "Epoch 91/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1695 - acc: 0.9454\n",
      "Epoch 92/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1711 - acc: 0.9415\n",
      "Epoch 93/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1934 - acc: 0.9373\n",
      "Epoch 94/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1747 - acc: 0.9427\n",
      "Epoch 95/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1685 - acc: 0.9449\n",
      "Epoch 96/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1622 - acc: 0.9484\n",
      "Epoch 97/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1472 - acc: 0.9496\n",
      "Epoch 98/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1482 - acc: 0.9506\n",
      "Epoch 99/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1456 - acc: 0.9538\n",
      "Epoch 100/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1330 - acc: 0.9545\n",
      "Epoch 101/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1550 - acc: 0.9516\n",
      "Epoch 102/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1569 - acc: 0.9484\n",
      "Epoch 103/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1382 - acc: 0.9567\n",
      "Epoch 104/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1242 - acc: 0.9595\n",
      "Epoch 105/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1380 - acc: 0.9588\n",
      "Epoch 106/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1827 - acc: 0.9451\n",
      "Epoch 107/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1498 - acc: 0.9521\n",
      "Epoch 108/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: 0.1329 - acc: 0.9551\n",
      "Epoch 109/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1161 - acc: 0.9640\n",
      "Epoch 110/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1326 - acc: 0.9593\n",
      "Epoch 111/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1244 - acc: 0.9610\n",
      "Epoch 112/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1158 - acc: 0.9656\n",
      "Epoch 113/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1284 - acc: 0.9577\n",
      "Epoch 114/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1307 - acc: 0.9620\n",
      "Epoch 115/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1149 - acc: 0.9609\n",
      "Epoch 116/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1117 - acc: 0.9661\n",
      "Epoch 117/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1082 - acc: 0.9662\n",
      "Epoch 118/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1051 - acc: 0.9704\n",
      "Epoch 119/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1143 - acc: 0.9669\n",
      "Epoch 120/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1048 - acc: 0.9688\n",
      "Epoch 121/125\n",
      "5952/5952 [==============================] - 9s 2ms/step - loss: 0.1103 - acc: 0.9699\n",
      "Epoch 122/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1053 - acc: 0.9714\n",
      "Epoch 123/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: 0.1031 - acc: 0.9679\n",
      "Epoch 124/125\n",
      "5952/5952 [==============================] - 8s 1ms/step - loss: nan - acc: 0.5370\n",
      "Epoch 125/125\n",
      "5952/5952 [==============================] - 7s 1ms/step - loss: nan - acc: 0.0160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x264a57210>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model3_merged.fit(x = encode_and_pad(trec_merged['question'].values),\n",
    "           y = one_hot_labels(trec_merged['label'].values),\n",
    "           epochs = 125\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model3_merged.save(\"../models/trec_lstm3.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
