{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.text import one_hot\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Flatten\n",
    "# from keras.layers import Embedding\n",
    "# from keras.layers import LSTM\n",
    "# from keras.layers import Activation\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Classification\n",
    "\n",
    "In this notebook I build a series of deep learning models that are used to classify questions based on their answer type and their detailed answer type. \n",
    "\n",
    "These models will initially be evaluated using a developement/test set, but then the entire dataset will be used for training, and the models will be evaluated by the utility of their predictions for the main downstream task - deduplicating question intent. \n",
    "\n",
    "Thank you Dr. Jason Brownlee for [this great post](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/) on how to use pretrained word embeddings!\n",
    "\n",
    "### Contents\n",
    "\n",
    "#### 0. Load `TREC` dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trec_train = pd.read_csv(\"../data/TREC/processed/train.csv\")\n",
    "trec_test = pd.read_csv(\"../data/TREC/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>extended_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>DESC:manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>ENTY:cremat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>DESC:manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>ENTY:animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What is the full form of . com ?</td>\n",
       "      <td>ABBR</td>\n",
       "      <td>ABBR:exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>What contemptible scoundrel stole the cork fro...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM:ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>What team did baseball 's St . Louis Browns be...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM:gr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>What is the oldest profession ?</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM:title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>What are liver enzymes ?</td>\n",
       "      <td>DESC</td>\n",
       "      <td>DESC:def</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Name the scar - faced bounty hunter of The Old...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM:ind</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           question label  \\\n",
       "0           1  How did serfdom develop in and then leave Russ...  DESC   \n",
       "1           2  What films featured the character Popeye Doyle ?   ENTY   \n",
       "2           3  How can I find a list of celebrities ' real na...  DESC   \n",
       "3           4  What fowl grabs the spotlight after the Chines...  ENTY   \n",
       "4           5                  What is the full form of . com ?   ABBR   \n",
       "5           6  What contemptible scoundrel stole the cork fro...   HUM   \n",
       "6           7  What team did baseball 's St . Louis Browns be...   HUM   \n",
       "7           8                   What is the oldest profession ?    HUM   \n",
       "8           9                          What are liver enzymes ?   DESC   \n",
       "9          10  Name the scar - faced bounty hunter of The Old...   HUM   \n",
       "\n",
       "  extended_label  \n",
       "0    DESC:manner  \n",
       "1    ENTY:cremat  \n",
       "2    DESC:manner  \n",
       "3    ENTY:animal  \n",
       "4       ABBR:exp  \n",
       "5        HUM:ind  \n",
       "6         HUM:gr  \n",
       "7      HUM:title  \n",
       "8       DESC:def  \n",
       "9        HUM:ind  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(LabelEncoder().fit_transform(trec_test['label'].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Shuffle TREC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trec_train = trec_train.sample(frac=1.0, random_state = 550)\n",
    "trec_test = trec_test.sample(frac = 1.0, random_state = 550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Prepare document encoding \n",
    "\n",
    "Using Kera's `Tokenizer` class, create a dictionary of all the types in both the quora and the TREC datasets. Encode each document as a vector of indecies of the corresponding types in the dictionary. \n",
    "\n",
    "#### 1.1  `Quora` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_quora = pd.read_csv(\"../data/processed/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor Koh - i - Noor D...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely ? How can I solv...</td>\n",
       "      <td>Find the remainder when math 23 24 math is div...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quickly sugar , sa...</td>\n",
       "      <td>Which fish would survive in salt water ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  qid1  qid2  \\\n",
       "0           1   0     1     2   \n",
       "1           2   1     3     4   \n",
       "2           3   2     5     6   \n",
       "3           4   3     7     8   \n",
       "4           5   4     9    10   \n",
       "\n",
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor Koh - i - Noor D...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely ? How can I solv...   \n",
       "4  Which one dissolve in water quickly sugar , sa...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when math 23 24 math is div...             0  \n",
       "4          Which fish would survive in salt water ?              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_quora.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Get all the questions, from both datasets, in Numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_questions = trec_train['question'].append(trec_test['question']).append(\n",
    "    train_quora['question1']).append(train_quora['question2']).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How did serfdom develop in and then leave Russia ? '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare tokenizer \n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for all the quesions in the joint datasets\n",
    "tokenizer.fit_on_texts(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93122"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of types in the joint datset\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# integer encode the questions\n",
    "encoded_questions = tokenizer.texts_to_sequences(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How did serfdom develop in and then leave Russia ? \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5, 49, 54929, 732, 8, 12, 254, 763, 656]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of a question integer embedding\n",
    "print(all_questions[0])\n",
    "encoded_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I find a list of celebrities ' real names ? \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5, 13, 4, 84, 6, 446, 10, 2450, 2001, 191, 956]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example. Notice how the word `how` is consistently endoded as the integer 5. \n",
    "print(all_questions[2])\n",
    "encoded_questions[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Prepare the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the pretrained fasttext embeddings (this takes a while)\n",
    "embedding_model = KeyedVectors.load_word2vec_format('../data/embeddings/wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each row in the matrix is the embedding of one word in the joint datasets. \n",
    "# The row index corresponds to the integer ecoding of that word. \n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in embedding_model:\n",
    "        embedding_matrix[i] = embedding_model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. An LSTM experiment parameter class\n",
    "\n",
    "I will likely exeperiment with many different models, with many different hyperparameters. It will be useful to keep all those parameters contained in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LstmParams(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 sequence_length = 10, \n",
    "                 labels = \"basic\"):\n",
    "        \n",
    "        # Number of tokens to include in each sequence\n",
    "        self.sequence_length = sequence_length\n",
    "        # whether to use basic or extended labels\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Keep track of the raw training and test questions\n",
    "        self.train_questions = trec_train['question'].values\n",
    "        self.test_questions = trec_test['question'].values\n",
    "        \n",
    "        # Encode the training and test quetions\n",
    "        self.train_questions_encoded = tokenizer.texts_to_sequences(self.train_questions)\n",
    "        self.test_questions_encoded = tokenizer.texts_to_sequences(self.test_questions)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    Return the labels as a numpy array of strings \n",
    "    Which labels to return depends on the objects 'labels' parameter. \n",
    "    '''\n",
    "    def get_train_labels(self):\n",
    "        if self.labels == \"basic\":\n",
    "            return(trec_train['label'].values)\n",
    "        elif self.labels == \"extended\":\n",
    "            return(trec_train[\"extended_label\"].values)\n",
    "        else:\n",
    "            print(\"Invalid `labels` parameter '%s'. Returning basic labels.\") % (self.labels)\n",
    "            return(trec_train['label'].values)\n",
    "        \n",
    "    def get_test_labels(self):\n",
    "        if self.labels == \"basic\":\n",
    "            return(trec_test['label'].values)\n",
    "        elif self.labels == \"extended\":\n",
    "            return(trec_test[\"extended_label\"].values)\n",
    "        else:\n",
    "            print(\"Invalid `labels` parameter '%s'. Returning basic labels.\") % (self.labels)\n",
    "            return(trec_test['label'].values)\n",
    "        \n",
    "\n",
    "    '''\n",
    "    Return the labels of the as a numpy ndarray, using one-hot encoding. \n",
    "    This is for transparency in my Neural Network archetecture. \n",
    "    '''\n",
    "    def get_train_labels_onehot(self):\n",
    "        if self.labels == \"basic\":\n",
    "            return(to_categorical(LabelEncoder().fit_transform(trec_train['label'].values)))\n",
    "        elif self.labels == \"extended\":\n",
    "            return(to_categorical(LabelEncoder().fit_transform(trec_train['extended_label'].values)))\n",
    "        else:\n",
    "            print(\"Invalid `labels` parameter '%s'. Returning basic labels.\") % (self.labels)\n",
    "            return(to_categorical(LabelEncoder().fit_transform(trec_train['label'].values)))\n",
    "        \n",
    "    def get_test_labels_onehot(self):\n",
    "        if self.labels == \"basic\":\n",
    "            return(to_categorical(LabelEncoder().fit_transform(trec_test['label'].values)))\n",
    "        elif self.labels == \"extended\":\n",
    "            return(to_categorical(LabelEncoder().fit_transform(trec_test['extended_label'].values)))\n",
    "        else:\n",
    "            print(\"Invalid `labels` parameter '%s'. Returning basic labels.\") % (self.labels)\n",
    "            return(to_categorical(LabelEncoder().fit_transform(trec_test['label'].values)))\n",
    "    '''\n",
    "    Return the encoded questions after padding. \n",
    "    Padding (or truncating) amount depends on attribute `self.sequence_length`\n",
    "    '''\n",
    "    def get_train_padded(self):\n",
    "        padded = pad_sequences(self.train_questions_encoded, \n",
    "                              maxlen = self.sequence_length,\n",
    "                              padding = \"post\", \n",
    "                              truncating = \"post\")\n",
    "        return(padded)\n",
    "    \n",
    "    def get_test_padded(self):\n",
    "        padded = pad_sequences(self.test_questions_encoded, \n",
    "                              maxlen = self.sequence_length,\n",
    "                              padding = \"post\", \n",
    "                              truncating = \"post\")\n",
    "        return(padded)\n",
    "    \n",
    "    '''\n",
    "    Get the number of classes (output layer dimension). \n",
    "    This is the number of unique classes. \n",
    "    '''\n",
    "    def get_num_classes(self):\n",
    "        n_unique = len(np.unique(self.get_test_labels().tolist() + self.get_train_labels().tolist()))\n",
    "        return(n_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 4. A first model!\n",
    "\n",
    "First, I'll try the most basic vanilla LSTM for the TREC classification problem (simple labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.0 Initialize Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params1 = LstmParams(sequence_length = 10,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Add layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model1.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params1.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model1.add(LSTM(params1.get_num_classes(), activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 10, 300)           27936600  \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 6)                 7368      \n",
      "=================================================================\n",
      "Total params: 27,943,968\n",
      "Trainable params: 7,368\n",
      "Non-trainable params: 27,936,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5452/5452 [==============================] - 4s 815us/step - loss: 1.7306 - acc: 0.2544\n",
      "Epoch 2/50\n",
      "5452/5452 [==============================] - 3s 493us/step - loss: 1.6703 - acc: 0.2804\n",
      "Epoch 3/50\n",
      "5452/5452 [==============================] - 3s 582us/step - loss: 1.6471 - acc: 0.2931\n",
      "Epoch 4/50\n",
      "5452/5452 [==============================] - 3s 518us/step - loss: 1.6338 - acc: 0.2948\n",
      "Epoch 5/50\n",
      "5452/5452 [==============================] - 3s 525us/step - loss: 1.6205 - acc: 0.3056\n",
      "Epoch 6/50\n",
      "5452/5452 [==============================] - 3s 590us/step - loss: 1.6015 - acc: 0.3206\n",
      "Epoch 7/50\n",
      "5452/5452 [==============================] - 3s 524us/step - loss: 1.5685 - acc: 0.3487\n",
      "Epoch 8/50\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 1.5240 - acc: 0.3782\n",
      "Epoch 9/50\n",
      "5452/5452 [==============================] - 3s 486us/step - loss: 1.4549 - acc: 0.4483\n",
      "Epoch 10/50\n",
      "5452/5452 [==============================] - 2s 410us/step - loss: 1.3846 - acc: 0.4868\n",
      "Epoch 11/50\n",
      "5452/5452 [==============================] - 3s 581us/step - loss: 1.3273 - acc: 0.5108\n",
      "Epoch 12/50\n",
      "5452/5452 [==============================] - 3s 535us/step - loss: 1.2721 - acc: 0.5348\n",
      "Epoch 13/50\n",
      "5452/5452 [==============================] - 3s 561us/step - loss: 1.2159 - acc: 0.5492\n",
      "Epoch 14/50\n",
      "5452/5452 [==============================] - 3s 491us/step - loss: 1.1648 - acc: 0.5721\n",
      "Epoch 15/50\n",
      "5452/5452 [==============================] - 3s 513us/step - loss: 1.1196 - acc: 0.5871\n",
      "Epoch 16/50\n",
      "5452/5452 [==============================] - 3s 560us/step - loss: 1.0780 - acc: 0.6045\n",
      "Epoch 17/50\n",
      "5452/5452 [==============================] - 3s 524us/step - loss: 1.0399 - acc: 0.6220\n",
      "Epoch 18/50\n",
      "5452/5452 [==============================] - 3s 542us/step - loss: 1.0074 - acc: 0.6346\n",
      "Epoch 19/50\n",
      "5452/5452 [==============================] - 3s 525us/step - loss: 0.9794 - acc: 0.6491\n",
      "Epoch 20/50\n",
      "5452/5452 [==============================] - 3s 521us/step - loss: 0.9533 - acc: 0.6596\n",
      "Epoch 21/50\n",
      "5452/5452 [==============================] - 3s 485us/step - loss: 0.9295 - acc: 0.6724\n",
      "Epoch 22/50\n",
      "5452/5452 [==============================] - 3s 478us/step - loss: 0.9074 - acc: 0.6831\n",
      "Epoch 23/50\n",
      "5452/5452 [==============================] - 3s 487us/step - loss: 0.8858 - acc: 0.6930\n",
      "Epoch 24/50\n",
      "5452/5452 [==============================] - 3s 519us/step - loss: 0.8655 - acc: 0.7019\n",
      "Epoch 25/50\n",
      "5452/5452 [==============================] - 3s 527us/step - loss: 0.8475 - acc: 0.7076\n",
      "Epoch 26/50\n",
      "5452/5452 [==============================] - 3s 480us/step - loss: 0.8300 - acc: 0.7093\n",
      "Epoch 27/50\n",
      "5452/5452 [==============================] - 3s 543us/step - loss: 0.8146 - acc: 0.7148\n",
      "Epoch 28/50\n",
      "5452/5452 [==============================] - 3s 490us/step - loss: 0.8006 - acc: 0.7163\n",
      "Epoch 29/50\n",
      "5452/5452 [==============================] - 3s 510us/step - loss: 0.7876 - acc: 0.7225\n",
      "Epoch 30/50\n",
      "5452/5452 [==============================] - 3s 476us/step - loss: 0.7751 - acc: 0.7230\n",
      "Epoch 31/50\n",
      "5452/5452 [==============================] - 3s 499us/step - loss: 0.7635 - acc: 0.7296\n",
      "Epoch 32/50\n",
      "5452/5452 [==============================] - 3s 471us/step - loss: 0.7537 - acc: 0.7350\n",
      "Epoch 33/50\n",
      "5452/5452 [==============================] - 3s 487us/step - loss: 0.7453 - acc: 0.7335\n",
      "Epoch 34/50\n",
      "5452/5452 [==============================] - 3s 490us/step - loss: 0.7362 - acc: 0.7383\n",
      "Epoch 35/50\n",
      "5452/5452 [==============================] - 3s 545us/step - loss: 0.7286 - acc: 0.7416\n",
      "Epoch 36/50\n",
      "5452/5452 [==============================] - 3s 505us/step - loss: 0.7201 - acc: 0.7430\n",
      "Epoch 37/50\n",
      "5452/5452 [==============================] - 3s 475us/step - loss: 0.7131 - acc: 0.7450\n",
      "Epoch 38/50\n",
      "5452/5452 [==============================] - 3s 512us/step - loss: 0.7070 - acc: 0.7504\n",
      "Epoch 39/50\n",
      "5452/5452 [==============================] - 3s 488us/step - loss: 0.7001 - acc: 0.7533\n",
      "Epoch 40/50\n",
      "5452/5452 [==============================] - 3s 470us/step - loss: 0.6937 - acc: 0.7546\n",
      "Epoch 41/50\n",
      "5452/5452 [==============================] - 3s 511us/step - loss: 0.6874 - acc: 0.7581\n",
      "Epoch 42/50\n",
      "5452/5452 [==============================] - 3s 503us/step - loss: 0.6818 - acc: 0.7616\n",
      "Epoch 43/50\n",
      "5452/5452 [==============================] - 3s 529us/step - loss: 0.6762 - acc: 0.7641\n",
      "Epoch 44/50\n",
      "5452/5452 [==============================] - 3s 485us/step - loss: 0.6714 - acc: 0.7636\n",
      "Epoch 45/50\n",
      "5452/5452 [==============================] - 3s 497us/step - loss: 0.6658 - acc: 0.7667\n",
      "Epoch 46/50\n",
      "5452/5452 [==============================] - 3s 475us/step - loss: 0.6613 - acc: 0.7694\n",
      "Epoch 47/50\n",
      "5452/5452 [==============================] - 3s 578us/step - loss: 0.6569 - acc: 0.7711\n",
      "Epoch 48/50\n",
      "5452/5452 [==============================] - 3s 477us/step - loss: 0.6514 - acc: 0.7711\n",
      "Epoch 49/50\n",
      "5452/5452 [==============================] - 3s 485us/step - loss: 0.6478 - acc: 0.7735\n",
      "Epoch 50/50\n",
      "5452/5452 [==============================] - 3s 474us/step - loss: 0.6426 - acc: 0.7760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2f8544ad0>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model1.fit(x = params1.get_train_padded(),\n",
    "           y = params1.get_train_labels_onehot(),\n",
    "           epochs = 50\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 73.0%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model1.evaluate(params1.get_test_padded(),\n",
    "                params1.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
