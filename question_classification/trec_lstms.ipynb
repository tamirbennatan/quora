{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Classification\n",
    "\n",
    "In this notebook I build a series of deep learning models that are used to classify questions based on their answer type and their detailed answer type. \n",
    "\n",
    "These models will initially be evaluated using a developement/test set, but then the entire dataset will be used for training, and the models will be evaluated by the utility of their predictions for the main downstream task - deduplicating question intent. \n",
    "\n",
    "Thank you Dr. Jason Brownlee for [this great post](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/) on how to use pretrained word embeddings!\n",
    "\n",
    "### Contents\n",
    "\n",
    "#### 0. Load `TREC` dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trec_train = pd.read_csv(\"../data/TREC/processed/train.csv\")\n",
    "trec_test = pd.read_csv(\"../data/TREC/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>extended_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>DESC:manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>ENTY:cremat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>DESC</td>\n",
       "      <td>DESC:manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>ENTY</td>\n",
       "      <td>ENTY:animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What is the full form of . com ?</td>\n",
       "      <td>ABBR</td>\n",
       "      <td>ABBR:exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>What contemptible scoundrel stole the cork fro...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM:ind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>What team did baseball 's St . Louis Browns be...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM:gr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>What is the oldest profession ?</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM:title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>What are liver enzymes ?</td>\n",
       "      <td>DESC</td>\n",
       "      <td>DESC:def</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Name the scar - faced bounty hunter of The Old...</td>\n",
       "      <td>HUM</td>\n",
       "      <td>HUM:ind</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           question label  \\\n",
       "0           1  How did serfdom develop in and then leave Russ...  DESC   \n",
       "1           2  What films featured the character Popeye Doyle ?   ENTY   \n",
       "2           3  How can I find a list of celebrities ' real na...  DESC   \n",
       "3           4  What fowl grabs the spotlight after the Chines...  ENTY   \n",
       "4           5                  What is the full form of . com ?   ABBR   \n",
       "5           6  What contemptible scoundrel stole the cork fro...   HUM   \n",
       "6           7  What team did baseball 's St . Louis Browns be...   HUM   \n",
       "7           8                   What is the oldest profession ?    HUM   \n",
       "8           9                          What are liver enzymes ?   DESC   \n",
       "9          10  Name the scar - faced bounty hunter of The Old...   HUM   \n",
       "\n",
       "  extended_label  \n",
       "0    DESC:manner  \n",
       "1    ENTY:cremat  \n",
       "2    DESC:manner  \n",
       "3    ENTY:animal  \n",
       "4       ABBR:exp  \n",
       "5        HUM:ind  \n",
       "6         HUM:gr  \n",
       "7      HUM:title  \n",
       "8       DESC:def  \n",
       "9        HUM:ind  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Shuffle TREC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trec_train = trec_train.sample(frac=1.0, random_state = 550)\n",
    "trec_test = trec_test.sample(frac = 1.0, random_state = 550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Prepare document encoding \n",
    "\n",
    "Using Kera's `Tokenizer` class, create a dictionary of all the types in both the quora and the TREC datasets. Encode each document as a vector of indecies of the corresponding types in the dictionary. \n",
    "\n",
    "#### 1.1  `Quora` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_quora = pd.read_csv(\"../data/processed/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor Koh - i - Noor D...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely ? How can I solv...</td>\n",
       "      <td>Find the remainder when math 23 24 math is div...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quickly sugar , sa...</td>\n",
       "      <td>Which fish would survive in salt water ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  qid1  qid2  \\\n",
       "0           1   0     1     2   \n",
       "1           2   1     3     4   \n",
       "2           3   2     5     6   \n",
       "3           4   3     7     8   \n",
       "4           5   4     9    10   \n",
       "\n",
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor Koh - i - Noor D...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely ? How can I solv...   \n",
       "4  Which one dissolve in water quickly sugar , sa...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when math 23 24 math is div...             0  \n",
       "4          Which fish would survive in salt water ?              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_quora.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Get all the questions, from both datasets, in Numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_questions = trec_train['question'].append(trec_test['question']).append(\n",
    "    train_quora['question1']).append(train_quora['question2']).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who comprised the now - defunct comic book team known as the Champions ? '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare tokenizer \n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for all the quesions in the joint datasets\n",
    "tokenizer.fit_on_texts(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93122"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of types in the joint datset\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# integer encode the questions\n",
    "encoded_questions = tokenizer.texts_to_sequences(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who comprised the now - defunct comic book team known as the Champions ? \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[39, 28302, 1, 165, 35438, 3840, 161, 765, 582, 46, 1, 7203]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of a question integer embedding\n",
    "print(all_questions[0])\n",
    "encoded_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the wingspan of a condor ? \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 54930, 10, 6, 41926]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example. Notice how the word `how` is consistently endoded as the integer 5. \n",
    "print(all_questions[2])\n",
    "encoded_questions[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Prepare the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the pretrained fasttext embeddings (this takes a while)\n",
    "embedding_model = KeyedVectors.load_word2vec_format('../data/embeddings/wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each row in the matrix is the embedding of one word in the joint datasets. \n",
    "# The row index corresponds to the integer ecoding of that word. \n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in embedding_model:\n",
    "        embedding_matrix[i] = embedding_model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. An LSTM experiment parameter class\n",
    "\n",
    "I will likely exeperiment with many different models, with many different hyperparameters. It will be useful to keep all those parameters contained in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LstmParams(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 sequence_length = 10, \n",
    "                 labels = \"basic\"):\n",
    "        \n",
    "        # Number of tokens to include in each sequence\n",
    "        self.sequence_length = sequence_length\n",
    "        # whether to use basic or extended labels\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Keep track of the raw training and test questions\n",
    "        self.train_questions = trec_train['question'].values\n",
    "        self.test_questions = trec_test['question'].values\n",
    "        \n",
    "        # Encode the training and test quetions\n",
    "        self.train_questions_encoded = tokenizer.texts_to_sequences(self.train_questions)\n",
    "        self.test_questions_encoded = tokenizer.texts_to_sequences(self.test_questions)\n",
    "        \n",
    "        # Store a label encode for each class. This will be particular to label type of the problem\n",
    "        self.label_encoder = self.get_encoder()\n",
    "        \n",
    "       \n",
    "    \n",
    "    '''\n",
    "    Fit a label encoder to the data, to represent the labels as one-hot vectors\n",
    "    The dimension of these vectors will depend on if the user decided to predict\n",
    "    basic or extended labels, which have 6 and 50 potential categories, respectively. \n",
    "    '''\n",
    "    def get_encoder(self):\n",
    "        # get all the labels, train and test sets, in one list\n",
    "        all_labels = self.get_train_labels().tolist() + self.get_test_labels().tolist()\n",
    "        # fit a label encoder to those labelse\n",
    "        encoder = LabelEncoder().fit(np.array(all_labels))\n",
    "        return(encoder)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Decode one-hot representation of labels back to regular labels.\n",
    "    input: an array of one-hot label arrays. \n",
    "    outpu: an array of regular character labels. \n",
    "    '''\n",
    "    def decode_labels(self,onehot_labels):\n",
    "        # first, get the labels as integers\n",
    "        integer_labels = [np.where(r==1)[0][0] for r in onehot_labels]\n",
    "        # now, return the decoded label\n",
    "        return(self.label_encoder.inverse_transform(integer_labels))\n",
    "    \n",
    "    '''\n",
    "    Return the labels as a numpy array of strings \n",
    "    Which labels to return depends on the objects 'labels' parameter. \n",
    "    '''\n",
    "    def get_train_labels(self):\n",
    "        if self.labels == \"basic\":\n",
    "            return(trec_train['label'].values)\n",
    "        elif self.labels == \"extended\":\n",
    "            return(trec_train[\"extended_label\"].values)\n",
    "        else:\n",
    "            print(\"Invalid `labels` parameter '%s'. Returning basic labels.\") % (self.labels)\n",
    "            return(trec_train['label'].values)\n",
    "        \n",
    "    def get_test_labels(self):\n",
    "        if self.labels == \"basic\":\n",
    "            return(trec_test['label'].values)\n",
    "        elif self.labels == \"extended\":\n",
    "            return(trec_test[\"extended_label\"].values)\n",
    "        else:\n",
    "            print(\"Invalid `labels` parameter '%s'. Returning basic labels.\") % (self.labels)\n",
    "            return(trec_test['label'].values)\n",
    "        \n",
    "\n",
    "    '''\n",
    "    Return the labels of the as a numpy ndarray, using one-hot encoding. \n",
    "    This is for transparency in my Neural Network archetecture. \n",
    "    '''\n",
    "    def get_train_labels_onehot(self):\n",
    "        return(to_categorical(self.label_encoder.transform(self.get_train_labels())))\n",
    "\n",
    "        \n",
    "    def get_test_labels_onehot(self):\n",
    "        return(to_categorical(self.label_encoder.transform(self.get_test_labels())))\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    Return the encoded questions after padding. \n",
    "    Padding (or truncating) amount depends on attribute `self.sequence_length`\n",
    "    '''\n",
    "    def get_train_padded(self):\n",
    "        padded = pad_sequences(self.train_questions_encoded, \n",
    "                              maxlen = self.sequence_length,\n",
    "                              padding = \"post\", \n",
    "                              truncating = \"post\")\n",
    "        return(padded)\n",
    "    \n",
    "    def get_test_padded(self):\n",
    "        padded = pad_sequences(self.test_questions_encoded, \n",
    "                              maxlen = self.sequence_length,\n",
    "                              padding = \"post\", \n",
    "                              truncating = \"post\")\n",
    "        return(padded)\n",
    "    \n",
    "    '''\n",
    "    Get the number of classes (output layer dimension). \n",
    "    This is the number of unique classes. \n",
    "    '''\n",
    "    def get_num_classes(self):\n",
    "        n_unique = len(np.unique(self.get_test_labels().tolist() + self.get_train_labels().tolist()))\n",
    "        return(n_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 4. A first model for the basic labels\n",
    "\n",
    "First, I'll try the most basic vanilla LSTM for the TREC classification problem (simple labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.0 Initialize Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params1 = LstmParams(sequence_length = 10,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Add layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model1.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params1.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model1.add(LSTM(params1.get_num_classes(), activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 10, 300)           27936600  \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 6)                 7368      \n",
      "=================================================================\n",
      "Total params: 27,943,968\n",
      "Trainable params: 7,368\n",
      "Non-trainable params: 27,936,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5452/5452 [==============================] - 5s 835us/step - loss: 1.7369 - acc: 0.2436\n",
      "Epoch 2/200\n",
      "5452/5452 [==============================] - 3s 468us/step - loss: 1.6752 - acc: 0.2731\n",
      "Epoch 3/200\n",
      "5452/5452 [==============================] - 3s 473us/step - loss: 1.6511 - acc: 0.3001\n",
      "Epoch 4/200\n",
      "5452/5452 [==============================] - 3s 627us/step - loss: 1.6373 - acc: 0.3083\n",
      "Epoch 5/200\n",
      "5452/5452 [==============================] - 3s 475us/step - loss: 1.6251 - acc: 0.3107\n",
      "Epoch 6/200\n",
      "5452/5452 [==============================] - 3s 489us/step - loss: 1.6090 - acc: 0.3278\n",
      "Epoch 7/200\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 1.5789 - acc: 0.3577\n",
      "Epoch 8/200\n",
      "5452/5452 [==============================] - 3s 535us/step - loss: 1.5238 - acc: 0.4206\n",
      "Epoch 9/200\n",
      "5452/5452 [==============================] - 3s 565us/step - loss: 1.4426 - acc: 0.4655\n",
      "Epoch 10/200\n",
      "5452/5452 [==============================] - 3s 471us/step - loss: 1.3750 - acc: 0.4868\n",
      "Epoch 11/200\n",
      "5452/5452 [==============================] - 4s 648us/step - loss: 1.3249 - acc: 0.5017\n",
      "Epoch 12/200\n",
      "5452/5452 [==============================] - 3s 501us/step - loss: 1.2773 - acc: 0.5222\n",
      "Epoch 13/200\n",
      "5452/5452 [==============================] - 3s 529us/step - loss: 1.2249 - acc: 0.5442\n",
      "Epoch 14/200\n",
      "5452/5452 [==============================] - 2s 422us/step - loss: 1.1753 - acc: 0.5644\n",
      "Epoch 15/200\n",
      "5452/5452 [==============================] - 2s 417us/step - loss: 1.1331 - acc: 0.5791\n",
      "Epoch 16/200\n",
      "5452/5452 [==============================] - 3s 537us/step - loss: 1.0964 - acc: 0.5972\n",
      "Epoch 17/200\n",
      "5452/5452 [==============================] - 3s 487us/step - loss: 1.0631 - acc: 0.6139\n",
      "Epoch 18/200\n",
      "5452/5452 [==============================] - 3s 535us/step - loss: 1.0304 - acc: 0.6278\n",
      "Epoch 19/200\n",
      "5452/5452 [==============================] - 3s 521us/step - loss: 0.9991 - acc: 0.6445\n",
      "Epoch 20/200\n",
      "5452/5452 [==============================] - 3s 525us/step - loss: 0.9686 - acc: 0.6640\n",
      "Epoch 21/200\n",
      "5452/5452 [==============================] - 2s 417us/step - loss: 0.9398 - acc: 0.6731\n",
      "Epoch 22/200\n",
      "5452/5452 [==============================] - 2s 403us/step - loss: 0.9121 - acc: 0.6862\n",
      "Epoch 23/200\n",
      "5452/5452 [==============================] - 3s 495us/step - loss: 0.8873 - acc: 0.6924\n",
      "Epoch 24/200\n",
      "5452/5452 [==============================] - 3s 482us/step - loss: 0.8641 - acc: 0.7019\n",
      "Epoch 25/200\n",
      "5452/5452 [==============================] - 3s 463us/step - loss: 0.8429 - acc: 0.7049\n",
      "Epoch 26/200\n",
      "5452/5452 [==============================] - 2s 400us/step - loss: 0.8233 - acc: 0.7089\n",
      "Epoch 27/200\n",
      "5452/5452 [==============================] - 3s 553us/step - loss: 0.8049 - acc: 0.7175\n",
      "Epoch 28/200\n",
      "5452/5452 [==============================] - 3s 489us/step - loss: 0.7907 - acc: 0.7192\n",
      "Epoch 29/200\n",
      "5452/5452 [==============================] - 3s 460us/step - loss: 0.7764 - acc: 0.7229\n",
      "Epoch 30/200\n",
      "5452/5452 [==============================] - 3s 474us/step - loss: 0.7634 - acc: 0.7287\n",
      "Epoch 31/200\n",
      "5452/5452 [==============================] - 2s 443us/step - loss: 0.7515 - acc: 0.7309\n",
      "Epoch 32/200\n",
      "5452/5452 [==============================] - 2s 393us/step - loss: 0.7403 - acc: 0.7348\n",
      "Epoch 33/200\n",
      "5452/5452 [==============================] - 3s 534us/step - loss: 0.7306 - acc: 0.7397\n",
      "Epoch 34/200\n",
      "5452/5452 [==============================] - 2s 448us/step - loss: 0.7211 - acc: 0.7427\n",
      "Epoch 35/200\n",
      "5452/5452 [==============================] - 2s 446us/step - loss: 0.7123 - acc: 0.7458\n",
      "Epoch 36/200\n",
      "5452/5452 [==============================] - 3s 471us/step - loss: 0.7033 - acc: 0.7494\n",
      "Epoch 37/200\n",
      "5452/5452 [==============================] - 2s 431us/step - loss: 0.6954 - acc: 0.7553\n",
      "Epoch 38/200\n",
      "5452/5452 [==============================] - 3s 545us/step - loss: 0.6891 - acc: 0.7559\n",
      "Epoch 39/200\n",
      "5452/5452 [==============================] - 2s 436us/step - loss: 0.6817 - acc: 0.7575\n",
      "Epoch 40/200\n",
      "5452/5452 [==============================] - 2s 426us/step - loss: 0.6747 - acc: 0.7627\n",
      "Epoch 41/200\n",
      "5452/5452 [==============================] - 3s 533us/step - loss: 0.6687 - acc: 0.7643\n",
      "Epoch 42/200\n",
      "5452/5452 [==============================] - 3s 460us/step - loss: 0.6615 - acc: 0.7663\n",
      "Epoch 43/200\n",
      "5452/5452 [==============================] - 2s 431us/step - loss: 0.6563 - acc: 0.7698\n",
      "Epoch 44/200\n",
      "5452/5452 [==============================] - 3s 548us/step - loss: 0.6504 - acc: 0.7720\n",
      "Epoch 45/200\n",
      "5452/5452 [==============================] - 4s 653us/step - loss: 0.6453 - acc: 0.7760\n",
      "Epoch 46/200\n",
      "5452/5452 [==============================] - 5s 955us/step - loss: 0.6397 - acc: 0.7790\n",
      "Epoch 47/200\n",
      "5452/5452 [==============================] - 3s 571us/step - loss: 0.6349 - acc: 0.7779\n",
      "Epoch 48/200\n",
      "5452/5452 [==============================] - 2s 386us/step - loss: 0.6298 - acc: 0.7782\n",
      "Epoch 49/200\n",
      "5452/5452 [==============================] - 2s 438us/step - loss: 0.6250 - acc: 0.7843\n",
      "Epoch 50/200\n",
      "5452/5452 [==============================] - 3s 638us/step - loss: 0.6204 - acc: 0.7832\n",
      "Epoch 51/200\n",
      "5452/5452 [==============================] - 4s 725us/step - loss: 0.6152 - acc: 0.7876\n",
      "Epoch 52/200\n",
      "5452/5452 [==============================] - 4s 657us/step - loss: 0.6105 - acc: 0.7900\n",
      "Epoch 53/200\n",
      "5452/5452 [==============================] - 3s 469us/step - loss: 0.6067 - acc: 0.7891\n",
      "Epoch 54/200\n",
      "5452/5452 [==============================] - 2s 424us/step - loss: 0.6027 - acc: 0.7887\n",
      "Epoch 55/200\n",
      "5452/5452 [==============================] - 2s 400us/step - loss: 0.5981 - acc: 0.7907\n",
      "Epoch 56/200\n",
      "5452/5452 [==============================] - 2s 430us/step - loss: 0.5926 - acc: 0.7926\n",
      "Epoch 57/200\n",
      "5452/5452 [==============================] - 2s 456us/step - loss: 0.5892 - acc: 0.7951\n",
      "Epoch 58/200\n",
      "5452/5452 [==============================] - 2s 388us/step - loss: 0.5847 - acc: 0.8003\n",
      "Epoch 59/200\n",
      "5452/5452 [==============================] - 2s 376us/step - loss: 0.5813 - acc: 0.8003\n",
      "Epoch 60/200\n",
      "5452/5452 [==============================] - 3s 460us/step - loss: 0.5776 - acc: 0.8025\n",
      "Epoch 61/200\n",
      "5452/5452 [==============================] - 2s 442us/step - loss: 0.5738 - acc: 0.8019\n",
      "Epoch 62/200\n",
      "5452/5452 [==============================] - 2s 416us/step - loss: 0.5708 - acc: 0.8054\n",
      "Epoch 63/200\n",
      "5452/5452 [==============================] - 2s 443us/step - loss: 0.5669 - acc: 0.8056\n",
      "Epoch 64/200\n",
      "5452/5452 [==============================] - 3s 466us/step - loss: 0.5632 - acc: 0.8087\n",
      "Epoch 65/200\n",
      "5452/5452 [==============================] - 3s 460us/step - loss: 0.5598 - acc: 0.8089\n",
      "Epoch 66/200\n",
      "5452/5452 [==============================] - 3s 472us/step - loss: 0.5575 - acc: 0.8078\n",
      "Epoch 67/200\n",
      "5452/5452 [==============================] - 2s 432us/step - loss: 0.5543 - acc: 0.8127\n",
      "Epoch 68/200\n",
      "5452/5452 [==============================] - 3s 480us/step - loss: 0.5509 - acc: 0.8122\n",
      "Epoch 69/200\n",
      "5452/5452 [==============================] - 2s 438us/step - loss: 0.5481 - acc: 0.8138\n",
      "Epoch 70/200\n",
      "5452/5452 [==============================] - 2s 442us/step - loss: 0.5442 - acc: 0.8138\n",
      "Epoch 71/200\n",
      "5452/5452 [==============================] - 3s 459us/step - loss: 0.5420 - acc: 0.8177\n",
      "Epoch 72/200\n",
      "5452/5452 [==============================] - 2s 430us/step - loss: 0.5383 - acc: 0.8184\n",
      "Epoch 73/200\n",
      "5452/5452 [==============================] - 3s 472us/step - loss: 0.5365 - acc: 0.8208\n",
      "Epoch 74/200\n",
      "5452/5452 [==============================] - 2s 436us/step - loss: 0.5333 - acc: 0.8213\n",
      "Epoch 75/200\n",
      "5452/5452 [==============================] - 2s 424us/step - loss: 0.5309 - acc: 0.8202\n",
      "Epoch 76/200\n",
      "5452/5452 [==============================] - 3s 469us/step - loss: 0.5276 - acc: 0.8230\n",
      "Epoch 77/200\n",
      "5452/5452 [==============================] - 2s 429us/step - loss: 0.5258 - acc: 0.8237\n",
      "Epoch 78/200\n",
      "5452/5452 [==============================] - 2s 418us/step - loss: 0.5222 - acc: 0.8247\n",
      "Epoch 79/200\n",
      "5452/5452 [==============================] - 2s 451us/step - loss: 0.5201 - acc: 0.8259\n",
      "Epoch 80/200\n",
      "5452/5452 [==============================] - 2s 441us/step - loss: 0.5181 - acc: 0.8272\n",
      "Epoch 81/200\n",
      "5452/5452 [==============================] - 3s 464us/step - loss: 0.5142 - acc: 0.8261\n",
      "Epoch 82/200\n",
      "5452/5452 [==============================] - 2s 402us/step - loss: 0.5119 - acc: 0.8276\n",
      "Epoch 83/200\n",
      "5452/5452 [==============================] - 2s 436us/step - loss: 0.5102 - acc: 0.8300\n",
      "Epoch 84/200\n",
      "5452/5452 [==============================] - 2s 448us/step - loss: 0.5079 - acc: 0.8320\n",
      "Epoch 85/200\n",
      "5452/5452 [==============================] - 2s 418us/step - loss: 0.5040 - acc: 0.8342\n",
      "Epoch 86/200\n",
      "5452/5452 [==============================] - 2s 416us/step - loss: 0.5013 - acc: 0.8358\n",
      "Epoch 87/200\n",
      "5452/5452 [==============================] - 2s 400us/step - loss: 0.4973 - acc: 0.8358\n",
      "Epoch 88/200\n",
      "5452/5452 [==============================] - 2s 438us/step - loss: 0.4916 - acc: 0.8329\n",
      "Epoch 89/200\n",
      "5452/5452 [==============================] - 2s 402us/step - loss: 0.4876 - acc: 0.8375\n",
      "Epoch 90/200\n",
      "5452/5452 [==============================] - 2s 434us/step - loss: 0.4853 - acc: 0.8382\n",
      "Epoch 91/200\n",
      "5452/5452 [==============================] - 2s 415us/step - loss: 0.4827 - acc: 0.8397\n",
      "Epoch 92/200\n",
      "5452/5452 [==============================] - 3s 506us/step - loss: 0.4807 - acc: 0.8399\n",
      "Epoch 93/200\n",
      "5452/5452 [==============================] - 4s 742us/step - loss: 0.4769 - acc: 0.8417\n",
      "Epoch 94/200\n",
      "5452/5452 [==============================] - 2s 448us/step - loss: 0.4745 - acc: 0.8439\n",
      "Epoch 95/200\n",
      "5452/5452 [==============================] - 2s 415us/step - loss: 0.4729 - acc: 0.8428\n",
      "Epoch 96/200\n",
      "5452/5452 [==============================] - 2s 382us/step - loss: 0.4703 - acc: 0.8445\n",
      "Epoch 97/200\n",
      "5452/5452 [==============================] - 2s 423us/step - loss: 0.4679 - acc: 0.8467\n",
      "Epoch 98/200\n",
      "5452/5452 [==============================] - 2s 432us/step - loss: 0.4656 - acc: 0.8450\n",
      "Epoch 99/200\n",
      "5452/5452 [==============================] - 2s 416us/step - loss: 0.4629 - acc: 0.8468\n",
      "Epoch 100/200\n",
      "5452/5452 [==============================] - 3s 462us/step - loss: 0.4598 - acc: 0.8494\n",
      "Epoch 101/200\n",
      "5452/5452 [==============================] - 2s 428us/step - loss: 0.4570 - acc: 0.8472\n",
      "Epoch 102/200\n",
      "5452/5452 [==============================] - 2s 420us/step - loss: 0.4535 - acc: 0.8487\n",
      "Epoch 103/200\n",
      "5452/5452 [==============================] - 3s 473us/step - loss: 0.4512 - acc: 0.8520\n",
      "Epoch 104/200\n",
      "5452/5452 [==============================] - 3s 514us/step - loss: 0.4502 - acc: 0.8536\n",
      "Epoch 105/200\n",
      "5452/5452 [==============================] - 3s 471us/step - loss: 0.4463 - acc: 0.8540\n",
      "Epoch 106/200\n",
      "5452/5452 [==============================] - 3s 466us/step - loss: 0.4456 - acc: 0.8545\n",
      "Epoch 107/200\n",
      "5452/5452 [==============================] - 2s 444us/step - loss: 0.4425 - acc: 0.8573\n",
      "Epoch 108/200\n",
      "5452/5452 [==============================] - 2s 430us/step - loss: 0.4403 - acc: 0.8571\n",
      "Epoch 109/200\n",
      "5452/5452 [==============================] - 2s 436us/step - loss: 0.4388 - acc: 0.8556\n",
      "Epoch 110/200\n",
      "5452/5452 [==============================] - 2s 435us/step - loss: 0.4367 - acc: 0.8556\n",
      "Epoch 111/200\n",
      "5452/5452 [==============================] - 2s 421us/step - loss: 0.4346 - acc: 0.8599\n",
      "Epoch 112/200\n",
      "5452/5452 [==============================] - 2s 417us/step - loss: 0.4335 - acc: 0.8588\n",
      "Epoch 113/200\n",
      "5452/5452 [==============================] - 2s 439us/step - loss: 0.4310 - acc: 0.8604\n",
      "Epoch 114/200\n",
      "5452/5452 [==============================] - 3s 564us/step - loss: 0.4291 - acc: 0.8599\n",
      "Epoch 115/200\n",
      "5452/5452 [==============================] - 4s 718us/step - loss: 0.4269 - acc: 0.8612\n",
      "Epoch 116/200\n",
      "5452/5452 [==============================] - 3s 641us/step - loss: 0.4259 - acc: 0.8639\n",
      "Epoch 117/200\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.4248 - acc: 0.8639\n",
      "Epoch 118/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 0.4224 - acc: 0.8650\n",
      "Epoch 119/200\n",
      "5452/5452 [==============================] - 3s 603us/step - loss: 0.4195 - acc: 0.8670\n",
      "Epoch 120/200\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.4191 - acc: 0.8676\n",
      "Epoch 121/200\n",
      "5452/5452 [==============================] - 4s 643us/step - loss: 0.4174 - acc: 0.8656\n",
      "Epoch 122/200\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 0.4147 - acc: 0.8681\n",
      "Epoch 123/200\n",
      "5452/5452 [==============================] - 3s 626us/step - loss: 0.4138 - acc: 0.8685\n",
      "Epoch 124/200\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.4119 - acc: 0.8694\n",
      "Epoch 125/200\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 0.4100 - acc: 0.8701\n",
      "Epoch 126/200\n",
      "5452/5452 [==============================] - 3s 610us/step - loss: 0.4094 - acc: 0.8692\n",
      "Epoch 127/200\n",
      "5452/5452 [==============================] - 3s 620us/step - loss: 0.4069 - acc: 0.8727\n",
      "Epoch 128/200\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.4053 - acc: 0.8736\n",
      "Epoch 129/200\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 0.4032 - acc: 0.8745\n",
      "Epoch 130/200\n",
      "5452/5452 [==============================] - 3s 610us/step - loss: 0.4024 - acc: 0.8729\n",
      "Epoch 131/200\n",
      "5452/5452 [==============================] - 3s 624us/step - loss: 0.4012 - acc: 0.8745\n",
      "Epoch 132/200\n",
      "5452/5452 [==============================] - 3s 619us/step - loss: 0.3995 - acc: 0.8733\n",
      "Epoch 133/200\n",
      "5452/5452 [==============================] - 3s 634us/step - loss: 0.3976 - acc: 0.8755\n",
      "Epoch 134/200\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 0.3963 - acc: 0.8758\n",
      "Epoch 135/200\n",
      "5452/5452 [==============================] - 3s 610us/step - loss: 0.3949 - acc: 0.8762\n",
      "Epoch 136/200\n",
      "5452/5452 [==============================] - 3s 631us/step - loss: 0.3926 - acc: 0.8797\n",
      "Epoch 137/200\n",
      "5452/5452 [==============================] - 4s 645us/step - loss: 0.3919 - acc: 0.8784\n",
      "Epoch 138/200\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 0.3905 - acc: 0.8789\n",
      "Epoch 139/200\n",
      "5452/5452 [==============================] - 3s 572us/step - loss: 0.3894 - acc: 0.8800\n",
      "Epoch 140/200\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 0.3870 - acc: 0.8800\n",
      "Epoch 141/200\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 0.3864 - acc: 0.8813\n",
      "Epoch 142/200\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 0.3858 - acc: 0.8817\n",
      "Epoch 143/200\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 0.3835 - acc: 0.8819\n",
      "Epoch 144/200\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 0.3824 - acc: 0.8802\n",
      "Epoch 145/200\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.3807 - acc: 0.8822\n",
      "Epoch 146/200\n",
      "5452/5452 [==============================] - 3s 605us/step - loss: 0.3795 - acc: 0.8824\n",
      "Epoch 147/200\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 0.3785 - acc: 0.8844\n",
      "Epoch 148/200\n",
      "5452/5452 [==============================] - 3s 592us/step - loss: 0.3773 - acc: 0.8854\n",
      "Epoch 149/200\n",
      "5452/5452 [==============================] - 3s 551us/step - loss: 0.3760 - acc: 0.8857\n",
      "Epoch 150/200\n",
      "5452/5452 [==============================] - 3s 568us/step - loss: 0.3745 - acc: 0.8850\n",
      "Epoch 151/200\n",
      "5452/5452 [==============================] - 4s 645us/step - loss: 0.3733 - acc: 0.8857\n",
      "Epoch 152/200\n",
      "5452/5452 [==============================] - 3s 603us/step - loss: 0.3723 - acc: 0.8863\n",
      "Epoch 153/200\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.3707 - acc: 0.8865\n",
      "Epoch 154/200\n",
      "5452/5452 [==============================] - 3s 624us/step - loss: 0.3689 - acc: 0.8877\n",
      "Epoch 155/200\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.3696 - acc: 0.8907\n",
      "Epoch 156/200\n",
      "5452/5452 [==============================] - 3s 632us/step - loss: 0.3668 - acc: 0.8883\n",
      "Epoch 157/200\n",
      "5452/5452 [==============================] - 3s 627us/step - loss: 0.3661 - acc: 0.8888\n",
      "Epoch 158/200\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.3647 - acc: 0.8923\n",
      "Epoch 159/200\n",
      "5452/5452 [==============================] - 3s 594us/step - loss: 0.3641 - acc: 0.8905\n",
      "Epoch 160/200\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 0.3627 - acc: 0.8890\n",
      "Epoch 161/200\n",
      "5452/5452 [==============================] - 3s 604us/step - loss: 0.3613 - acc: 0.8920\n",
      "Epoch 162/200\n",
      "5452/5452 [==============================] - 3s 594us/step - loss: 0.3609 - acc: 0.8931\n",
      "Epoch 163/200\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.3585 - acc: 0.8933\n",
      "Epoch 164/200\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 0.3580 - acc: 0.8934\n",
      "Epoch 165/200\n",
      "5452/5452 [==============================] - 4s 665us/step - loss: 0.3592 - acc: 0.8945\n",
      "Epoch 166/200\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.3556 - acc: 0.8927\n",
      "Epoch 167/200\n",
      "5452/5452 [==============================] - 3s 603us/step - loss: 0.3547 - acc: 0.8947\n",
      "Epoch 168/200\n",
      "5452/5452 [==============================] - 3s 586us/step - loss: 0.3533 - acc: 0.8947\n",
      "Epoch 169/200\n",
      "5452/5452 [==============================] - 4s 683us/step - loss: 0.3520 - acc: 0.8962\n",
      "Epoch 170/200\n",
      "5452/5452 [==============================] - 3s 630us/step - loss: 0.3517 - acc: 0.8964\n",
      "Epoch 171/200\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 0.3500 - acc: 0.8956\n",
      "Epoch 172/200\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 0.3498 - acc: 0.8958\n",
      "Epoch 173/200\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 0.3478 - acc: 0.8977\n",
      "Epoch 174/200\n",
      "5452/5452 [==============================] - 3s 625us/step - loss: 0.3485 - acc: 0.8958\n",
      "Epoch 175/200\n",
      "5452/5452 [==============================] - 3s 609us/step - loss: 0.3475 - acc: 0.8958\n",
      "Epoch 176/200\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 0.3456 - acc: 0.8958\n",
      "Epoch 177/200\n",
      "5452/5452 [==============================] - 3s 588us/step - loss: 0.3441 - acc: 0.8984\n",
      "Epoch 178/200\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 0.3438 - acc: 0.8975\n",
      "Epoch 179/200\n",
      "5452/5452 [==============================] - 3s 630us/step - loss: 0.3425 - acc: 0.8991\n",
      "Epoch 180/200\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.3418 - acc: 0.8988\n",
      "Epoch 181/200\n",
      "5452/5452 [==============================] - 3s 609us/step - loss: 0.3422 - acc: 0.9002\n",
      "Epoch 182/200\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.3397 - acc: 0.9008\n",
      "Epoch 183/200\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 0.3384 - acc: 0.9010\n",
      "Epoch 184/200\n",
      "5452/5452 [==============================] - 3s 622us/step - loss: 0.3363 - acc: 0.9013\n",
      "Epoch 185/200\n",
      "5452/5452 [==============================] - 3s 639us/step - loss: 0.3357 - acc: 0.9033\n",
      "Epoch 186/200\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 0.3354 - acc: 0.9024\n",
      "Epoch 187/200\n",
      "5452/5452 [==============================] - 3s 588us/step - loss: 0.3349 - acc: 0.9024\n",
      "Epoch 188/200\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 0.3338 - acc: 0.9011\n",
      "Epoch 189/200\n",
      "5452/5452 [==============================] - 3s 597us/step - loss: 0.3321 - acc: 0.9043\n",
      "Epoch 190/200\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.3312 - acc: 0.9037\n",
      "Epoch 191/200\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.3309 - acc: 0.9026\n",
      "Epoch 192/200\n",
      "5452/5452 [==============================] - 3s 602us/step - loss: 0.3298 - acc: 0.9057\n",
      "Epoch 193/200\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.3279 - acc: 0.9046\n",
      "Epoch 194/200\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 0.3267 - acc: 0.9033\n",
      "Epoch 195/200\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.3263 - acc: 0.9065\n",
      "Epoch 196/200\n",
      "5452/5452 [==============================] - 3s 629us/step - loss: 0.3255 - acc: 0.9059\n",
      "Epoch 197/200\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 0.3237 - acc: 0.9065\n",
      "Epoch 198/200\n",
      "5452/5452 [==============================] - 4s 711us/step - loss: 0.3227 - acc: 0.9070\n",
      "Epoch 199/200\n",
      "5452/5452 [==============================] - 4s 799us/step - loss: 0.3227 - acc: 0.9070\n",
      "Epoch 200/200\n",
      "5452/5452 [==============================] - 3s 582us/step - loss: 0.3207 - acc: 0.9096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d0aacb50>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model1.fit(x = params1.get_train_padded(),\n",
    "           y = params1.get_train_labels_onehot(),\n",
    "           epochs = 200\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 80.5999999046%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model1.evaluate(params1.get_test_padded(),\n",
    "                params1.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. A first model for the extended labels\n",
    "\n",
    "On the very first try, I got a test set accuracy of 80% when predicting the basic labels. Although without the use of a developmement set and without using proper experimentation I can't claim anything about the generalizability of this first classifier, it does show me that the proble of prdicting the basic label is a very tractable one. \n",
    "\n",
    "Now, I'll train this same basic classifier to predict the extended label to see how much harder that problem is, if at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params2 = LstmParams(sequence_length = 10, labels=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model2.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params2.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model2.add(LSTM(params2.get_num_classes(), activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 10, 300)           27936600  \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 50)                70200     \n",
      "=================================================================\n",
      "Total params: 28,006,800\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 27,936,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5452/5452 [==============================] - 5s 952us/step - loss: 3.7752 - acc: 0.1097\n",
      "Epoch 2/200\n",
      "5452/5452 [==============================] - 3s 639us/step - loss: 3.4962 - acc: 0.1669\n",
      "Epoch 3/200\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 3.3841 - acc: 0.1942\n",
      "Epoch 4/200\n",
      "5452/5452 [==============================] - 3s 588us/step - loss: 3.3287 - acc: 0.2164\n",
      "Epoch 5/200\n",
      "5452/5452 [==============================] - 3s 636us/step - loss: 3.2167 - acc: 0.2570\n",
      "Epoch 6/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 3.1223 - acc: 0.2680\n",
      "Epoch 7/200\n",
      "5452/5452 [==============================] - 3s 630us/step - loss: 3.0692 - acc: 0.2729\n",
      "Epoch 8/200\n",
      "5452/5452 [==============================] - 3s 592us/step - loss: 3.0181 - acc: 0.2771\n",
      "Epoch 9/200\n",
      "5452/5452 [==============================] - 3s 581us/step - loss: 2.9705 - acc: 0.2799\n",
      "Epoch 10/200\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 2.9258 - acc: 0.2904\n",
      "Epoch 11/200\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 2.8764 - acc: 0.3025\n",
      "Epoch 12/200\n",
      "5452/5452 [==============================] - 3s 626us/step - loss: 2.8159 - acc: 0.3173\n",
      "Epoch 13/200\n",
      "5452/5452 [==============================] - 4s 674us/step - loss: 2.7756 - acc: 0.3230\n",
      "Epoch 14/200\n",
      "5452/5452 [==============================] - 3s 618us/step - loss: 2.7373 - acc: 0.3346\n",
      "Epoch 15/200\n",
      "5452/5452 [==============================] - 3s 577us/step - loss: 2.7029 - acc: 0.3467\n",
      "Epoch 16/200\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 2.6693 - acc: 0.3538\n",
      "Epoch 17/200\n",
      "5452/5452 [==============================] - 3s 641us/step - loss: 2.6387 - acc: 0.3624\n",
      "Epoch 18/200\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 2.6122 - acc: 0.3639\n",
      "Epoch 19/200\n",
      "5452/5452 [==============================] - 3s 602us/step - loss: 2.5881 - acc: 0.3733\n",
      "Epoch 20/200\n",
      "5452/5452 [==============================] - 3s 570us/step - loss: 2.5647 - acc: 0.3778\n",
      "Epoch 21/200\n",
      "5452/5452 [==============================] - 3s 617us/step - loss: 2.5429 - acc: 0.3844\n",
      "Epoch 22/200\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 2.5215 - acc: 0.3916\n",
      "Epoch 23/200\n",
      "5452/5452 [==============================] - 3s 578us/step - loss: 2.5027 - acc: 0.3949\n",
      "Epoch 24/200\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 2.4845 - acc: 0.4037\n",
      "Epoch 25/200\n",
      "5452/5452 [==============================] - 3s 602us/step - loss: 2.4672 - acc: 0.4055\n",
      "Epoch 26/200\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 2.4506 - acc: 0.4103\n",
      "Epoch 27/200\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 2.4355 - acc: 0.4142\n",
      "Epoch 28/200\n",
      "5452/5452 [==============================] - 3s 605us/step - loss: 2.4207 - acc: 0.4142\n",
      "Epoch 29/200\n",
      "5452/5452 [==============================] - 3s 625us/step - loss: 2.4074 - acc: 0.4182\n",
      "Epoch 30/200\n",
      "5452/5452 [==============================] - 3s 598us/step - loss: 2.3938 - acc: 0.4209\n",
      "Epoch 31/200\n",
      "5452/5452 [==============================] - 3s 622us/step - loss: 2.3804 - acc: 0.4248\n",
      "Epoch 32/200\n",
      "5452/5452 [==============================] - 3s 624us/step - loss: 2.3678 - acc: 0.4299\n",
      "Epoch 33/200\n",
      "5452/5452 [==============================] - 3s 577us/step - loss: 2.3556 - acc: 0.4316\n",
      "Epoch 34/200\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 2.3443 - acc: 0.4356\n",
      "Epoch 35/200\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 2.3288 - acc: 0.4376\n",
      "Epoch 36/200\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 2.3130 - acc: 0.4420\n",
      "Epoch 37/200\n",
      "5452/5452 [==============================] - 3s 576us/step - loss: 2.3010 - acc: 0.4417\n",
      "Epoch 38/200\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 2.2896 - acc: 0.4450\n",
      "Epoch 39/200\n",
      "5452/5452 [==============================] - 3s 569us/step - loss: 2.2794 - acc: 0.4448\n",
      "Epoch 40/200\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 2.2673 - acc: 0.4486\n",
      "Epoch 41/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 2.2565 - acc: 0.4518\n",
      "Epoch 42/200\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 2.2467 - acc: 0.4536\n",
      "Epoch 43/200\n",
      "5452/5452 [==============================] - 3s 563us/step - loss: 2.2367 - acc: 0.4560\n",
      "Epoch 44/200\n",
      "5452/5452 [==============================] - 3s 581us/step - loss: 2.2268 - acc: 0.4573\n",
      "Epoch 45/200\n",
      "5452/5452 [==============================] - 3s 554us/step - loss: 2.2164 - acc: 0.4589\n",
      "Epoch 46/200\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 2.2063 - acc: 0.4631\n",
      "Epoch 47/200\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 2.1967 - acc: 0.4640\n",
      "Epoch 48/200\n",
      "5452/5452 [==============================] - 3s 564us/step - loss: 2.1881 - acc: 0.4666\n",
      "Epoch 49/200\n",
      "5452/5452 [==============================] - 3s 537us/step - loss: 2.1794 - acc: 0.4675\n",
      "Epoch 50/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 2.1702 - acc: 0.4686\n",
      "Epoch 51/200\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 2.1618 - acc: 0.4705\n",
      "Epoch 52/200\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 2.1532 - acc: 0.4716\n",
      "Epoch 53/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 2.1451 - acc: 0.4729\n",
      "Epoch 54/200\n",
      "5452/5452 [==============================] - 3s 604us/step - loss: 2.1364 - acc: 0.4756\n",
      "Epoch 55/200\n",
      "5452/5452 [==============================] - 3s 556us/step - loss: 2.1283 - acc: 0.4760\n",
      "Epoch 56/200\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 2.1200 - acc: 0.4780\n",
      "Epoch 57/200\n",
      "5452/5452 [==============================] - 3s 609us/step - loss: 2.1113 - acc: 0.4809\n",
      "Epoch 58/200\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 2.1035 - acc: 0.4807\n",
      "Epoch 59/200\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 2.0950 - acc: 0.4835\n",
      "Epoch 60/200\n",
      "5452/5452 [==============================] - 3s 521us/step - loss: 2.0866 - acc: 0.4868\n",
      "Epoch 61/200\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 2.0777 - acc: 0.4894\n",
      "Epoch 62/200\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 2.0679 - acc: 0.4917\n",
      "Epoch 63/200\n",
      "5452/5452 [==============================] - 3s 634us/step - loss: 2.0585 - acc: 0.4943\n",
      "Epoch 64/200\n",
      "5452/5452 [==============================] - 3s 484us/step - loss: 2.0504 - acc: 0.4939\n",
      "Epoch 65/200\n",
      "5452/5452 [==============================] - 3s 633us/step - loss: 2.0429 - acc: 0.4976\n",
      "Epoch 66/200\n",
      "5452/5452 [==============================] - 3s 577us/step - loss: 2.0356 - acc: 0.4998\n",
      "Epoch 67/200\n",
      "5452/5452 [==============================] - 3s 521us/step - loss: 2.0281 - acc: 0.5028\n",
      "Epoch 68/200\n",
      "5452/5452 [==============================] - 3s 548us/step - loss: 2.0209 - acc: 0.5064\n",
      "Epoch 69/200\n",
      "5452/5452 [==============================] - 3s 640us/step - loss: 2.0131 - acc: 0.5101\n",
      "Epoch 70/200\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 2.0064 - acc: 0.5114\n",
      "Epoch 71/200\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 1.9997 - acc: 0.5136\n",
      "Epoch 72/200\n",
      "5452/5452 [==============================] - 4s 656us/step - loss: 1.9924 - acc: 0.5152\n",
      "Epoch 73/200\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 1.9843 - acc: 0.5189\n",
      "Epoch 74/200\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 1.9768 - acc: 0.5200\n",
      "Epoch 75/200\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 1.9702 - acc: 0.5244\n",
      "Epoch 76/200\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 1.9642 - acc: 0.5244\n",
      "Epoch 77/200\n",
      "5452/5452 [==============================] - 3s 584us/step - loss: 1.9584 - acc: 0.5279\n",
      "Epoch 78/200\n",
      "5452/5452 [==============================] - 3s 592us/step - loss: 1.9518 - acc: 0.5286\n",
      "Epoch 79/200\n",
      "5452/5452 [==============================] - 3s 507us/step - loss: 1.9459 - acc: 0.5325\n",
      "Epoch 80/200\n",
      "5452/5452 [==============================] - 3s 507us/step - loss: 1.9397 - acc: 0.5321\n",
      "Epoch 81/200\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 1.9341 - acc: 0.5332\n",
      "Epoch 82/200\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 1.9283 - acc: 0.5352\n",
      "Epoch 83/200\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 1.9227 - acc: 0.5360\n",
      "Epoch 84/200\n",
      "5452/5452 [==============================] - 3s 632us/step - loss: 1.9170 - acc: 0.5396\n",
      "Epoch 85/200\n",
      "5452/5452 [==============================] - 3s 603us/step - loss: 1.9118 - acc: 0.5396\n",
      "Epoch 86/200\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 1.9067 - acc: 0.5383\n",
      "Epoch 87/200\n",
      "5452/5452 [==============================] - 4s 645us/step - loss: 1.9014 - acc: 0.5418\n",
      "Epoch 88/200\n",
      "5452/5452 [==============================] - 3s 575us/step - loss: 1.8959 - acc: 0.5429\n",
      "Epoch 89/200\n",
      "5452/5452 [==============================] - 3s 522us/step - loss: 1.8914 - acc: 0.5442\n",
      "Epoch 90/200\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 1.8863 - acc: 0.5451\n",
      "Epoch 91/200\n",
      "5452/5452 [==============================] - 3s 539us/step - loss: 1.8816 - acc: 0.5471\n",
      "Epoch 92/200\n",
      "5452/5452 [==============================] - 3s 591us/step - loss: 1.8769 - acc: 0.5503\n",
      "Epoch 93/200\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 1.8718 - acc: 0.5512\n",
      "Epoch 94/200\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 1.8669 - acc: 0.5530\n",
      "Epoch 95/200\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 1.8619 - acc: 0.5539\n",
      "Epoch 96/200\n",
      "5452/5452 [==============================] - 3s 591us/step - loss: 1.8564 - acc: 0.5552\n",
      "Epoch 97/200\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 1.8512 - acc: 0.5550\n",
      "Epoch 98/200\n",
      "5452/5452 [==============================] - 3s 525us/step - loss: 1.8458 - acc: 0.5583\n",
      "Epoch 99/200\n",
      "5452/5452 [==============================] - 3s 554us/step - loss: 1.8412 - acc: 0.5592\n",
      "Epoch 100/200\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 1.8364 - acc: 0.5618\n",
      "Epoch 101/200\n",
      "5452/5452 [==============================] - 3s 579us/step - loss: 1.8312 - acc: 0.5616\n",
      "Epoch 102/200\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 1.8264 - acc: 0.5613\n",
      "Epoch 103/200\n",
      "5452/5452 [==============================] - 3s 565us/step - loss: 1.8215 - acc: 0.5640\n",
      "Epoch 104/200\n",
      "5452/5452 [==============================] - 3s 558us/step - loss: 1.8177 - acc: 0.5642\n",
      "Epoch 105/200\n",
      "5452/5452 [==============================] - 3s 547us/step - loss: 1.8126 - acc: 0.5668\n",
      "Epoch 106/200\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 1.8079 - acc: 0.5699\n",
      "Epoch 107/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 1.8030 - acc: 0.5680\n",
      "Epoch 108/200\n",
      "5452/5452 [==============================] - 3s 567us/step - loss: 1.7979 - acc: 0.5702\n",
      "Epoch 109/200\n",
      "5452/5452 [==============================] - 3s 571us/step - loss: 1.7938 - acc: 0.5725\n",
      "Epoch 110/200\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 1.7889 - acc: 0.5706\n",
      "Epoch 111/200\n",
      "5452/5452 [==============================] - 3s 591us/step - loss: 1.7846 - acc: 0.5745\n",
      "Epoch 112/200\n",
      "5452/5452 [==============================] - 3s 610us/step - loss: 1.7805 - acc: 0.5743\n",
      "Epoch 113/200\n",
      "5452/5452 [==============================] - 3s 619us/step - loss: 1.7762 - acc: 0.5759\n",
      "Epoch 114/200\n",
      "5452/5452 [==============================] - 3s 571us/step - loss: 1.7719 - acc: 0.5774\n",
      "Epoch 115/200\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 1.7681 - acc: 0.5791\n",
      "Epoch 116/200\n",
      "5452/5452 [==============================] - 3s 545us/step - loss: 1.7642 - acc: 0.5800\n",
      "Epoch 117/200\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 1.7601 - acc: 0.5813\n",
      "Epoch 118/200\n",
      "5452/5452 [==============================] - 3s 576us/step - loss: 1.7555 - acc: 0.5816\n",
      "Epoch 119/200\n",
      "5452/5452 [==============================] - 3s 532us/step - loss: 1.7521 - acc: 0.5829\n",
      "Epoch 120/200\n",
      "5452/5452 [==============================] - 3s 580us/step - loss: 1.7477 - acc: 0.5835\n",
      "Epoch 121/200\n",
      "5452/5452 [==============================] - 3s 534us/step - loss: 1.7440 - acc: 0.5829\n",
      "Epoch 122/200\n",
      "5452/5452 [==============================] - 3s 565us/step - loss: 1.7400 - acc: 0.5864\n",
      "Epoch 123/200\n",
      "5452/5452 [==============================] - 3s 549us/step - loss: 1.7367 - acc: 0.5877\n",
      "Epoch 124/200\n",
      "5452/5452 [==============================] - 3s 566us/step - loss: 1.7327 - acc: 0.5886\n",
      "Epoch 125/200\n",
      "5452/5452 [==============================] - 3s 559us/step - loss: 1.7282 - acc: 0.5908\n",
      "Epoch 126/200\n",
      "5452/5452 [==============================] - 3s 567us/step - loss: 1.7247 - acc: 0.5880\n",
      "Epoch 127/200\n",
      "5452/5452 [==============================] - 3s 586us/step - loss: 1.7210 - acc: 0.5921\n",
      "Epoch 128/200\n",
      "5452/5452 [==============================] - 3s 567us/step - loss: 1.7170 - acc: 0.5939\n",
      "Epoch 129/200\n",
      "5452/5452 [==============================] - 3s 530us/step - loss: 1.7127 - acc: 0.5932\n",
      "Epoch 130/200\n",
      "5452/5452 [==============================] - 3s 563us/step - loss: 1.7093 - acc: 0.5945\n",
      "Epoch 131/200\n",
      "5452/5452 [==============================] - 3s 561us/step - loss: 1.7057 - acc: 0.5952\n",
      "Epoch 132/200\n",
      "5452/5452 [==============================] - 3s 590us/step - loss: 1.7015 - acc: 0.5974\n",
      "Epoch 133/200\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 1.6975 - acc: 0.5979\n",
      "Epoch 134/200\n",
      "5452/5452 [==============================] - 3s 563us/step - loss: 1.6939 - acc: 0.5985\n",
      "Epoch 135/200\n",
      "5452/5452 [==============================] - 3s 596us/step - loss: 1.6898 - acc: 0.5990\n",
      "Epoch 136/200\n",
      "5452/5452 [==============================] - 3s 584us/step - loss: 1.6853 - acc: 0.5990\n",
      "Epoch 137/200\n",
      "5452/5452 [==============================] - 3s 528us/step - loss: 1.6818 - acc: 0.6003\n",
      "Epoch 138/200\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 1.6776 - acc: 0.6012\n",
      "Epoch 139/200\n",
      "5452/5452 [==============================] - 3s 567us/step - loss: 1.6734 - acc: 0.6014\n",
      "Epoch 140/200\n",
      "5452/5452 [==============================] - 3s 555us/step - loss: 1.6694 - acc: 0.6016\n",
      "Epoch 141/200\n",
      "5452/5452 [==============================] - 3s 604us/step - loss: 1.6659 - acc: 0.6020\n",
      "Epoch 142/200\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 1.6615 - acc: 0.6031\n",
      "Epoch 143/200\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 1.6580 - acc: 0.6042\n",
      "Epoch 144/200\n",
      "5452/5452 [==============================] - 3s 479us/step - loss: 1.6542 - acc: 0.6056\n",
      "Epoch 145/200\n",
      "5452/5452 [==============================] - 3s 542us/step - loss: 1.6506 - acc: 0.6045\n",
      "Epoch 146/200\n",
      "5452/5452 [==============================] - 3s 588us/step - loss: 1.6468 - acc: 0.6079\n",
      "Epoch 147/200\n",
      "5452/5452 [==============================] - 3s 547us/step - loss: 1.6431 - acc: 0.6053\n",
      "Epoch 148/200\n",
      "5452/5452 [==============================] - 3s 548us/step - loss: 1.6398 - acc: 0.6082\n",
      "Epoch 149/200\n",
      "5452/5452 [==============================] - 3s 535us/step - loss: 1.6356 - acc: 0.6084\n",
      "Epoch 150/200\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 1.6318 - acc: 0.6084\n",
      "Epoch 151/200\n",
      "5452/5452 [==============================] - 3s 597us/step - loss: 1.6283 - acc: 0.6095\n",
      "Epoch 152/200\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 1.6246 - acc: 0.6090\n",
      "Epoch 153/200\n",
      "5452/5452 [==============================] - 3s 552us/step - loss: 1.6210 - acc: 0.6101\n",
      "Epoch 154/200\n",
      "5452/5452 [==============================] - 3s 584us/step - loss: 1.6178 - acc: 0.6112\n",
      "Epoch 155/200\n",
      "5452/5452 [==============================] - 3s 548us/step - loss: 1.6137 - acc: 0.6113\n",
      "Epoch 156/200\n",
      "5452/5452 [==============================] - 3s 540us/step - loss: 1.6097 - acc: 0.6123\n",
      "Epoch 157/200\n",
      "5452/5452 [==============================] - 3s 529us/step - loss: 1.6045 - acc: 0.6145\n",
      "Epoch 158/200\n",
      "5452/5452 [==============================] - 3s 549us/step - loss: 1.5986 - acc: 0.6143\n",
      "Epoch 159/200\n",
      "5452/5452 [==============================] - 3s 534us/step - loss: 1.5938 - acc: 0.6148\n",
      "Epoch 160/200\n",
      "5452/5452 [==============================] - 3s 570us/step - loss: 1.5899 - acc: 0.6168\n",
      "Epoch 161/200\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 1.5863 - acc: 0.6185\n",
      "Epoch 162/200\n",
      "5452/5452 [==============================] - 3s 560us/step - loss: 1.5826 - acc: 0.6179\n",
      "Epoch 163/200\n",
      "5452/5452 [==============================] - 3s 555us/step - loss: 1.5792 - acc: 0.6185\n",
      "Epoch 164/200\n",
      "5452/5452 [==============================] - 3s 550us/step - loss: 1.5756 - acc: 0.6190\n",
      "Epoch 165/200\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 1.5720 - acc: 0.6190\n",
      "Epoch 166/200\n",
      "5452/5452 [==============================] - 3s 576us/step - loss: 1.5689 - acc: 0.6196\n",
      "Epoch 167/200\n",
      "5452/5452 [==============================] - 3s 537us/step - loss: 1.5656 - acc: 0.6227\n",
      "Epoch 168/200\n",
      "5452/5452 [==============================] - 3s 555us/step - loss: 1.5623 - acc: 0.6218\n",
      "Epoch 169/200\n",
      "5452/5452 [==============================] - 3s 579us/step - loss: 1.5587 - acc: 0.6225\n",
      "Epoch 170/200\n",
      "5452/5452 [==============================] - 3s 580us/step - loss: 1.5556 - acc: 0.6247\n",
      "Epoch 171/200\n",
      "5452/5452 [==============================] - 3s 547us/step - loss: 1.5524 - acc: 0.6242\n",
      "Epoch 172/200\n",
      "5452/5452 [==============================] - 3s 572us/step - loss: 1.5484 - acc: 0.6264\n",
      "Epoch 173/200\n",
      "5452/5452 [==============================] - 3s 569us/step - loss: 1.5460 - acc: 0.6278\n",
      "Epoch 174/200\n",
      "5452/5452 [==============================] - 3s 552us/step - loss: 1.5426 - acc: 0.6282\n",
      "Epoch 175/200\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 1.5391 - acc: 0.6275\n",
      "Epoch 176/200\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 1.5364 - acc: 0.6289\n",
      "Epoch 177/200\n",
      "5452/5452 [==============================] - 3s 575us/step - loss: 1.5327 - acc: 0.6299\n",
      "Epoch 178/200\n",
      "5452/5452 [==============================] - 3s 543us/step - loss: 1.5298 - acc: 0.6302\n",
      "Epoch 179/200\n",
      "5452/5452 [==============================] - 3s 581us/step - loss: 1.5261 - acc: 0.6310\n",
      "Epoch 180/200\n",
      "5452/5452 [==============================] - 3s 548us/step - loss: 1.5235 - acc: 0.6311\n",
      "Epoch 181/200\n",
      "5452/5452 [==============================] - 3s 550us/step - loss: 1.5200 - acc: 0.6322\n",
      "Epoch 182/200\n",
      "5452/5452 [==============================] - 3s 569us/step - loss: 1.5165 - acc: 0.6335\n",
      "Epoch 183/200\n",
      "5452/5452 [==============================] - 3s 570us/step - loss: 1.5140 - acc: 0.6335\n",
      "Epoch 184/200\n",
      "5452/5452 [==============================] - 3s 572us/step - loss: 1.5107 - acc: 0.6330\n",
      "Epoch 185/200\n",
      "5452/5452 [==============================] - 3s 570us/step - loss: 1.5075 - acc: 0.6352\n",
      "Epoch 186/200\n",
      "5452/5452 [==============================] - 3s 567us/step - loss: 1.5042 - acc: 0.6366\n",
      "Epoch 187/200\n",
      "5452/5452 [==============================] - 3s 569us/step - loss: 1.5010 - acc: 0.6379\n",
      "Epoch 188/200\n",
      "5452/5452 [==============================] - 3s 556us/step - loss: 1.4978 - acc: 0.6381\n",
      "Epoch 189/200\n",
      "5452/5452 [==============================] - 3s 554us/step - loss: 1.4952 - acc: 0.6374\n",
      "Epoch 190/200\n",
      "5452/5452 [==============================] - 3s 520us/step - loss: 1.4921 - acc: 0.6398\n",
      "Epoch 191/200\n",
      "5452/5452 [==============================] - 3s 544us/step - loss: 1.4890 - acc: 0.6399\n",
      "Epoch 192/200\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 1.4857 - acc: 0.6399\n",
      "Epoch 193/200\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 1.4826 - acc: 0.6420\n",
      "Epoch 194/200\n",
      "5452/5452 [==============================] - 3s 539us/step - loss: 1.4800 - acc: 0.6403\n",
      "Epoch 195/200\n",
      "5452/5452 [==============================] - 3s 558us/step - loss: 1.4767 - acc: 0.6421\n",
      "Epoch 196/200\n",
      "5452/5452 [==============================] - 3s 552us/step - loss: 1.4735 - acc: 0.6427\n",
      "Epoch 197/200\n",
      "5452/5452 [==============================] - 3s 580us/step - loss: 1.4704 - acc: 0.6442\n",
      "Epoch 198/200\n",
      "5452/5452 [==============================] - 3s 577us/step - loss: 1.4672 - acc: 0.6429\n",
      "Epoch 199/200\n",
      "5452/5452 [==============================] - 3s 530us/step - loss: 1.4638 - acc: 0.6449\n",
      "Epoch 200/200\n",
      "5452/5452 [==============================] - 3s 582us/step - loss: 1.4609 - acc: 0.6456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d238ce10>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model2.fit(x = params2.get_train_padded(),\n",
    "           y = params2.get_train_labels_onehot(),\n",
    "           epochs = 200\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 55.8000000477%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model2.evaluate(params2.get_test_padded(),\n",
    "                params2.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, predicting the fine grained label is a much harder problem. The training accuracy never exceeds 65%, which indicates that we may need to include a wider window, and perhaps a more complex archetecture. \n",
    "\n",
    "Further, we can already see signs of overfitting, as the test error is much higher than the training error. If we add more complex layers, it may be worth adding some dropout units in order to combat overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Adding a dense layer when predicting basic labels\n",
    "\n",
    "It might be worth adding a dense layer to the basic label classifier to learn more complex functions. If there are signs of overfitting, then I'll add some dropout units to the LSTM cells.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params3 = LstmParams(sequence_length = 10,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Add embedding and LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model3.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params3.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use a dense hidden layer of 50 nodes after the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model3.add(LSTM(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a Dense laer, and apply the softmax activation on their outputs. \n",
    "model3.add(Dense(params3.get_num_classes(), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 10, 300)           27936600  \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 28,007,106\n",
      "Trainable params: 70,506\n",
      "Non-trainable params: 27,936,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Train that model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5452/5452 [==============================] - 5s 915us/step - loss: 1.5497 - acc: 0.3443\n",
      "Epoch 2/100\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 1.0009 - acc: 0.5996\n",
      "Epoch 3/100\n",
      "5452/5452 [==============================] - 3s 618us/step - loss: 0.7937 - acc: 0.7010\n",
      "Epoch 4/100\n",
      "5452/5452 [==============================] - 3s 632us/step - loss: 0.7080 - acc: 0.7344\n",
      "Epoch 5/100\n",
      "5452/5452 [==============================] - 3s 571us/step - loss: 0.6547 - acc: 0.7537\n",
      "Epoch 6/100\n",
      "5452/5452 [==============================] - 3s 569us/step - loss: 0.6199 - acc: 0.7702\n",
      "Epoch 7/100\n",
      "5452/5452 [==============================] - 3s 625us/step - loss: 0.5894 - acc: 0.7823\n",
      "Epoch 8/100\n",
      "5452/5452 [==============================] - 3s 631us/step - loss: 0.5536 - acc: 0.7997\n",
      "Epoch 9/100\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 0.5351 - acc: 0.8074\n",
      "Epoch 10/100\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 0.5159 - acc: 0.8120\n",
      "Epoch 11/100\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 0.4980 - acc: 0.8221\n",
      "Epoch 12/100\n",
      "5452/5452 [==============================] - 3s 622us/step - loss: 0.4823 - acc: 0.8237\n",
      "Epoch 13/100\n",
      "5452/5452 [==============================] - 3s 610us/step - loss: 0.4702 - acc: 0.8302\n",
      "Epoch 14/100\n",
      "5452/5452 [==============================] - 3s 530us/step - loss: 0.4551 - acc: 0.8347\n",
      "Epoch 15/100\n",
      "5452/5452 [==============================] - 4s 701us/step - loss: 0.4476 - acc: 0.8393\n",
      "Epoch 16/100\n",
      "5452/5452 [==============================] - 4s 710us/step - loss: 0.4307 - acc: 0.8441\n",
      "Epoch 17/100\n",
      "5452/5452 [==============================] - 3s 483us/step - loss: 0.4141 - acc: 0.8501\n",
      "Epoch 18/100\n",
      "5452/5452 [==============================] - 4s 751us/step - loss: 0.3953 - acc: 0.8621\n",
      "Epoch 19/100\n",
      "5452/5452 [==============================] - 3s 562us/step - loss: 0.3993 - acc: 0.8588\n",
      "Epoch 20/100\n",
      "5452/5452 [==============================] - 3s 575us/step - loss: 0.3896 - acc: 0.8623\n",
      "Epoch 21/100\n",
      "5452/5452 [==============================] - 3s 580us/step - loss: 0.3603 - acc: 0.8716\n",
      "Epoch 22/100\n",
      "5452/5452 [==============================] - 4s 710us/step - loss: 0.3602 - acc: 0.8723\n",
      "Epoch 23/100\n",
      "5452/5452 [==============================] - 3s 634us/step - loss: 0.3419 - acc: 0.8811\n",
      "Epoch 24/100\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 0.3279 - acc: 0.8877\n",
      "Epoch 25/100\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.3246 - acc: 0.8887\n",
      "Epoch 26/100\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 0.3123 - acc: 0.8894\n",
      "Epoch 27/100\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 0.3107 - acc: 0.8877\n",
      "Epoch 28/100\n",
      "5452/5452 [==============================] - 3s 605us/step - loss: 0.2993 - acc: 0.8951\n",
      "Epoch 29/100\n",
      "5452/5452 [==============================] - 3s 617us/step - loss: 0.2743 - acc: 0.9041\n",
      "Epoch 30/100\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.2668 - acc: 0.9068\n",
      "Epoch 31/100\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.2690 - acc: 0.9052\n",
      "Epoch 32/100\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.2608 - acc: 0.9087\n",
      "Epoch 33/100\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 0.2370 - acc: 0.9175\n",
      "Epoch 34/100\n",
      "5452/5452 [==============================] - 3s 634us/step - loss: 0.2370 - acc: 0.9158\n",
      "Epoch 35/100\n",
      "5452/5452 [==============================] - 3s 603us/step - loss: 0.2304 - acc: 0.9209\n",
      "Epoch 36/100\n",
      "5452/5452 [==============================] - 3s 609us/step - loss: 0.2107 - acc: 0.9301\n",
      "Epoch 37/100\n",
      "5452/5452 [==============================] - 3s 630us/step - loss: 0.2196 - acc: 0.9272\n",
      "Epoch 38/100\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 0.2067 - acc: 0.9259\n",
      "Epoch 39/100\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.1879 - acc: 0.9371\n",
      "Epoch 40/100\n",
      "5452/5452 [==============================] - 3s 622us/step - loss: 0.1839 - acc: 0.9356\n",
      "Epoch 41/100\n",
      "5452/5452 [==============================] - 3s 632us/step - loss: 0.1678 - acc: 0.9437\n",
      "Epoch 42/100\n",
      "5452/5452 [==============================] - 3s 610us/step - loss: 0.1736 - acc: 0.9428\n",
      "Epoch 43/100\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 0.1721 - acc: 0.9431\n",
      "Epoch 44/100\n",
      "5452/5452 [==============================] - 3s 584us/step - loss: 0.1613 - acc: 0.9494\n",
      "Epoch 45/100\n",
      "5452/5452 [==============================] - 3s 630us/step - loss: 0.1588 - acc: 0.9496\n",
      "Epoch 46/100\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.1508 - acc: 0.9497\n",
      "Epoch 47/100\n",
      "5452/5452 [==============================] - 3s 617us/step - loss: 0.1335 - acc: 0.9556\n",
      "Epoch 48/100\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 0.1469 - acc: 0.9485\n",
      "Epoch 49/100\n",
      "5452/5452 [==============================] - 3s 630us/step - loss: 0.1293 - acc: 0.9589\n",
      "Epoch 50/100\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.1353 - acc: 0.9554\n",
      "Epoch 51/100\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 0.1108 - acc: 0.9639\n",
      "Epoch 52/100\n",
      "5452/5452 [==============================] - 3s 632us/step - loss: 0.1204 - acc: 0.9613\n",
      "Epoch 53/100\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.0992 - acc: 0.9692\n",
      "Epoch 54/100\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 0.0941 - acc: 0.9701\n",
      "Epoch 55/100\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 0.0988 - acc: 0.9677\n",
      "Epoch 56/100\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 0.1088 - acc: 0.9652\n",
      "Epoch 57/100\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.1117 - acc: 0.9626\n",
      "Epoch 58/100\n",
      "5452/5452 [==============================] - 4s 649us/step - loss: 0.0914 - acc: 0.9707\n",
      "Epoch 59/100\n",
      "5452/5452 [==============================] - 4s 658us/step - loss: 0.1003 - acc: 0.9659\n",
      "Epoch 60/100\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 0.1137 - acc: 0.9611\n",
      "Epoch 61/100\n",
      "5452/5452 [==============================] - 3s 630us/step - loss: 0.0753 - acc: 0.9771\n",
      "Epoch 62/100\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 0.0775 - acc: 0.9771\n",
      "Epoch 63/100\n",
      "5452/5452 [==============================] - 3s 619us/step - loss: 0.0660 - acc: 0.9795\n",
      "Epoch 64/100\n",
      "5452/5452 [==============================] - 3s 625us/step - loss: 0.0685 - acc: 0.9780\n",
      "Epoch 65/100\n",
      "5452/5452 [==============================] - 3s 610us/step - loss: 0.0656 - acc: 0.9798\n",
      "Epoch 66/100\n",
      "5452/5452 [==============================] - 3s 579us/step - loss: 0.0612 - acc: 0.9800\n",
      "Epoch 67/100\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 0.0841 - acc: 0.9712\n",
      "Epoch 68/100\n",
      "5452/5452 [==============================] - 3s 588us/step - loss: 0.0617 - acc: 0.9811\n",
      "Epoch 69/100\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 0.0693 - acc: 0.9793\n",
      "Epoch 70/100\n",
      "5452/5452 [==============================] - 3s 633us/step - loss: 0.0657 - acc: 0.9789\n",
      "Epoch 71/100\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.0612 - acc: 0.9835\n",
      "Epoch 72/100\n",
      "5452/5452 [==============================] - 3s 620us/step - loss: 0.0595 - acc: 0.9807\n",
      "Epoch 73/100\n",
      "5452/5452 [==============================] - 3s 641us/step - loss: 0.0523 - acc: 0.9844\n",
      "Epoch 74/100\n",
      "5452/5452 [==============================] - 4s 672us/step - loss: 0.0374 - acc: 0.9905\n",
      "Epoch 75/100\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 0.0308 - acc: 0.9921\n",
      "Epoch 76/100\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.0313 - acc: 0.9919\n",
      "Epoch 77/100\n",
      "5452/5452 [==============================] - 3s 597us/step - loss: 0.0321 - acc: 0.9914\n",
      "Epoch 78/100\n",
      "5452/5452 [==============================] - 3s 594us/step - loss: 0.0428 - acc: 0.9872\n",
      "Epoch 79/100\n",
      "5452/5452 [==============================] - 3s 560us/step - loss: 0.0504 - acc: 0.9850\n",
      "Epoch 80/100\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.0613 - acc: 0.9791\n",
      "Epoch 81/100\n",
      "5452/5452 [==============================] - 3s 585us/step - loss: 0.0583 - acc: 0.9837\n",
      "Epoch 82/100\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 0.0427 - acc: 0.9888\n",
      "Epoch 83/100\n",
      "5452/5452 [==============================] - 3s 565us/step - loss: 0.0296 - acc: 0.9927\n",
      "Epoch 84/100\n",
      "5452/5452 [==============================] - 3s 598us/step - loss: 0.0171 - acc: 0.9960\n",
      "Epoch 85/100\n",
      "5452/5452 [==============================] - 3s 634us/step - loss: 0.0194 - acc: 0.9954\n",
      "Epoch 86/100\n",
      "5452/5452 [==============================] - 3s 642us/step - loss: 0.0310 - acc: 0.9914\n",
      "Epoch 87/100\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 0.0442 - acc: 0.9861\n",
      "Epoch 88/100\n",
      "5452/5452 [==============================] - 4s 651us/step - loss: 0.0655 - acc: 0.9800\n",
      "Epoch 89/100\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.0372 - acc: 0.9890\n",
      "Epoch 90/100\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.0317 - acc: 0.9908\n",
      "Epoch 91/100\n",
      "5452/5452 [==============================] - 3s 632us/step - loss: 0.0130 - acc: 0.9978\n",
      "Epoch 92/100\n",
      "5452/5452 [==============================] - 4s 654us/step - loss: 0.0118 - acc: 0.9982\n",
      "Epoch 93/100\n",
      "5452/5452 [==============================] - 4s 668us/step - loss: 0.0089 - acc: 0.9982\n",
      "Epoch 94/100\n",
      "5452/5452 [==============================] - 3s 617us/step - loss: 0.0084 - acc: 0.9982\n",
      "Epoch 95/100\n",
      "5452/5452 [==============================] - 3s 632us/step - loss: 0.0065 - acc: 0.9989\n",
      "Epoch 96/100\n",
      "5452/5452 [==============================] - 3s 592us/step - loss: 0.0056 - acc: 0.9991\n",
      "Epoch 97/100\n",
      "5452/5452 [==============================] - 3s 594us/step - loss: 0.0051 - acc: 0.9993\n",
      "Epoch 98/100\n",
      "5452/5452 [==============================] - 4s 659us/step - loss: 0.0047 - acc: 0.9994\n",
      "Epoch 99/100\n",
      "5452/5452 [==============================] - 4s 644us/step - loss: 0.1608 - acc: 0.9580\n",
      "Epoch 100/100\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.0442 - acc: 0.9868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2db5c75d0>"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model3.fit(x = params3.get_train_padded(),\n",
    "           y = params3.get_train_labels_onehot(),\n",
    "           epochs = 100\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 81.4%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model3.evaluate(params3.get_test_padded(),\n",
    "                params3.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this network is overfit. WE achieve almost perfect accuracy on the training set, but the test accuracy is no better than the simple LSTM modle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Adding a dropout layer\n",
    "\n",
    "I'll re-use the same archetecutre, but this time use dropout layers between the embedding and LSTM layers, as well as between the LSTM and dense layers.\n",
    "\n",
    "Later, it might be worth using Within-cell recurrent dropout, provided via the `Keras` interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params4 = LstmParams(sequence_length = 10,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model4.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params4.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a droput layer\n",
    "model4.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model4.add(LSTM(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# another dropout before a dense layer\n",
    "model4.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a Dense layer, and apply the softmax activation on their outputs. \n",
    "model4.add(Dense(params4.get_num_classes(), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 300)           27936600  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 28,007,106\n",
      "Trainable params: 70,506\n",
      "Non-trainable params: 27,936,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5452/5452 [==============================] - 5s 918us/step - loss: 1.5409 - acc: 0.3461\n",
      "Epoch 2/100\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 1.0324 - acc: 0.5946\n",
      "Epoch 3/100\n",
      "5452/5452 [==============================] - 3s 629us/step - loss: 0.8909 - acc: 0.6588\n",
      "Epoch 4/100\n",
      "5452/5452 [==============================] - 3s 631us/step - loss: 0.8268 - acc: 0.6825\n",
      "Epoch 5/100\n",
      "5452/5452 [==============================] - 3s 641us/step - loss: 0.7804 - acc: 0.7096\n",
      "Epoch 6/100\n",
      "5452/5452 [==============================] - 3s 549us/step - loss: 0.7555 - acc: 0.7159\n",
      "Epoch 7/100\n",
      "5452/5452 [==============================] - 3s 566us/step - loss: 0.7223 - acc: 0.7280\n",
      "Epoch 8/100\n",
      "5452/5452 [==============================] - 3s 506us/step - loss: 0.6972 - acc: 0.7401\n",
      "Epoch 9/100\n",
      "5452/5452 [==============================] - 4s 653us/step - loss: 0.6808 - acc: 0.7463\n",
      "Epoch 10/100\n",
      "5452/5452 [==============================] - 3s 554us/step - loss: 0.6607 - acc: 0.7524\n",
      "Epoch 11/100\n",
      "5452/5452 [==============================] - 4s 765us/step - loss: 0.6461 - acc: 0.7550\n",
      "Epoch 12/100\n",
      "5452/5452 [==============================] - 3s 634us/step - loss: 0.6370 - acc: 0.7674\n",
      "Epoch 13/100\n",
      "5452/5452 [==============================] - 4s 715us/step - loss: 0.6148 - acc: 0.7720\n",
      "Epoch 14/100\n",
      "5452/5452 [==============================] - 4s 695us/step - loss: 0.6065 - acc: 0.7738\n",
      "Epoch 15/100\n",
      "5452/5452 [==============================] - 4s 775us/step - loss: 0.6029 - acc: 0.7748\n",
      "Epoch 16/100\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.5967 - acc: 0.7748\n",
      "Epoch 17/100\n",
      "5452/5452 [==============================] - 3s 618us/step - loss: 0.5851 - acc: 0.7788\n",
      "Epoch 18/100\n",
      "5452/5452 [==============================] - 4s 723us/step - loss: 0.5702 - acc: 0.7885\n",
      "Epoch 19/100\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 0.5605 - acc: 0.7893\n",
      "Epoch 20/100\n",
      "5452/5452 [==============================] - 3s 558us/step - loss: 0.5634 - acc: 0.7847\n",
      "Epoch 21/100\n",
      "5452/5452 [==============================] - 4s 652us/step - loss: 0.5484 - acc: 0.7937\n",
      "Epoch 22/100\n",
      "5452/5452 [==============================] - 4s 679us/step - loss: 0.5274 - acc: 0.8041\n",
      "Epoch 23/100\n",
      "5452/5452 [==============================] - 3s 560us/step - loss: 0.5277 - acc: 0.8085\n",
      "Epoch 24/100\n",
      "5452/5452 [==============================] - 3s 592us/step - loss: 0.5211 - acc: 0.8036\n",
      "Epoch 25/100\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 0.5067 - acc: 0.8105\n",
      "Epoch 26/100\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.5038 - acc: 0.8147\n",
      "Epoch 27/100\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 0.4925 - acc: 0.8122\n",
      "Epoch 28/100\n",
      "5452/5452 [==============================] - 3s 607us/step - loss: 0.4951 - acc: 0.8146\n",
      "Epoch 29/100\n",
      "5452/5452 [==============================] - 3s 543us/step - loss: 0.4823 - acc: 0.8210\n",
      "Epoch 30/100\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 0.4674 - acc: 0.8267\n",
      "Epoch 31/100\n",
      "5452/5452 [==============================] - 3s 491us/step - loss: 0.4560 - acc: 0.8331\n",
      "Epoch 32/100\n",
      "5452/5452 [==============================] - 3s 639us/step - loss: 0.4531 - acc: 0.8307\n",
      "Epoch 33/100\n",
      "5452/5452 [==============================] - 3s 620us/step - loss: 0.4537 - acc: 0.8336\n",
      "Epoch 34/100\n",
      "5452/5452 [==============================] - 3s 640us/step - loss: 0.4420 - acc: 0.8313\n",
      "Epoch 35/100\n",
      "5452/5452 [==============================] - 3s 548us/step - loss: 0.4332 - acc: 0.8375\n",
      "Epoch 36/100\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 0.4225 - acc: 0.8468\n",
      "Epoch 37/100\n",
      "5452/5452 [==============================] - 3s 610us/step - loss: 0.4102 - acc: 0.8479\n",
      "Epoch 38/100\n",
      "5452/5452 [==============================] - 4s 701us/step - loss: 0.4056 - acc: 0.8531\n",
      "Epoch 39/100\n",
      "5452/5452 [==============================] - 4s 673us/step - loss: 0.3967 - acc: 0.8577\n",
      "Epoch 40/100\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.3965 - acc: 0.8512\n",
      "Epoch 41/100\n",
      "5452/5452 [==============================] - 3s 532us/step - loss: 0.3852 - acc: 0.8553\n",
      "Epoch 42/100\n",
      "5452/5452 [==============================] - 3s 538us/step - loss: 0.3783 - acc: 0.8575\n",
      "Epoch 43/100\n",
      "5452/5452 [==============================] - 3s 526us/step - loss: 0.3665 - acc: 0.8615\n",
      "Epoch 44/100\n",
      "5452/5452 [==============================] - 3s 571us/step - loss: 0.3627 - acc: 0.8648\n",
      "Epoch 45/100\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.3615 - acc: 0.8679\n",
      "Epoch 46/100\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 0.3383 - acc: 0.8725\n",
      "Epoch 47/100\n",
      "5452/5452 [==============================] - 3s 605us/step - loss: 0.3393 - acc: 0.8788\n",
      "Epoch 48/100\n",
      "5452/5452 [==============================] - 3s 602us/step - loss: 0.3350 - acc: 0.8734\n",
      "Epoch 49/100\n",
      "5452/5452 [==============================] - 4s 644us/step - loss: 0.3259 - acc: 0.8764\n",
      "Epoch 50/100\n",
      "5452/5452 [==============================] - 3s 603us/step - loss: 0.3212 - acc: 0.8799\n",
      "Epoch 51/100\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 0.3263 - acc: 0.8802\n",
      "Epoch 52/100\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 0.3124 - acc: 0.8870\n",
      "Epoch 53/100\n",
      "5452/5452 [==============================] - 3s 548us/step - loss: 0.3023 - acc: 0.8903\n",
      "Epoch 54/100\n",
      "5452/5452 [==============================] - 3s 572us/step - loss: 0.3079 - acc: 0.8914\n",
      "Epoch 55/100\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 0.2909 - acc: 0.8944\n",
      "Epoch 56/100\n",
      "5452/5452 [==============================] - 4s 693us/step - loss: 0.2889 - acc: 0.8931\n",
      "Epoch 57/100\n",
      "5452/5452 [==============================] - 4s 664us/step - loss: 0.2753 - acc: 0.8944\n",
      "Epoch 58/100\n",
      "5452/5452 [==============================] - 3s 585us/step - loss: 0.2679 - acc: 0.9010\n",
      "Epoch 59/100\n",
      "5452/5452 [==============================] - 3s 549us/step - loss: 0.2668 - acc: 0.9061\n",
      "Epoch 60/100\n",
      "5452/5452 [==============================] - 3s 534us/step - loss: 0.2642 - acc: 0.9006\n",
      "Epoch 61/100\n",
      "5452/5452 [==============================] - 3s 563us/step - loss: 0.2582 - acc: 0.9077\n",
      "Epoch 62/100\n",
      "5452/5452 [==============================] - 3s 597us/step - loss: 0.2465 - acc: 0.9105\n",
      "Epoch 63/100\n",
      "5452/5452 [==============================] - 3s 548us/step - loss: 0.2450 - acc: 0.9125\n",
      "Epoch 64/100\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.2370 - acc: 0.9171\n",
      "Epoch 65/100\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 0.2303 - acc: 0.9167\n",
      "Epoch 66/100\n",
      "5452/5452 [==============================] - 3s 524us/step - loss: 0.2263 - acc: 0.9204\n",
      "Epoch 67/100\n",
      "5452/5452 [==============================] - 3s 572us/step - loss: 0.2387 - acc: 0.9132\n",
      "Epoch 68/100\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.2163 - acc: 0.9197\n",
      "Epoch 69/100\n",
      "5452/5452 [==============================] - 3s 564us/step - loss: 0.2165 - acc: 0.9204\n",
      "Epoch 70/100\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 0.2059 - acc: 0.9255\n",
      "Epoch 71/100\n",
      "5452/5452 [==============================] - 3s 594us/step - loss: 0.2000 - acc: 0.9296\n",
      "Epoch 72/100\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 0.2110 - acc: 0.9261\n",
      "Epoch 73/100\n",
      "5452/5452 [==============================] - 3s 568us/step - loss: 0.2044 - acc: 0.9281\n",
      "Epoch 74/100\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 0.1869 - acc: 0.9288\n",
      "Epoch 75/100\n",
      "5452/5452 [==============================] - 3s 586us/step - loss: 0.1829 - acc: 0.9367\n",
      "Epoch 76/100\n",
      "5452/5452 [==============================] - 3s 552us/step - loss: 0.1792 - acc: 0.9362\n",
      "Epoch 77/100\n",
      "5452/5452 [==============================] - 3s 571us/step - loss: 0.1780 - acc: 0.9386\n",
      "Epoch 78/100\n",
      "5452/5452 [==============================] - 3s 583us/step - loss: 0.1695 - acc: 0.9365\n",
      "Epoch 79/100\n",
      "5452/5452 [==============================] - 3s 564us/step - loss: 0.1735 - acc: 0.9375\n",
      "Epoch 80/100\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 0.1640 - acc: 0.9417\n",
      "Epoch 81/100\n",
      "5452/5452 [==============================] - 3s 552us/step - loss: 0.1692 - acc: 0.9406\n",
      "Epoch 82/100\n",
      "5452/5452 [==============================] - 3s 569us/step - loss: 0.1553 - acc: 0.9433\n",
      "Epoch 83/100\n",
      "5452/5452 [==============================] - 3s 543us/step - loss: 0.1642 - acc: 0.9435\n",
      "Epoch 84/100\n",
      "5452/5452 [==============================] - 3s 585us/step - loss: 0.1624 - acc: 0.9431\n",
      "Epoch 85/100\n",
      "5452/5452 [==============================] - 3s 520us/step - loss: 0.1526 - acc: 0.9448\n",
      "Epoch 86/100\n",
      "5452/5452 [==============================] - 3s 603us/step - loss: 0.1539 - acc: 0.9424\n",
      "Epoch 87/100\n",
      "5452/5452 [==============================] - 3s 564us/step - loss: 0.1419 - acc: 0.9486\n",
      "Epoch 88/100\n",
      "5452/5452 [==============================] - 3s 566us/step - loss: 0.1533 - acc: 0.9450\n",
      "Epoch 89/100\n",
      "5452/5452 [==============================] - 3s 571us/step - loss: 0.1381 - acc: 0.9507\n",
      "Epoch 90/100\n",
      "5452/5452 [==============================] - 3s 564us/step - loss: 0.1412 - acc: 0.9503\n",
      "Epoch 91/100\n",
      "5452/5452 [==============================] - 3s 553us/step - loss: 0.1419 - acc: 0.9486\n",
      "Epoch 92/100\n",
      "5452/5452 [==============================] - 3s 555us/step - loss: 0.1290 - acc: 0.9540\n",
      "Epoch 93/100\n",
      "5452/5452 [==============================] - 3s 564us/step - loss: 0.1282 - acc: 0.9536\n",
      "Epoch 94/100\n",
      "5452/5452 [==============================] - 3s 576us/step - loss: 0.1292 - acc: 0.9512\n",
      "Epoch 95/100\n",
      "5452/5452 [==============================] - 3s 561us/step - loss: 0.1179 - acc: 0.9591\n",
      "Epoch 96/100\n",
      "5452/5452 [==============================] - 3s 579us/step - loss: 0.1234 - acc: 0.9560\n",
      "Epoch 97/100\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 0.1266 - acc: 0.9562\n",
      "Epoch 98/100\n",
      "5452/5452 [==============================] - 3s 584us/step - loss: 0.1156 - acc: 0.9604\n",
      "Epoch 99/100\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 0.1202 - acc: 0.9574\n",
      "Epoch 100/100\n",
      "5452/5452 [==============================] - 3s 574us/step - loss: 0.1085 - acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23afb0550>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model4.fit(x = params4.get_train_padded(),\n",
    "           y = params4.get_train_labels_onehot(),\n",
    "           epochs = 100\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 86.0000000954%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model4.evaluate(params4.get_test_padded(),\n",
    "                params4.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Trying dropout and recurrent dropout within the LSTM cells\n",
    "\n",
    "This will mask some of the time-specific idiosyncracies in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params5 = LstmParams(sequence_length = 10,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model5 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model5.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params5.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model5.add(LSTM(50,dropout=0.2, recurrent_dropout=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a Dense layer, and apply the softmax activation on their outputs. \n",
    "model5.add(Dense(params5.get_num_classes(), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model5.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 300)           27936600  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 28,007,106\n",
      "Trainable params: 70,506\n",
      "Non-trainable params: 27,936,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "5452/5452 [==============================] - 4s 800us/step - loss: 1.5877 - acc: 0.3184\n",
      "Epoch 2/150\n",
      "5452/5452 [==============================] - 4s 669us/step - loss: 1.1298 - acc: 0.5398\n",
      "Epoch 3/150\n",
      "5452/5452 [==============================] - 3s 629us/step - loss: 0.9401 - acc: 0.6392\n",
      "Epoch 4/150\n",
      "5452/5452 [==============================] - 4s 769us/step - loss: 0.8396 - acc: 0.6843\n",
      "Epoch 5/150\n",
      "5452/5452 [==============================] - 4s 682us/step - loss: 0.8055 - acc: 0.7018\n",
      "Epoch 6/150\n",
      "5452/5452 [==============================] - 3s 641us/step - loss: 0.7623 - acc: 0.7128\n",
      "Epoch 7/150\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.7297 - acc: 0.7282\n",
      "Epoch 8/150\n",
      "5452/5452 [==============================] - 4s 695us/step - loss: 0.7236 - acc: 0.7342\n",
      "Epoch 9/150\n",
      "5452/5452 [==============================] - 4s 667us/step - loss: 0.7037 - acc: 0.7368\n",
      "Epoch 10/150\n",
      "5452/5452 [==============================] - 4s 666us/step - loss: 0.6905 - acc: 0.7460\n",
      "Epoch 11/150\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.6778 - acc: 0.7465\n",
      "Epoch 12/150\n",
      "5452/5452 [==============================] - 4s 712us/step - loss: 0.6770 - acc: 0.7476\n",
      "Epoch 13/150\n",
      "5452/5452 [==============================] - 4s 662us/step - loss: 0.6497 - acc: 0.7584\n",
      "Epoch 14/150\n",
      "5452/5452 [==============================] - 4s 745us/step - loss: 0.6313 - acc: 0.7665\n",
      "Epoch 15/150\n",
      "5452/5452 [==============================] - 3s 544us/step - loss: 0.6251 - acc: 0.7676\n",
      "Epoch 16/150\n",
      "5452/5452 [==============================] - 4s 663us/step - loss: 0.6306 - acc: 0.7656\n",
      "Epoch 17/150\n",
      "5452/5452 [==============================] - 3s 618us/step - loss: 0.6184 - acc: 0.7724\n",
      "Epoch 18/150\n",
      "5452/5452 [==============================] - 4s 655us/step - loss: 0.6027 - acc: 0.7801\n",
      "Epoch 19/150\n",
      "5452/5452 [==============================] - 4s 655us/step - loss: 0.5963 - acc: 0.7821\n",
      "Epoch 20/150\n",
      "5452/5452 [==============================] - 3s 626us/step - loss: 0.5915 - acc: 0.7828\n",
      "Epoch 21/150\n",
      "5452/5452 [==============================] - 4s 657us/step - loss: 0.5714 - acc: 0.7887\n",
      "Epoch 22/150\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.5669 - acc: 0.7907\n",
      "Epoch 23/150\n",
      "5452/5452 [==============================] - 3s 619us/step - loss: 0.5699 - acc: 0.7920\n",
      "Epoch 24/150\n",
      "5452/5452 [==============================] - 3s 621us/step - loss: 0.5635 - acc: 0.7929\n",
      "Epoch 25/150\n",
      "5452/5452 [==============================] - 4s 679us/step - loss: 0.5444 - acc: 0.8048\n",
      "Epoch 26/150\n",
      "5452/5452 [==============================] - 4s 708us/step - loss: 0.5398 - acc: 0.8030\n",
      "Epoch 27/150\n",
      "5452/5452 [==============================] - 3s 608us/step - loss: 0.5289 - acc: 0.8072\n",
      "Epoch 28/150\n",
      "5452/5452 [==============================] - 4s 649us/step - loss: 0.5275 - acc: 0.8063\n",
      "Epoch 29/150\n",
      "5452/5452 [==============================] - 4s 644us/step - loss: 0.5142 - acc: 0.8092\n",
      "Epoch 30/150\n",
      "5452/5452 [==============================] - 3s 623us/step - loss: 0.5167 - acc: 0.8105\n",
      "Epoch 31/150\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.5047 - acc: 0.8153\n",
      "Epoch 32/150\n",
      "5452/5452 [==============================] - 4s 663us/step - loss: 0.5087 - acc: 0.8142\n",
      "Epoch 33/150\n",
      "5452/5452 [==============================] - 4s 644us/step - loss: 0.4967 - acc: 0.8160\n",
      "Epoch 34/150\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.4791 - acc: 0.8263\n",
      "Epoch 35/150\n",
      "5452/5452 [==============================] - 3s 535us/step - loss: 0.4733 - acc: 0.8294\n",
      "Epoch 36/150\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 0.4786 - acc: 0.8267\n",
      "Epoch 37/150\n",
      "5452/5452 [==============================] - 3s 630us/step - loss: 0.4648 - acc: 0.8274\n",
      "Epoch 38/150\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 0.4621 - acc: 0.8292\n",
      "Epoch 39/150\n",
      "5452/5452 [==============================] - 4s 651us/step - loss: 0.4494 - acc: 0.8377\n",
      "Epoch 40/150\n",
      "5452/5452 [==============================] - 4s 645us/step - loss: 0.4456 - acc: 0.8368\n",
      "Epoch 41/150\n",
      "5452/5452 [==============================] - 3s 627us/step - loss: 0.4518 - acc: 0.8340\n",
      "Epoch 42/150\n",
      "5452/5452 [==============================] - 3s 620us/step - loss: 0.4381 - acc: 0.8362\n",
      "Epoch 43/150\n",
      "5452/5452 [==============================] - 4s 736us/step - loss: 0.4269 - acc: 0.8412\n",
      "Epoch 44/150\n",
      "5452/5452 [==============================] - 4s 654us/step - loss: 0.4161 - acc: 0.8476\n",
      "Epoch 45/150\n",
      "5452/5452 [==============================] - 4s 741us/step - loss: 0.4087 - acc: 0.8470\n",
      "Epoch 46/150\n",
      "5452/5452 [==============================] - 4s 655us/step - loss: 0.4034 - acc: 0.8553\n",
      "Epoch 47/150\n",
      "5452/5452 [==============================] - 3s 625us/step - loss: 0.4000 - acc: 0.8498\n",
      "Epoch 48/150\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 0.3949 - acc: 0.8586\n",
      "Epoch 49/150\n",
      "5452/5452 [==============================] - 3s 591us/step - loss: 0.3987 - acc: 0.8496\n",
      "Epoch 50/150\n",
      "5452/5452 [==============================] - 4s 647us/step - loss: 0.3827 - acc: 0.8523\n",
      "Epoch 51/150\n",
      "5452/5452 [==============================] - 3s 603us/step - loss: 0.3812 - acc: 0.8621\n",
      "Epoch 52/150\n",
      "5452/5452 [==============================] - 3s 617us/step - loss: 0.3724 - acc: 0.8604\n",
      "Epoch 53/150\n",
      "5452/5452 [==============================] - 4s 644us/step - loss: 0.3628 - acc: 0.8679\n",
      "Epoch 54/150\n",
      "5452/5452 [==============================] - 4s 648us/step - loss: 0.3611 - acc: 0.8679\n",
      "Epoch 55/150\n",
      "5452/5452 [==============================] - 4s 645us/step - loss: 0.3530 - acc: 0.8727\n",
      "Epoch 56/150\n",
      "5452/5452 [==============================] - 4s 665us/step - loss: 0.3515 - acc: 0.8738\n",
      "Epoch 57/150\n",
      "5452/5452 [==============================] - 3s 632us/step - loss: 0.3496 - acc: 0.8716\n",
      "Epoch 58/150\n",
      "5452/5452 [==============================] - 3s 637us/step - loss: 0.3354 - acc: 0.8773\n",
      "Epoch 59/150\n",
      "5452/5452 [==============================] - 4s 786us/step - loss: 0.3321 - acc: 0.8806\n",
      "Epoch 60/150\n",
      "5452/5452 [==============================] - 4s 660us/step - loss: 0.3288 - acc: 0.8788\n",
      "Epoch 61/150\n",
      "5452/5452 [==============================] - 4s 812us/step - loss: 0.3239 - acc: 0.8808\n",
      "Epoch 62/150\n",
      "5452/5452 [==============================] - 4s 677us/step - loss: 0.3115 - acc: 0.8855\n",
      "Epoch 63/150\n",
      "5452/5452 [==============================] - 4s 707us/step - loss: 0.3183 - acc: 0.8828\n",
      "Epoch 64/150\n",
      "5452/5452 [==============================] - 3s 617us/step - loss: 0.3119 - acc: 0.8844\n",
      "Epoch 65/150\n",
      "5452/5452 [==============================] - 3s 609us/step - loss: 0.2984 - acc: 0.8944\n",
      "Epoch 66/150\n",
      "5452/5452 [==============================] - 4s 678us/step - loss: 0.3024 - acc: 0.8912\n",
      "Epoch 67/150\n",
      "5452/5452 [==============================] - 4s 652us/step - loss: 0.2939 - acc: 0.8909\n",
      "Epoch 68/150\n",
      "5452/5452 [==============================] - 3s 640us/step - loss: 0.2858 - acc: 0.8988\n",
      "Epoch 69/150\n",
      "5452/5452 [==============================] - 3s 573us/step - loss: 0.2842 - acc: 0.8986\n",
      "Epoch 70/150\n",
      "5452/5452 [==============================] - 3s 598us/step - loss: 0.2750 - acc: 0.9035\n",
      "Epoch 71/150\n",
      "5452/5452 [==============================] - 3s 622us/step - loss: 0.2796 - acc: 0.8975\n",
      "Epoch 72/150\n",
      "5452/5452 [==============================] - 3s 618us/step - loss: 0.2721 - acc: 0.9021\n",
      "Epoch 73/150\n",
      "5452/5452 [==============================] - 3s 595us/step - loss: 0.2688 - acc: 0.9026\n",
      "Epoch 74/150\n",
      "5452/5452 [==============================] - 4s 657us/step - loss: 0.2631 - acc: 0.9039\n",
      "Epoch 75/150\n",
      "5452/5452 [==============================] - 3s 619us/step - loss: 0.2615 - acc: 0.9050\n",
      "Epoch 76/150\n",
      "5452/5452 [==============================] - 3s 619us/step - loss: 0.2579 - acc: 0.9088\n",
      "Epoch 77/150\n",
      "5452/5452 [==============================] - 3s 633us/step - loss: 0.2554 - acc: 0.9054\n",
      "Epoch 78/150\n",
      "5452/5452 [==============================] - 4s 779us/step - loss: 0.2442 - acc: 0.9140\n",
      "Epoch 79/150\n",
      "5452/5452 [==============================] - 4s 674us/step - loss: 0.2387 - acc: 0.9160\n",
      "Epoch 80/150\n",
      "5452/5452 [==============================] - 4s 747us/step - loss: 0.2342 - acc: 0.9145\n",
      "Epoch 81/150\n",
      "5452/5452 [==============================] - 5s 934us/step - loss: 0.2431 - acc: 0.9143\n",
      "Epoch 82/150\n",
      "5452/5452 [==============================] - 4s 652us/step - loss: 0.2305 - acc: 0.9197\n",
      "Epoch 83/150\n",
      "5452/5452 [==============================] - 4s 679us/step - loss: 0.2272 - acc: 0.9208\n",
      "Epoch 84/150\n",
      "5452/5452 [==============================] - 4s 735us/step - loss: 0.2277 - acc: 0.9182\n",
      "Epoch 85/150\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 0.2298 - acc: 0.9202\n",
      "Epoch 86/150\n",
      "5452/5452 [==============================] - 4s 644us/step - loss: 0.2260 - acc: 0.9200\n",
      "Epoch 87/150\n",
      "5452/5452 [==============================] - 4s 778us/step - loss: 0.2150 - acc: 0.9222\n",
      "Epoch 88/150\n",
      "5452/5452 [==============================] - 3s 587us/step - loss: 0.2124 - acc: 0.9241\n",
      "Epoch 89/150\n",
      "5452/5452 [==============================] - 3s 626us/step - loss: 0.2111 - acc: 0.9281\n",
      "Epoch 90/150\n",
      "5452/5452 [==============================] - 3s 585us/step - loss: 0.2069 - acc: 0.9290\n",
      "Epoch 91/150\n",
      "5452/5452 [==============================] - 3s 579us/step - loss: 0.2013 - acc: 0.9272\n",
      "Epoch 92/150\n",
      "5452/5452 [==============================] - 3s 596us/step - loss: 0.2045 - acc: 0.9266\n",
      "Epoch 93/150\n",
      "5452/5452 [==============================] - 4s 781us/step - loss: 0.1939 - acc: 0.9338\n",
      "Epoch 94/150\n",
      "5452/5452 [==============================] - 4s 707us/step - loss: 0.1925 - acc: 0.9298\n",
      "Epoch 95/150\n",
      "5452/5452 [==============================] - 4s 723us/step - loss: 0.1869 - acc: 0.9349\n",
      "Epoch 96/150\n",
      "5452/5452 [==============================] - 4s 704us/step - loss: 0.1865 - acc: 0.9332\n",
      "Epoch 97/150\n",
      "5452/5452 [==============================] - 4s 700us/step - loss: 0.1888 - acc: 0.9307\n",
      "Epoch 98/150\n",
      "5452/5452 [==============================] - 4s 668us/step - loss: 0.1842 - acc: 0.9314\n",
      "Epoch 99/150\n",
      "5452/5452 [==============================] - 4s 670us/step - loss: 0.1833 - acc: 0.9358\n",
      "Epoch 100/150\n",
      "5452/5452 [==============================] - 4s 681us/step - loss: 0.1770 - acc: 0.9345\n",
      "Epoch 101/150\n",
      "5452/5452 [==============================] - 4s 651us/step - loss: 0.1676 - acc: 0.9435\n",
      "Epoch 102/150\n",
      "5452/5452 [==============================] - 4s 643us/step - loss: 0.1605 - acc: 0.9426\n",
      "Epoch 103/150\n",
      "5452/5452 [==============================] - 4s 664us/step - loss: 0.1705 - acc: 0.9387\n",
      "Epoch 104/150\n",
      "5452/5452 [==============================] - 3s 606us/step - loss: 0.1646 - acc: 0.9400\n",
      "Epoch 105/150\n",
      "5452/5452 [==============================] - 3s 590us/step - loss: 0.1602 - acc: 0.9415\n",
      "Epoch 106/150\n",
      "5452/5452 [==============================] - 3s 618us/step - loss: 0.1620 - acc: 0.9461\n",
      "Epoch 107/150\n",
      "5452/5452 [==============================] - 3s 588us/step - loss: 0.1633 - acc: 0.9444\n",
      "Epoch 108/150\n",
      "5452/5452 [==============================] - 3s 593us/step - loss: 0.1527 - acc: 0.9455\n",
      "Epoch 109/150\n",
      "5452/5452 [==============================] - 3s 572us/step - loss: 0.1581 - acc: 0.9464\n",
      "Epoch 110/150\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 0.1505 - acc: 0.9486\n",
      "Epoch 111/150\n",
      "5452/5452 [==============================] - 3s 557us/step - loss: 0.1420 - acc: 0.9519\n",
      "Epoch 112/150\n",
      "5452/5452 [==============================] - 4s 719us/step - loss: 0.1363 - acc: 0.9530\n",
      "Epoch 113/150\n",
      "5452/5452 [==============================] - 4s 724us/step - loss: 0.1452 - acc: 0.9514\n",
      "Epoch 114/150\n",
      "5452/5452 [==============================] - 4s 657us/step - loss: 0.1348 - acc: 0.9556\n",
      "Epoch 115/150\n",
      "5452/5452 [==============================] - 4s 642us/step - loss: 0.1314 - acc: 0.9556\n",
      "Epoch 116/150\n",
      "5452/5452 [==============================] - 3s 613us/step - loss: 0.1432 - acc: 0.9477\n",
      "Epoch 117/150\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.1367 - acc: 0.9529\n",
      "Epoch 118/150\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 0.1455 - acc: 0.9444\n",
      "Epoch 119/150\n",
      "5452/5452 [==============================] - 3s 580us/step - loss: 0.1289 - acc: 0.9562\n",
      "Epoch 120/150\n",
      "5452/5452 [==============================] - 3s 628us/step - loss: 0.1421 - acc: 0.9516\n",
      "Epoch 121/150\n",
      "5452/5452 [==============================] - 3s 618us/step - loss: 0.1318 - acc: 0.9545\n",
      "Epoch 122/150\n",
      "5452/5452 [==============================] - 3s 631us/step - loss: 0.1276 - acc: 0.9580\n",
      "Epoch 123/150\n",
      "5452/5452 [==============================] - 3s 609us/step - loss: 0.1259 - acc: 0.9574\n",
      "Epoch 124/150\n",
      "5452/5452 [==============================] - 3s 589us/step - loss: 0.1247 - acc: 0.9573\n",
      "Epoch 125/150\n",
      "5452/5452 [==============================] - 3s 588us/step - loss: 0.1226 - acc: 0.9589\n",
      "Epoch 126/150\n",
      "5452/5452 [==============================] - 3s 639us/step - loss: 0.1225 - acc: 0.9569\n",
      "Epoch 127/150\n",
      "5452/5452 [==============================] - 3s 599us/step - loss: 0.1143 - acc: 0.9617\n",
      "Epoch 128/150\n",
      "5452/5452 [==============================] - 3s 564us/step - loss: 0.1065 - acc: 0.9626\n",
      "Epoch 129/150\n",
      "5452/5452 [==============================] - 3s 610us/step - loss: 0.1181 - acc: 0.9576\n",
      "Epoch 130/150\n",
      "5452/5452 [==============================] - 3s 611us/step - loss: 0.1119 - acc: 0.9629\n",
      "Epoch 131/150\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.1045 - acc: 0.9652\n",
      "Epoch 132/150\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.1056 - acc: 0.9631\n",
      "Epoch 133/150\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.1107 - acc: 0.9604\n",
      "Epoch 134/150\n",
      "5452/5452 [==============================] - 3s 588us/step - loss: 0.1103 - acc: 0.9595\n",
      "Epoch 135/150\n",
      "5452/5452 [==============================] - 3s 605us/step - loss: 0.1079 - acc: 0.9628\n",
      "Epoch 136/150\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.1130 - acc: 0.9604\n",
      "Epoch 137/150\n",
      "5452/5452 [==============================] - 3s 594us/step - loss: 0.1037 - acc: 0.9666\n",
      "Epoch 138/150\n",
      "5452/5452 [==============================] - 3s 629us/step - loss: 0.0981 - acc: 0.9679\n",
      "Epoch 139/150\n",
      "5452/5452 [==============================] - 3s 580us/step - loss: 0.1011 - acc: 0.9635\n",
      "Epoch 140/150\n",
      "5452/5452 [==============================] - 3s 605us/step - loss: 0.0992 - acc: 0.9657\n",
      "Epoch 141/150\n",
      "5452/5452 [==============================] - 3s 601us/step - loss: 0.1005 - acc: 0.9675\n",
      "Epoch 142/150\n",
      "5452/5452 [==============================] - 3s 602us/step - loss: 0.1004 - acc: 0.9663\n",
      "Epoch 143/150\n",
      "5452/5452 [==============================] - 3s 616us/step - loss: 0.0893 - acc: 0.9718\n",
      "Epoch 144/150\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.0918 - acc: 0.9714\n",
      "Epoch 145/150\n",
      "5452/5452 [==============================] - 3s 615us/step - loss: 0.0988 - acc: 0.9668\n",
      "Epoch 146/150\n",
      "5452/5452 [==============================] - 3s 600us/step - loss: 0.0773 - acc: 0.9756\n",
      "Epoch 147/150\n",
      "5452/5452 [==============================] - 3s 603us/step - loss: 0.0842 - acc: 0.9710\n",
      "Epoch 148/150\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.0910 - acc: 0.9703\n",
      "Epoch 149/150\n",
      "5452/5452 [==============================] - 3s 614us/step - loss: 0.0915 - acc: 0.9688\n",
      "Epoch 150/150\n",
      "5452/5452 [==============================] - 3s 612us/step - loss: 0.0869 - acc: 0.9705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x249310b90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model5.fit(x = params5.get_train_padded(),\n",
    "           y = params5.get_train_labels_onehot(),\n",
    "           epochs = 150\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 89.6000000954%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model5.evaluate(params5.get_test_padded(),\n",
    "                params5.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no telling whether this increase in performance really means that using internal dropout leads to a better classifier - I'm certainly overfitting to the test data by tuning my hyperparameters according to the test error. \n",
    "\n",
    "It does show, however, that internal droput seems to combat overfitting reasonably effectively. Still not enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 expanding the word context window. \n",
    "\n",
    "So far, I've been working with a context window of 10 words. Perhaps 15 will be better? I'll try it with my two most trecent model configurations, `model4` and `model5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params6 = LstmParams(sequence_length = 15,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model6 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model6.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params6.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a dropout layer\n",
    "model6.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model6.add(LSTM(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model6.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a Dense layer, and apply the softmax activation on their outputs. \n",
    "model6.add(Dense(params6.get_num_classes(), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model6.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 15, 300)           27936600  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 28,007,106\n",
      "Trainable params: 70,506\n",
      "Non-trainable params: 27,936,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5452/5452 [==============================] - 10s 2ms/step - loss: 1.6449 - acc: 0.2742\n",
      "Epoch 2/100\n",
      "5452/5452 [==============================] - 5s 846us/step - loss: 1.2574 - acc: 0.4545\n",
      "Epoch 3/100\n",
      "5452/5452 [==============================] - 4s 779us/step - loss: 1.1335 - acc: 0.5216\n",
      "Epoch 4/100\n",
      "5452/5452 [==============================] - 4s 802us/step - loss: 1.0463 - acc: 0.5743\n",
      "Epoch 5/100\n",
      "5452/5452 [==============================] - 4s 730us/step - loss: 0.9044 - acc: 0.6554\n",
      "Epoch 6/100\n",
      "5452/5452 [==============================] - 4s 745us/step - loss: 0.8194 - acc: 0.6911\n",
      "Epoch 7/100\n",
      "5452/5452 [==============================] - 4s 799us/step - loss: 0.7748 - acc: 0.7133\n",
      "Epoch 8/100\n",
      "5452/5452 [==============================] - 5s 840us/step - loss: 0.7357 - acc: 0.7197\n",
      "Epoch 9/100\n",
      "5452/5452 [==============================] - 5s 832us/step - loss: 0.7207 - acc: 0.7337\n",
      "Epoch 10/100\n",
      "5452/5452 [==============================] - 5s 843us/step - loss: 0.7001 - acc: 0.7383\n",
      "Epoch 11/100\n",
      "5452/5452 [==============================] - 4s 750us/step - loss: 0.6882 - acc: 0.7469\n",
      "Epoch 12/100\n",
      "5452/5452 [==============================] - 4s 720us/step - loss: 0.6606 - acc: 0.7592\n",
      "Epoch 13/100\n",
      "5452/5452 [==============================] - 4s 823us/step - loss: 0.6519 - acc: 0.7590\n",
      "Epoch 14/100\n",
      "5452/5452 [==============================] - 4s 792us/step - loss: 0.6462 - acc: 0.7645\n",
      "Epoch 15/100\n",
      "5452/5452 [==============================] - 4s 792us/step - loss: 0.6348 - acc: 0.7674\n",
      "Epoch 16/100\n",
      "5452/5452 [==============================] - 4s 822us/step - loss: 0.6255 - acc: 0.7748\n",
      "Epoch 17/100\n",
      "5452/5452 [==============================] - 4s 794us/step - loss: 0.6117 - acc: 0.7841\n",
      "Epoch 18/100\n",
      "5452/5452 [==============================] - 4s 783us/step - loss: 0.5868 - acc: 0.7909\n",
      "Epoch 19/100\n",
      "5452/5452 [==============================] - 4s 819us/step - loss: 0.5951 - acc: 0.7843\n",
      "Epoch 20/100\n",
      "5452/5452 [==============================] - 4s 799us/step - loss: 0.5742 - acc: 0.7905\n",
      "Epoch 21/100\n",
      "5452/5452 [==============================] - 4s 784us/step - loss: 0.5708 - acc: 0.7970\n",
      "Epoch 22/100\n",
      "5452/5452 [==============================] - 5s 833us/step - loss: 0.5685 - acc: 0.7948\n",
      "Epoch 23/100\n",
      "5452/5452 [==============================] - 4s 825us/step - loss: 0.5564 - acc: 0.8021\n",
      "Epoch 24/100\n",
      "5452/5452 [==============================] - 4s 819us/step - loss: 0.5498 - acc: 0.8045\n",
      "Epoch 25/100\n",
      "5452/5452 [==============================] - 4s 771us/step - loss: 0.5385 - acc: 0.8043\n",
      "Epoch 26/100\n",
      "5452/5452 [==============================] - 5s 831us/step - loss: 0.5263 - acc: 0.8091\n",
      "Epoch 27/100\n",
      "5452/5452 [==============================] - 4s 774us/step - loss: 0.5277 - acc: 0.8147\n",
      "Epoch 28/100\n",
      "5452/5452 [==============================] - 4s 822us/step - loss: 0.5126 - acc: 0.8164\n",
      "Epoch 29/100\n",
      "5452/5452 [==============================] - 4s 766us/step - loss: 0.5033 - acc: 0.8190\n",
      "Epoch 30/100\n",
      "5452/5452 [==============================] - 4s 802us/step - loss: 0.5092 - acc: 0.8136\n",
      "Epoch 31/100\n",
      "5452/5452 [==============================] - 4s 825us/step - loss: 0.4868 - acc: 0.8269\n",
      "Epoch 32/100\n",
      "5452/5452 [==============================] - 4s 719us/step - loss: 0.4813 - acc: 0.8303\n",
      "Epoch 33/100\n",
      "5452/5452 [==============================] - 4s 820us/step - loss: 0.4804 - acc: 0.8228\n",
      "Epoch 34/100\n",
      "5452/5452 [==============================] - 4s 763us/step - loss: 0.4663 - acc: 0.8336\n",
      "Epoch 35/100\n",
      "5452/5452 [==============================] - 4s 765us/step - loss: 0.4729 - acc: 0.8333\n",
      "Epoch 36/100\n",
      "5452/5452 [==============================] - 5s 844us/step - loss: 0.4571 - acc: 0.8391\n",
      "Epoch 37/100\n",
      "5452/5452 [==============================] - 4s 799us/step - loss: 0.4577 - acc: 0.8415\n",
      "Epoch 38/100\n",
      "5452/5452 [==============================] - 5s 841us/step - loss: 0.4362 - acc: 0.8489\n",
      "Epoch 39/100\n",
      "5452/5452 [==============================] - 5s 834us/step - loss: 0.4319 - acc: 0.8479\n",
      "Epoch 40/100\n",
      "5452/5452 [==============================] - 4s 748us/step - loss: 0.4281 - acc: 0.8505\n",
      "Epoch 41/100\n",
      "5452/5452 [==============================] - 4s 742us/step - loss: 0.4234 - acc: 0.8538\n",
      "Epoch 42/100\n",
      "5452/5452 [==============================] - 4s 759us/step - loss: 0.4192 - acc: 0.8496\n",
      "Epoch 43/100\n",
      "5452/5452 [==============================] - 4s 764us/step - loss: 0.4163 - acc: 0.8538\n",
      "Epoch 44/100\n",
      "5452/5452 [==============================] - 4s 783us/step - loss: 0.3952 - acc: 0.8602\n",
      "Epoch 45/100\n",
      "5452/5452 [==============================] - 4s 766us/step - loss: 0.3918 - acc: 0.8602\n",
      "Epoch 46/100\n",
      "5452/5452 [==============================] - 4s 786us/step - loss: 0.3864 - acc: 0.8637\n",
      "Epoch 47/100\n",
      "5452/5452 [==============================] - 4s 759us/step - loss: 0.3813 - acc: 0.8637\n",
      "Epoch 48/100\n",
      "5452/5452 [==============================] - 4s 762us/step - loss: 0.3705 - acc: 0.8718\n",
      "Epoch 49/100\n",
      "5452/5452 [==============================] - 4s 814us/step - loss: 0.3698 - acc: 0.8670\n",
      "Epoch 50/100\n",
      "5452/5452 [==============================] - 4s 788us/step - loss: 0.3696 - acc: 0.8705\n",
      "Epoch 51/100\n",
      "5452/5452 [==============================] - 4s 767us/step - loss: 0.3512 - acc: 0.8740\n",
      "Epoch 52/100\n",
      "5452/5452 [==============================] - 4s 809us/step - loss: 0.3417 - acc: 0.8854\n",
      "Epoch 53/100\n",
      "5452/5452 [==============================] - 4s 774us/step - loss: 0.3336 - acc: 0.8866\n",
      "Epoch 54/100\n",
      "5452/5452 [==============================] - 4s 794us/step - loss: 0.3230 - acc: 0.8857\n",
      "Epoch 55/100\n",
      "5452/5452 [==============================] - 4s 735us/step - loss: 0.3346 - acc: 0.8835\n",
      "Epoch 56/100\n",
      "5452/5452 [==============================] - 4s 780us/step - loss: 0.3262 - acc: 0.8901\n",
      "Epoch 57/100\n",
      "5452/5452 [==============================] - 4s 759us/step - loss: 0.2995 - acc: 0.8964\n",
      "Epoch 58/100\n",
      "5452/5452 [==============================] - 4s 762us/step - loss: 0.3078 - acc: 0.8912\n",
      "Epoch 59/100\n",
      "5452/5452 [==============================] - 4s 772us/step - loss: 0.3091 - acc: 0.8901\n",
      "Epoch 60/100\n",
      "5452/5452 [==============================] - 4s 740us/step - loss: 0.3012 - acc: 0.8903\n",
      "Epoch 61/100\n",
      "5452/5452 [==============================] - 4s 762us/step - loss: 0.3048 - acc: 0.8945\n",
      "Epoch 62/100\n",
      "5452/5452 [==============================] - 4s 728us/step - loss: 0.2990 - acc: 0.8967\n",
      "Epoch 63/100\n",
      "5452/5452 [==============================] - 4s 706us/step - loss: 0.2746 - acc: 0.9037\n",
      "Epoch 64/100\n",
      "5452/5452 [==============================] - 4s 789us/step - loss: 0.2809 - acc: 0.9000\n",
      "Epoch 65/100\n",
      "5452/5452 [==============================] - 4s 796us/step - loss: 0.2761 - acc: 0.8993\n",
      "Epoch 66/100\n",
      "5452/5452 [==============================] - 4s 784us/step - loss: 0.2765 - acc: 0.9044\n",
      "Epoch 67/100\n",
      "5452/5452 [==============================] - 4s 750us/step - loss: 0.2660 - acc: 0.9046\n",
      "Epoch 68/100\n",
      "5452/5452 [==============================] - 4s 753us/step - loss: 0.2502 - acc: 0.9114\n",
      "Epoch 69/100\n",
      "5452/5452 [==============================] - 4s 756us/step - loss: 0.2646 - acc: 0.9070\n",
      "Epoch 70/100\n",
      "5452/5452 [==============================] - 4s 779us/step - loss: 0.2474 - acc: 0.9134\n",
      "Epoch 71/100\n",
      "5452/5452 [==============================] - 4s 744us/step - loss: 0.2257 - acc: 0.9244\n",
      "Epoch 72/100\n",
      "5452/5452 [==============================] - 4s 787us/step - loss: 0.2274 - acc: 0.9198\n",
      "Epoch 73/100\n",
      "5452/5452 [==============================] - 4s 797us/step - loss: 0.2362 - acc: 0.9158\n",
      "Epoch 74/100\n",
      "5452/5452 [==============================] - 4s 777us/step - loss: 0.2296 - acc: 0.9215\n",
      "Epoch 75/100\n",
      "5452/5452 [==============================] - 4s 797us/step - loss: 0.2322 - acc: 0.9215\n",
      "Epoch 76/100\n",
      "5452/5452 [==============================] - 4s 757us/step - loss: 0.2071 - acc: 0.9261\n",
      "Epoch 77/100\n",
      "5452/5452 [==============================] - 4s 759us/step - loss: 0.2145 - acc: 0.9239\n",
      "Epoch 78/100\n",
      "5452/5452 [==============================] - 4s 789us/step - loss: 0.2153 - acc: 0.9222\n",
      "Epoch 79/100\n",
      "5452/5452 [==============================] - 4s 812us/step - loss: 0.2205 - acc: 0.9215\n",
      "Epoch 80/100\n",
      "5452/5452 [==============================] - 4s 803us/step - loss: 0.2118 - acc: 0.9264\n",
      "Epoch 81/100\n",
      "5452/5452 [==============================] - 5s 866us/step - loss: 0.2072 - acc: 0.9264\n",
      "Epoch 82/100\n",
      "5452/5452 [==============================] - 5s 885us/step - loss: 0.2102 - acc: 0.9224\n",
      "Epoch 83/100\n",
      "5452/5452 [==============================] - 4s 771us/step - loss: 0.1912 - acc: 0.9331\n",
      "Epoch 84/100\n",
      "5452/5452 [==============================] - 4s 775us/step - loss: 0.1990 - acc: 0.9285\n",
      "Epoch 85/100\n",
      "5452/5452 [==============================] - 4s 806us/step - loss: 0.1920 - acc: 0.9338\n",
      "Epoch 86/100\n",
      "5452/5452 [==============================] - 4s 760us/step - loss: 0.1776 - acc: 0.9398\n",
      "Epoch 87/100\n",
      "5452/5452 [==============================] - 4s 795us/step - loss: 0.1904 - acc: 0.9338\n",
      "Epoch 88/100\n",
      "5452/5452 [==============================] - 4s 778us/step - loss: 0.1850 - acc: 0.9349\n",
      "Epoch 89/100\n",
      "5452/5452 [==============================] - 4s 771us/step - loss: 0.1718 - acc: 0.9422\n",
      "Epoch 90/100\n",
      "5452/5452 [==============================] - 4s 748us/step - loss: 0.1813 - acc: 0.9369\n",
      "Epoch 91/100\n",
      "5452/5452 [==============================] - 4s 754us/step - loss: 0.1622 - acc: 0.9455\n",
      "Epoch 92/100\n",
      "5452/5452 [==============================] - 4s 750us/step - loss: 0.1664 - acc: 0.9426\n",
      "Epoch 93/100\n",
      "5452/5452 [==============================] - 4s 799us/step - loss: 0.1523 - acc: 0.9441\n",
      "Epoch 94/100\n",
      "5452/5452 [==============================] - 4s 804us/step - loss: 0.1571 - acc: 0.9446\n",
      "Epoch 95/100\n",
      "5452/5452 [==============================] - 4s 792us/step - loss: 0.1638 - acc: 0.9486\n",
      "Epoch 96/100\n",
      "5452/5452 [==============================] - 4s 778us/step - loss: 0.1534 - acc: 0.9474\n",
      "Epoch 97/100\n",
      "5452/5452 [==============================] - 4s 763us/step - loss: 0.1329 - acc: 0.9530\n",
      "Epoch 98/100\n",
      "5452/5452 [==============================] - 4s 785us/step - loss: 0.1451 - acc: 0.9525\n",
      "Epoch 99/100\n",
      "5452/5452 [==============================] - 4s 763us/step - loss: 0.1450 - acc: 0.9512\n",
      "Epoch 100/100\n",
      "5452/5452 [==============================] - 4s 754us/step - loss: 0.1526 - acc: 0.9486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23cb1c2d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model6.fit(x = params6.get_train_padded(),\n",
    "           y = params6.get_train_labels_onehot(),\n",
    "           epochs = 100\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 85.8000000477%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model6.evaluate(params6.get_test_padded(),\n",
    "                params6.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes no obvious difference. \n",
    "\n",
    "Because the data is so small, and I know my LSTM has a propensity to overfit, I prefer a smaller context window over a larger one (Occam's razor). \n",
    "\n",
    "Intuitively, it would make sense that the neccessary window for learning the broad categorization of sentence will be small, becuas seeing the words _What is the..._ versus _Who is the..._ might already tell you that the first question is a entity, while the second is an human. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Stacked LSTMs\n",
    "\n",
    "I'll now try stacking two LSTM's on top of one another, and using recurrent dropout. Hopefully, the second LSTM will learn some other (more intersting) features than the dense layer did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params7 = LstmParams(sequence_length = 10,\n",
    "                    labels = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model7 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the word embedding layer\n",
    "model7.add(Embedding(input_dim = vocab_size, \n",
    "                     output_dim = 300, \n",
    "                     input_length = params7.sequence_length,\n",
    "                     trainable = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add an LSTM layer\n",
    "model7.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2, return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model7.add(LSTM(params7.get_num_classes(), dropout=0.2, recurrent_dropout=0.2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model7.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 10, 300)           27936600  \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 10, 100)           160400    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 6)                 2568      \n",
      "=================================================================\n",
      "Total params: 28,099,568\n",
      "Trainable params: 162,968\n",
      "Non-trainable params: 27,936,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5452/5452 [==============================] - 11s 2ms/step - loss: 1.5168 - acc: 0.3450\n",
      "Epoch 2/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 1.1412 - acc: 0.5372\n",
      "Epoch 3/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.9977 - acc: 0.6108\n",
      "Epoch 4/100\n",
      "5452/5452 [==============================] - 8s 1ms/step - loss: 0.8815 - acc: 0.6623\n",
      "Epoch 5/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.8183 - acc: 0.6864\n",
      "Epoch 6/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.7884 - acc: 0.6994\n",
      "Epoch 7/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.7494 - acc: 0.7161\n",
      "Epoch 8/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.7291 - acc: 0.7232\n",
      "Epoch 9/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.7207 - acc: 0.7285\n",
      "Epoch 10/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6977 - acc: 0.7449\n",
      "Epoch 11/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6849 - acc: 0.7469\n",
      "Epoch 12/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.7428 - acc: 0.7285\n",
      "Epoch 13/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.6937 - acc: 0.7491\n",
      "Epoch 14/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6771 - acc: 0.7472\n",
      "Epoch 15/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.6554 - acc: 0.7605\n",
      "Epoch 16/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6423 - acc: 0.7616\n",
      "Epoch 17/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.6419 - acc: 0.7658\n",
      "Epoch 18/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.6382 - acc: 0.7638\n",
      "Epoch 19/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.6184 - acc: 0.7716\n",
      "Epoch 20/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.6239 - acc: 0.7746\n",
      "Epoch 21/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.6162 - acc: 0.7711\n",
      "Epoch 22/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.6054 - acc: 0.7751\n",
      "Epoch 23/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.5928 - acc: 0.7814\n",
      "Epoch 24/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.6001 - acc: 0.7808\n",
      "Epoch 25/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.5890 - acc: 0.7883\n",
      "Epoch 26/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.5846 - acc: 0.7841\n",
      "Epoch 27/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5900 - acc: 0.7825\n",
      "Epoch 28/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.5811 - acc: 0.7896\n",
      "Epoch 29/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.5613 - acc: 0.7971\n",
      "Epoch 30/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.5607 - acc: 0.7993\n",
      "Epoch 31/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.5520 - acc: 0.8001\n",
      "Epoch 32/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.5449 - acc: 0.7999\n",
      "Epoch 33/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.5389 - acc: 0.8014\n",
      "Epoch 34/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.5260 - acc: 0.8045\n",
      "Epoch 35/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5359 - acc: 0.8043\n",
      "Epoch 36/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.5191 - acc: 0.8058\n",
      "Epoch 37/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5430 - acc: 0.8021\n",
      "Epoch 38/100\n",
      "5452/5452 [==============================] - 8s 1ms/step - loss: 0.5242 - acc: 0.8124\n",
      "Epoch 39/100\n",
      "5452/5452 [==============================] - 8s 1ms/step - loss: 0.5224 - acc: 0.8063\n",
      "Epoch 40/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5140 - acc: 0.8109\n",
      "Epoch 41/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4790 - acc: 0.8325\n",
      "Epoch 42/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4962 - acc: 0.8208\n",
      "Epoch 43/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4828 - acc: 0.8226\n",
      "Epoch 44/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4784 - acc: 0.8254\n",
      "Epoch 45/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4846 - acc: 0.8261\n",
      "Epoch 46/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4816 - acc: 0.8188\n",
      "Epoch 47/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.4667 - acc: 0.8259\n",
      "Epoch 48/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4570 - acc: 0.8325\n",
      "Epoch 49/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.4593 - acc: 0.8292\n",
      "Epoch 50/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4543 - acc: 0.8344\n",
      "Epoch 51/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4381 - acc: 0.8399\n",
      "Epoch 52/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4382 - acc: 0.8432\n",
      "Epoch 53/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4301 - acc: 0.8399\n",
      "Epoch 54/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4498 - acc: 0.8368\n",
      "Epoch 55/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4179 - acc: 0.8423\n",
      "Epoch 56/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.4076 - acc: 0.8483\n",
      "Epoch 57/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.4206 - acc: 0.8424\n",
      "Epoch 58/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.3946 - acc: 0.8523\n",
      "Epoch 59/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3978 - acc: 0.8556\n",
      "Epoch 60/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.3884 - acc: 0.8590\n",
      "Epoch 61/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3872 - acc: 0.8608\n",
      "Epoch 62/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3730 - acc: 0.8623\n",
      "Epoch 63/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3590 - acc: 0.8674\n",
      "Epoch 64/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3584 - acc: 0.8652\n",
      "Epoch 65/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3595 - acc: 0.8643\n",
      "Epoch 66/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3504 - acc: 0.8685\n",
      "Epoch 67/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3475 - acc: 0.8755\n",
      "Epoch 68/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3412 - acc: 0.8788\n",
      "Epoch 69/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3510 - acc: 0.8723\n",
      "Epoch 70/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3816 - acc: 0.8670\n",
      "Epoch 71/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3395 - acc: 0.8775\n",
      "Epoch 72/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3282 - acc: 0.8810\n",
      "Epoch 73/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3066 - acc: 0.8857\n",
      "Epoch 74/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3197 - acc: 0.8850\n",
      "Epoch 75/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3143 - acc: 0.8850\n",
      "Epoch 76/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3007 - acc: 0.8910\n",
      "Epoch 77/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2984 - acc: 0.8855\n",
      "Epoch 78/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2879 - acc: 0.8942\n",
      "Epoch 79/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2905 - acc: 0.8960\n",
      "Epoch 80/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2700 - acc: 0.9010\n",
      "Epoch 81/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3552 - acc: 0.8758\n",
      "Epoch 82/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3141 - acc: 0.8866\n",
      "Epoch 83/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.5814 - acc: 0.8333\n",
      "Epoch 84/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3789 - acc: 0.8659\n",
      "Epoch 85/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3447 - acc: 0.8795\n",
      "Epoch 86/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3271 - acc: 0.8808\n",
      "Epoch 87/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.3171 - acc: 0.8855\n",
      "Epoch 88/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2997 - acc: 0.8940\n",
      "Epoch 89/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2954 - acc: 0.8955\n",
      "Epoch 90/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2884 - acc: 0.8938\n",
      "Epoch 91/100\n",
      "5452/5452 [==============================] - 6s 1ms/step - loss: 0.2818 - acc: 0.8955\n",
      "Epoch 92/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2838 - acc: 0.8955\n",
      "Epoch 93/100\n",
      "5452/5452 [==============================] - 8s 1ms/step - loss: 0.2797 - acc: 0.8938\n",
      "Epoch 94/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2628 - acc: 0.9054\n",
      "Epoch 95/100\n",
      "5452/5452 [==============================] - 8s 1ms/step - loss: 0.2633 - acc: 0.9032\n",
      "Epoch 96/100\n",
      "5452/5452 [==============================] - 8s 1ms/step - loss: 0.2808 - acc: 0.9010\n",
      "Epoch 97/100\n",
      "5452/5452 [==============================] - 8s 1ms/step - loss: 0.2656 - acc: 0.9043\n",
      "Epoch 98/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2633 - acc: 0.9052\n",
      "Epoch 99/100\n",
      "5452/5452 [==============================] - 8s 1ms/step - loss: 0.2491 - acc: 0.9103\n",
      "Epoch 100/100\n",
      "5452/5452 [==============================] - 7s 1ms/step - loss: 0.2432 - acc: 0.9131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x257936290>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model\n",
    "model7.fit(x = params7.get_train_padded(),\n",
    "           y = params7.get_train_labels_onehot(),\n",
    "           epochs = 100\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 87.2000000954%\n"
     ]
    }
   ],
   "source": [
    "print \"Test Set Accuracy: {0}%\".format(model7.evaluate(params7.get_test_padded(),\n",
    "                params7.get_test_labels_onehot(),\n",
    "                verbose=0)[1]*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
